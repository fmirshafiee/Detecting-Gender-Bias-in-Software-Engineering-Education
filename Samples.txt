____________________________________________

SecretsAndLies.pdf page: 4

____________________________________________

“just as applied cryptography was the bible for cryptographers in the 90’s,
so secrets and lies will be the official bible for infosec in the new mil-
lennium. i didn’t think it was possible that a book on business security
could make me laugh and smile, but schneier has made this subject very
enjoyable.”
–jim wallner, national security agency
“the news media offer examples of our chronic computer security woes
on a near-daily basis, but until now there hasn’t been a clear, comprehen-
sive guide that puts the wide range of digital threats in context. the
ultimate knowledgeable insider, schneier not only provides definitions,
explanations, stories, and strategies, but a measure of hope that we can
get through it all.”
–steven levy, author of hackers and crypto
“in his newest book, secrets and lies: digital security in a networked world,
schneier emphasizes the limitations of technology and offers managed
security monitoring as the solution of the future.”
–forbes magazine
453803_fm_3.qxd:453803_fm_3.qxd  4/18/13  8:40 am  page ii


 
 

____________________________________________

SecretsAndLies.pdf page: 15

____________________________________________

been significant: time, expense, reduced functionality, frustrated end-
users.  (increasing security regularly frustrates end-users.)  on the other
hand, the costs of ignoring security and getting hacked have been, in the
scheme of things, relatively small.  we in the computer security field like
to think they’re enormous, but they haven’t really affected a company’s
bottom line.  from the ceo’s perspective, the risks include the possibil-
ity of bad press and angry customers and network downtime—none of
which is permanent.  and there’s some regulatory pressure, from audits or
lawsuits, which adds additional costs.  the result: a smart organization
does what everyone else does, and no more.  things are changing; slowly,
but they’re changing.  the risks are increasing, and as a result spending is
increasing.
this same kind of economic reasoning explains why software vendors
spend so little effort securing their own products.  we in computer secu-
rity think the vendors are all a bunch of idiots, but they’re behaving com-
pletely rationally from their own point of view.  the costs of adding good
security to software products are essentially the same ones incurred in
increasing network security—large expenses, reduced functionality,
delayed product releases, annoyed users—while the costs of ignoring
security are minor: occasional bad press, and maybe some users switching
to competitors’ products.  the financial losses to industry worldwide due
to vulnerabilities in the microsoft windows operating system are not
borne by microsoft, so microsoft doesn’t have the financial incentive to
fix them. if the ceo of a major software company told his board of
directors that he would be cutting the company’s earnings per share by a
third because he was going to really—no more pretending—take security
seriously, the board would fire him.  if i were on the board, i would fire
him.  any smart software vendor will talk big about security, but do as
little as possible, because that’s what makes the most economic sense.
think about why firewalls succeeded in the marketplace.  it’s not
because they’re effective; most firewalls are configured so poorly that
they’re barely effective, and there are many more effective security prod-
ucts that have never seen widespread deployment (such as e-mail encryp-
tion).  firewalls are ubiquitous because corporate auditors started
demanding them.  this changed the cost equation for businesses.  the
cost of adding a firewall was expense and user annoyance, but the cost of
not having a firewall was failing an audit.  and even worse, a company
introduction to the paperback edition xiii
453803_fm_3.qxd:453803_fm_3.qxd  4/18/13  8:40 am  page xiii


 
 

____________________________________________

SecretsAndLies.pdf page: 16

____________________________________________

without a firewall could be accused of not following industry best
practices in a lawsuit.  the result: everyone has firewalls all over their net-
work, whether they do any actual good or not.
as scientists, we are awash in security technologies.  we know how
to build much more secure operating systems.  we know how to build
much more secure access control systems.  we know how to build much
more secure networks.  to be sure, there are still technological problems,
and research continues.  but in the real world, network security is a busi-
ness problem.  the only way to fix it is to concentrate on the business
motivations.  we need to change the economic costs and benefits of
security.  we need to make the organizations in the best position to fix
the problem want to fix the problem.
to do that, i have a three-step program.  none of the steps has
anything to do with technology; they all have to do with businesses,
economics, and people.
step one: enforce liabilities 
this is essential.  remember that i said the costs of bad security are not
borne by the software vendors that produce the bad security.  in eco-
nomics this is known as an externality: a cost of a decision that is borne by
people other than those making the decision.  today there are no real
consequences for having bad security, or having low-quality software of
any kind.  even worse, the marketplace often rewards low quality.  more
precisely, it rewards additional features and timely release dates, even if
they come at the expense of quality.  if we expect software vendors to
reduce features, lengthen development cycles, and invest in secure soft-
ware development processes, they must be liable for security vulnerabili-
ties in their products.  if we expect ceos to spend significant resources
on their own network security—especially the security of their cus-
tomers—they must be liable for mishandling their customers’ data.  basi-
cally, we have to tweak the risk equation so the ceo cares about actually
fixing the problem.  and putting pressure on his balance sheet is the best
way to do that.
this could happen in several different ways.  legislatures could
impose liability on the computer industry by forcing software manufac-
turers to live with the same product liability laws that affect other indus-
xiv introduction to the paperback edition
453803_fm_3.qxd:453803_fm_3.qxd  4/18/13  8:40 am  page xiv


 
 

____________________________________________

SecretsAndLies.pdf page: 18

____________________________________________

way.  and when they do, they’re going to drive the computer security
industry...just as they drive the security industry in the brick-and-mortar
world.
a ceo doesn’t buy security for his company’s warehouse—strong
locks, window bars, or an alarm system—because it makes him feel safe.
he buys that security because the insurance rates go down.  the same
thing will hold true for computer security.  once enough policies are
being written, insurance companies will start charging different premiums
for different levels of security.  even without legislated liability, the ceo
will start noticing how his insurance rates change.  and once the ceo
starts buying security products based on his insurance premiums, the
insurance industry will wield enormous power in the marketplace.  they
will determine which security products are ubiquitous, and which are
ignored.  and since the insurance companies pay for the actual losses, they
have a great incentive to be rational about risk analysis and the effective-
ness of security products.  this is different from a bunch of auditors decid-
ing that firewalls are important; these are companies with a financial
incentive to get it right.  they’re not going to be swayed by press releases
and pr campaigns; they’re going to demand real results.
and software companies will take notice, and will strive to increase
the security in the products they sell, in order to make them competitive
in this new “cost plus insurance cost” world. 
step three: provide mechanisms 
to reduce risk  
this will also happen automatically.  once insurance companies start
demanding real security in products, it will result in a sea change in the
computer industry.  insurance companies will reward companies that pro-
vide real security, and punish companies that don’t—and this will be
entirely market driven.  security will improve because the insurance
industry will push for improvements, just as they have in fire safety,
electrical safety, automobile safety, bank security, and other industries.
moreover, insurance companies will want it done in standard models
that they can build policies around.  a network that changes every month
or a product that is updated every few months will be much harder to
xvi introduction to the paperback edition
453803_fm_3.qxd:453803_fm_3.qxd  4/18/13  8:40 am  page xvi


 
 

____________________________________________

SecretsAndLies.pdf page: 29

____________________________________________

1
introduction
during march 2000, i kept a log of security events from various
sources. here are the news highlights:
someone broke into the business-to-business web site for salesgate.com
and stole about 3,000 customer records, including credit card numbers
and other personal information. he posted some of them on the
internet.
for years, personal information has “leaked” from web sites (such as
intuit) to advertisers (such as doubleclick). when visitors used vari-
ous financial calculators on the intuit site, a design glitch in the web
site’s programming allowed information they entered to be sent to
doubleclick. this happened without the users’ knowledge or con-
sent, and (more surprising) without intuit’s knowledge or consent.
convicted criminal hacker kevin mitnick testified before congress. he
told them that social engineering is a major security vulnerability:
he can often get passwords and other secrets just by pretending to be
someone else and asking.
a gallup poll showed that a third of online consumers said that they might
be less likely to make a purchase from a web site, in light of recent
computer-security events.
personal data from customers who ordered the playstation 2 from the
sony web site were accidentally leaked to some other customers.
(this is actually a rampant problem on all sorts of sites. people try to
1
453803_ch01_3.qxd:453803_ch01_3.qxd  4/11/13  1:25 pm  page 1


 
 

____________________________________________

SecretsAndLies.pdf page: 30

____________________________________________

check out, only to be presented with the information of another ran-
dom web customer.)
amazon.com pays commissions to third-party web sites for referrals.
someone found a way to subvert the program that manages this,
enabling anyone to channel information to whomever. it is unclear
whether amazon considers this a problem.
the cia director denied that the united states engages in economic
espionage, but did not go on to deny the existence of the massive
intelligence-gathering system called echelon. 
pierre-guy lavoie, 22, was convicted in quebec of breaking into several
canadian and u.s. government computers. he will serve 12 months
in prison.
japan’s defense agency delayed deployment of a new defense computer
system after it discovered that the software had been developed by the
members of the aum shinrikyo cult.
a new e-mail worm, called pretty park, spread across the internet. it’s a
minor modification of one that appeared last year. it spreads automati-
cally, by sending itself to all the addresses listed in a user’s outlook
express program. 
novell and microsoft continued to exchange barbs about an alleged secu-
rity bug with windows 2000’s active directory. whether or not this
is a real problem depends on what kind of security properties you
expect from your directory. (i believe it’s a design flaw in windows,
and not a bug.)
two people in sicily (giuseppe russo and his wife, sandra elazar) were
arrested after stealing about 1,000 u.s. credit card numbers on 
the internet and using them to purchase luxury goods and lottery
tickets.
a hacker (actually a bored teenager) known as “coolio” denied launching
massive denial-of-service attacks in february 2000. he admitted to
hacking into about 100 sites in the past, including cryptography com-
pany rsa security and a site belonging to the u.s. state department.
attackers launched a denial-of-service attack against microsoft’s israeli
web site.
jonathan bosanac, a.k.a. “the gatsby,” was sentenced to 18 months in
prison for hacking into three telephone company sites.
2 c h a p t e r  o n e
453803_ch01_3.qxd:453803_ch01_3.qxd  4/11/13  1:25 pm  page 2


 
 

____________________________________________

SecretsAndLies.pdf page: 31

____________________________________________

the military of taiwan announced that it discovered more than 7,000
attempts by chinese hackers to enter the country’s security systems.
this tantalizing statistic was not elaborated on.
here are some software vulnerabilities reported during march 2000: 
a vulnerability was reported in microsoft internet explorer 5.0 (in windows
95, 98, nt 4.0, and 2000) that allows an attacker to set up a web page
giving him the ability to execute any program on a visitor’s machine. 
by modifying the url, an attacker can completely bypass the authentica-
tion mechanisms protecting the remote-management screens of the
axis starpoint cd-rom servers.
if an attacker sends the netscape enterprise server 3.6 a certain type of
long message, a buffer overflow crashes a particular process. the
attacker can then execute arbitrary code remotely on the server.
it is possible to launch some attacks (one denial-of-service attack, and
another attack against a cgi script) that internet security systems’s
realsecure network intrusion detection software does not detect.
by sending a certain url to a server running allaire’s coldfusion prod-
uct, an attacker can receive an error message giving information about
the physical paths to various files.
omniback is a hewlett-packard product that performs system backup rou-
tines. an attacker can manipulate the product to cause a denial-of-ser-
vice attack.
there is a vulnerability in the configuration of dosemu, the dos emula-
tor shipped with corel linux 1.0, that allows users to execute
commands with root privileges.
by manipulating the contents of certain variables, an attacker can exploit a
vulnerability in dnstools 1.0.8 to execute arbitrary code.
sgi has a package called infosearch that automatically converts text docu-
mentation to html web content. a bug in the cgi script allows
attackers to execute commands on the server at the web server privi-
lege level.
several vulnerabilities were discovered in the e-mail client the bat!,
allowing an attacker to steal files from users’ computers.
introduction 3
453803_ch01_3.qxd:453803_ch01_3.qxd  4/11/13  1:25 pm  page 3


 
 

____________________________________________

SecretsAndLies.pdf page: 34

____________________________________________

admiral grace hopper said: “life was simple before world war ii.
after that, we had systems.” this is an insightful comment.
once you start conceptualizing systems, it’s possible to design and
build on a more complex scale. it’s the difference between a building and
a skyscraper, a cannon and a patriot missile, a landing strip and an airport.
anyone can build a traffic light, but it takes a different mindset to
conceive of a citywide traffic control system.
the internet is probably the most complex system ever developed. it
contains millions of computers, connected in an inconceivably complex
physical network. each computer has hundreds of software programs run-
ning on it; some of these programs interact with other programs on the
computer, some of them interact with other programs on other comput-
ers across the network. the system accepts user input from millions of
people, sometimes all at the same time.
as the man said: “sir, it is like a dog standing upon his hind legs, you
are not surprised to see it not done well, you are surprised to see it done
at all.” 
systems have several interesting properties relevant to this book.
first, they are complex. machines are simple: a hammer, a door
hinge, a steak knife. systems are much more complicated; they have com-
ponents, feedback loops, mean times between failure, infrastructure.
digital systems are daedal; even a simple computer program has hundreds
of thousands of lines of computer code doing all sorts of different things.
a complex computer program has thousands of components, each of
which has to work by itself and in interaction with all the other
components. this is why object-oriented programming was developed:
to deal with the complexity of digital systems.
second, systems interact with each other, forming even larger sys-
tems. this can happen on purpose—programmers use objects to deliber-
ately break large systems down into smaller systems, engineers break large
mechanical systems into smaller subsystems, and so on—and it can happen
naturally. the invention of the automobile led to the development of the
modern system of roads and highways, and this in turn interacted with
other systems in our daily lives to produce the suburb. the air-
traffic control system interacts with the navigation systems on aircrafts,
and the weather prediction system. the human body interacts with other
human bodies and with the other systems on the planet. the internet has
intertwined itself with almost every major system in our society.
6 c h a p t e r  o n e
453803_ch01_3.qxd:453803_ch01_3.qxd  4/11/13  1:25 pm  page 6


 
 

____________________________________________

SecretsAndLies.pdf page: 35

____________________________________________

third, systems have emergent properties. in other words, they do
things that are not anticipated by the users or designers. the telephone
system, for example, changed the way people interact. (alexander
 graham bell had no idea that a telephone was a personal communications
device; he thought you could use it to call ahead to warn that a telegram
was coming.) automobiles changed the way people meet, date, and fall in
love. environmental-control systems in buildings have effects on people’s
health, which affects the health care system. word processing systems
have changed the way people write. the internet is full of emergent
properties; think about ebay, virtual sex, collaborative authoring.
and fourth, systems have bugs. a bug is a particular kind of failure.
it’s an emergent property of a system, one that is not desirable. it’s differ-
ent from a malfunction. when something malfunctions, it no longer
works properly. when something has a bug, it misbehaves in a particular
way, possibly unrepeatable, and possibly unexplainable. bugs are unique
to systems. machines can break, or fail, or not work, but only a system can
have a bug.
systems and security
these properties all have profound effects on the security of systems.
finessing the precise definition of secure for now, the reason that it is so
hard to secure a complex system like the internet is, basically, because it’s
a complex system. systems are hard to secure, and complex systems are
that much more operose.
for computerized systems, the usual coping mechanism is to ignore
the system and concentrate on the individual machines . . . the technolo-
gies. this is why we have lots of work on security technologies like cryp-
tography, firewalls, public-key infrastructures, and tamper-resistance.
these technologies are much easier to understand and to discuss, and
much easier to secure. the conceit is that these technologies can mysti-
cally imbue the systems with the property of <reverence type =
“hushed”> security </reverence>.
this doesn’t work, and the results can be seen in my security log from
seven days of march 2000. most of the security events can be traced to
one or more of the four system properties previously listed.
introduction 7
453803_ch01_3.qxd:453803_ch01_3.qxd  4/11/13  1:25 pm  page 7


 
 

____________________________________________

SecretsAndLies.pdf page: 44

____________________________________________

against digital systems will be the same as attacks against their analog ana-
logues.
this means we can look in the past to see what the future will hold.
the attacks will look different—the burglar will manipulate digital con-
nections and database entries instead of lockpicks and crowbars, the ter-
rorist will target information systems instead of airplanes—but the
motivation and psychology will be the same. it also means we don’t need
a completely different legal system to deal with the future. if the future is
like the past—except with cooler special effects—then a legal system that
worked in the past is likely to work in the future.
willie sutton robbed banks because that was where the money was.
today, the money isn’t in banks; it’s zipping around computer networks.
every day, the world’s banks transfer billions of dollars among themselves
by simply modifying numbers in computerized databases. meanwhile, the
average physical bank robbery grosses a little over fifteen hundred dollars.
and cyberspace will get even more enticing; the dollar value of electronic
commerce gets larger every year.
where there’s money, there are criminals. walking into a bank or a
liquor store wearing a ski mask and brandishing a .45 isn’t completely
passé, but it’s not the preferred method of criminals drug-free enough to
sit down and think about the problem. organized crime prefers to attack
large-scale systems to make a large-scale profit. fraud against credit cards
and check systems has gotten more sophisticated over the years, as
defenses have gotten more sophisticated. automatic teller machine
(atm) fraud has followed the same pattern. if we haven’t seen wide-
spread fraud against internet payment systems yet, it’s because there isn’t
a lot of money to be made there yet. when there is, criminals will be
there trying. and if history is any guide, they will succeed.
privacy violations are nothing new, either. an amazing array of legal
paperwork is public record: real estate transactions, boat sales, civil and
criminal trials and judgments, bankruptcies. want to know who owns
that boat and how much he paid for it? it’s a matter of public record. even
more personal information is held in the 20,000 or so (in the united
states) personal databases held by corporations: financial details, medical
information, lifestyle habits.
investigators (private and police) have long used this and other data to
track down people. even supposedly confidential data gets used in this
fashion. no tv private investigator has survived half a season without a
16 c h a p t e r  t w o
453803_ch02.qxd:453803_ch02.qxd  4/12/13  9:27 am  page 16


 
 

____________________________________________

SecretsAndLies.pdf page: 45

____________________________________________

friend in the local police force willing to look up a name or a license plate
or a criminal record in the police files. police routinely use industry
databases. and every few years, some bored irs operator gets caught
looking up the tax returns of famous people.
marketers have long used whatever data they could get their hands on
to target particular people and demographics. in the united states, per-
sonal data do not belong to the person whom the data are about, they
belong to the organization that collected it. your financial information
isn’t your property, it’s your bank’s. your medical information isn’t yours,
it’s your doctor’s. doctors swear oaths to protect your privacy, but insur-
ance providers and hmos do not. do you really want everyone to know
about your heart defect or your family’s history of glaucoma? how about
your bout with alcoholism, or that embarrassing brush with venereal
disease two decades ago?
privacy violations can easily lead to fraud. in the novel paper moon,
joe david brown wrote about the depression-era trick of selling bibles
and other merchandise to the relatives of the recently deceased. other
scams targeted the mothers and widows of overseas war dead—“for only
pennies a day we’ll care for his grave”—and people who won sweep-
stakes. in many areas in the country, public utilities are installing
telephone-based systems to read meters: water, electricity, and the like.
it’s a great idea, until some enterprising criminal uses the data to track
when people go away on vacation. or when they use alarm monitoring
systems that give up-to-the-minute details on building occupancy.
wherever data can be exploited, someone will try it, computers or no
computers.
nothing in cyberspace is new. child pornography: old hat. money
laundering: seen it. bizarre cults offering everlasting life in exchange for
your personal check: how déclassé. the underworld is no better than
businesspeople at figuring out what the net is good for; they’re just
repackaging their old tricks for the new medium, taking advantage of the
subtle differences and exploiting the net’s reach and scalability.
the changing nature of attacks
the threats may be the same, but cyberspace changes everything.
although attacks in the digital world might have the same goals and share
digital threats 17
453803_ch02.qxd:453803_ch02.qxd  4/12/13  9:27 am  page 17


 
 

____________________________________________

SecretsAndLies.pdf page: 49

____________________________________________

this difference in laws among various states and countries can even
lead to a high-tech form of jurisdiction shopping. sometimes this can
work in the favor of the prosecutor, because this is exactly what the ten-
nessee conviction of the california bbs was. other times it can work in
the favor of the criminal: any organized crime syndicate with enough
money to launch a large-scale attack against a financial system would do
well to find a country with poor computer crime laws, easily bribable
police officers, and no extradition treaties.
technique propagation
the third difference is the ease with which successful techniques can
propagate through cyberspace. hbo doesn’t care very much if someone
can build a decoder in his basement. it requires time, skill, and some
money. but what if that person published an easy way for everyone to get
free satellite tv? no work. no hardware. “just punch these seven digits
into your remote control, and you never have to pay for cable tv again.”
that would increase the number of nonpaying customers to the millions,
and could significantly affect the company’s profitability.
physical counterfeiting is a problem, but it’s a manageable problem.
over two decades ago, we sold the shah of iran some of our old intaglio
printing presses. when ayatollah khomeini took over, he realized that it
was more profitable to mint $100 bills than iranian rials. the fbi calls
them supernotes, and they’re near perfect. (this is why the united states
redesigned its currency.) at the same time the fbi and the secret service
were throwing up their hands, the department of the treasury did some
calculating: the iranian presses can only print so much money a minute,
there are only so many minutes in a year, so there’s a maximum to the
amount of counterfeit money they can manufacture. treasury decided
that the amount of counterfeit currency couldn’t affect the money supply,
so it wasn’t a serious concern to the nation’s stability.
if the counterfeiting were electronic, it would be different. an elec-
tronic counterfeiter could automate the hack and publish it on some web
site somewhere. people could download this program and start unde-
tectably counterfeiting electronic money. by morning it could be in the
hands of 1,000 first-time counterfeiters; another 100,000 could have it in
a week. the u.s. currency system could collapse in a week.
digital threats 21
453803_ch02.qxd:453803_ch02.qxd  4/12/13  9:27 am  page 21


 
 

____________________________________________

SecretsAndLies.pdf page: 55

____________________________________________

 didn’t take too much advantage of it. that’s history now, and we’ll never
get back to that point again.
brand theft
virtual identity is vital to businesses as well as individuals. it takes time and
money to develop a corporate identity. this identity is more than logos
and slogans and catchy jingles. it’s product, bricks-and-mortar buildings,
customer service representatives—things to touch, people to talk to.
brand equals reputation.
on the internet, the barrier to entry is minimal. anyone can have a
web site, from citibank to fred’s safe-money mattress. and everyone
does. how do users know which sites are worth visiting, worth book-
marking, worth establishing a relationship with? thousands of companies
sell pcs on the web. who is real, and who is fly-by-night?
branding is the only answer to this question. when the web first
entered the public eye, pundits claimed that it heralded the end of the big
brand. because anyone could go on the web and compete with the big
names, brands were meaningless. the reality is exactly the opposite. since
anyone can go on the web and compete with the big names, the only
way to tell products apart is by their brands. users look at brands, and they
return to the sites they trust. a brand has real value, and it’s worth
stealing.
an example: a malaysian company wanted to market condoms using
the “visa” brand. they claimed that it had nothing to do with the credit
card company, but was a pun on “permit to entry.” visa was unamused,
and sued. it won, and i believe this ruling has profound implications for
brand ownership.
cyberspace has many opportunities for brand theft. in 1998, someone
forged a domain-name transfer request to network solutions and stole
sex.com; the original owner is still trying to get it back. another recent
case involved a plumber who rerouted customer phone calls for another
plumber to his own number. organized crime syndicates in las vegas
have done the same thing with escort-service phone numbers. this kind
of attack is nothing new. almon strowger was an undertaker in kansas
city. he was convinced that telephone operators were rerouting tele-
phone calls to rival businesses, so he invented the dial telephone in 1887
to bypass the operators.
attacks 27
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 27


 
 

____________________________________________

SecretsAndLies.pdf page: 57

____________________________________________

would be no less guilty if he used a pen and paper.) and extortion is no
better or worse if carried out using computer viruses or old-fashioned
compromising photos. good laws are written to be independent of
technology. in a world where technology advances much faster than
con gressional sessions, this is what can work today. faster and more
responsive mechanisms for legislation, prosecution, and adjudication . . .
maybe someday.
privacy violations
privacy violations are not necessarily criminal, but they can be. (they can
be a prelude to identity theft, for example.) in the united states, most
privacy violations are legal. people do not own their own data. if a credit
bureau or a marketing research firm collects data about you—your per-
sonal habits, your buying patterns, your financial status, your physical
health—it can sell it to anyone who wants it without your knowledge or
consent. it’s different elsewhere. privacy laws in much of europe (includ-
ing the european union), taiwan, new zealand, and canada are more
restrictive.
other types of privacy violations are legal, too. hiring a private inves-
tigator to collect information on a person or a company is legal, as long as
the investigator doesn’t use any illegal methods. all sorts of privacy viola-
tions by the police are legal with a warrant, and many are legal without.
(did you know that in the united states police don’t need a warrant to
demand a copy of the photographs you dropped off for developing?)
there are two types of privacy violations—targeted attacks and data
harvesting—and they are fundamentally different. in a targeted attack, an
attacker wants to know everything about alice. if “alice” is a person, it’s
called stalking. if “alice” is a company, it’s called industrial espionage.
if “alice” is a government, it’s called national intelligence or spying.
all of these will get you thrown in jail if you use some techniques, but not
if you use others.
computer security can protect alice against a targeted attack, but
only up to a point. if attackers are well enough funded, they can always
get around computer security measures. they can install a bug in alice’s
office, rummage through alice’s trash, or spy with a telescope. informa-
tion is information, and computer security only protects the information
attacks 29
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 29


 
 

____________________________________________

SecretsAndLies.pdf page: 59

____________________________________________

in shirt buttons that allowed police to pinpoint everyone’s location. van
eck devices can read what’s on your computer monitor from halfway
down the street. (right now this is an expensive and complicated attack,
but just wait until wireless lans become popular.) pinhole cameras—
now being sold in electronics catalogs—can hide in the smallest cracks;
satellite cameras can read your license plate from orbit. and the depart-
ment of defense is prototyping micro air vehicles, the size of small birds
or butterflies, that can scout out enemy snipers, locate hostages in occu-
pied buildings, or spy on just about anybody.
the ability to trail someone remotely has existed for a while, but it is
only used in exceptional circumstances (except on tv). in 1993, colom-
bian drug lord pablo escobar was identified partly by tracking him
through his cellular phone usage: a technique known as pinpointing. in
1996, the russian army killed chechnyan leader dzholar dudayev with
an air-to-surface missile after pinpointing his location from the trans -
missions of his personal satellite phone. the fbi found the truck
belonging to the oklahoma city federal building’s bomber because
agents collected the tapes from every surveillance camera in the city,
correlated them by time (the explosion acted as a giant synch pulse), and
looked for it. invisible identification tags are printed on virtually all color
xerographic output, from all of the manufacturers. (these machines also
include anticounterfeiting measures, such as dumping extra cyan toner
onto images when the unit detects an attempt to copy u.s. currency.)
explosives have embedded taggants.
the technology to automatically search for drug negotiations in ran-
dom telephone conversations, for suspicious behavior in satellite images,
or for faces on a “wanted list” of criminals in on-street cameras isn’t com-
monplace yet, but it’s just a matter of time. face recognition will be able
to pick individual people out of a crowd. voice recognition will be able
to scan millions of telephone calls listening for a particular person; it can
already scan for suspicious words or phrases and pick conversations out of
a crowd. moore’s law, which predicts the industry can double the
computing power of a microchip every 18 months, affects surveillance
computing just as it does everything else: the next generation will be
smaller, faster, a lot cheaper, and more easily available. as soon as the
recognition technologies isolate the people, the computers will be able to
do the searching.
attacks 31
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 31


 
 

____________________________________________

SecretsAndLies.pdf page: 62

____________________________________________

online law enforcement databases are a great boon to the police—it
really helps to be able to automatically download a criminal record or
mugshot directly to a squad car—but privacy fears remain. police data-
bases are not much more secure than any other commercial database, and
the information is a lot more sensitive.
traffic analysis
traffic analysis is the study of communication patterns. not the content of
the messages themselves, but characteristics about them. who communi-
cates with whom? when? how long are the messages? how quickly are
the replies sent, and how long are they? what kinds of communications
happen after a certain message is received? these are all traffic analysis
questions, and their answers can reveal a lot of information.
for example, if each time alice sends a long message to bob, bob
sends a short reply back to alice and a long message to five other people,
this indicates a chain of command. alice is clearly sending orders to bob,
who is relaying them to his subordinates. if alice sends regular short
messages to bob, and suddenly sends a series of long ones, this indicates
that something (what?) has changed.
often the patterns of communication are just as important as the con-
tents of communication. for example, the simple fact that alice
telephones a known terrorist every week is more important than the
details of their conversation. the nazis used the traffic-analysis data in
itemized french phone bills to arrest friends of the arrested; they didn’t
really care what the conversations were about. calls from the white
house to monica lewinsky were embarrassing enough, even without a
transcription of the conversation. in the hours preceding the u.s. bomb-
ing of iraq in 1991, pizza deliveries to the pentagon increased one hun-
dredfold. anyone paying attention certainly knew something was up.
(interestingly enough, the cia had the same number of pizzas delivered
as any other night.) some studies have shown that even if you encrypt
your web traffic, traffic analysis based on the size of the encrypted web
pages is more than enough to figure out what you’re browsing.
while militaries have used traffic analysis for decades, it is still a new
area of study in the academic world. we don’t really know how vulner-
able our communications—especially our internet communications—are
34 c h a p t e r  t h r e e
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 34


 
 

____________________________________________

SecretsAndLies.pdf page: 64

____________________________________________

traffic analysis is even more important. traffic patterns reveal a lot
about any organization and are much easier to collect and analyze than
actual communications data. they also provide additional information to
a diagnostic engine. elaborate databases of traffic patterns are un doubtedly
the heart of any echelon-like system.
one last note: in a world where most communications are unen-
crypted, encrypted communications are probably routinely recorded. the
mere indication that the conversers do not want to be overheard would
be enough to raise an alarm.
publicity attacks
the publicity attack is conceptually simple: “how can i get my name in
the newspapers by attacking the system?” this type of attack is relatively
new in the digital world: a few years ago, computer hacks weren’t con-
sidered newsworthy, and i can’t think of any other technology in history
that people would try to break simply to get their names in the paper. in
the physical world, this attack is ancient: the man who burned down the
temple of artemis in ancient greece did so because he wanted his name
to be remembered forever. (his name was herostratus, by the way.)
more recently, the kids who shot up columbine high school wanted
infamy.
most attackers of this type are hackers: skilled individuals who know
a lot about systems and their security. they often have access to significant
resources, either as students of large universities or as employees of large
companies. they usually don’t have a lot of money, but sometimes have
a lot of time. furthermore, they are not likely to do anything that will put
them in jail; the idea is publicity, not incarceration.
the canonical example of this is the breaking of netscape navigator’s
encryption scheme by two berkeley graduate students in 1995. these
students didn’t use the weakness for ill-gotten gain; they called the
new york times. netscape’s reaction was something on the order of “we
did some calculations, and thought it would take umpteen dollars of com-
puting power; we didn’t think it was worth anyone’s trouble to break it.”
they were right; it wasn’t worth anyone’s trouble . . . anyone who was
interested in the money. the grad students had all sorts of skills, access to
all the unused computer time at their university, and no social lives.
36 c h a p t e r  t h r e e
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 36


 
 

____________________________________________

SecretsAndLies.pdf page: 66

____________________________________________

stunt. (they tried to pretend it was hackers, but the parody site looked
too professional, and the hacked page was uploaded to the site three days
before the legitimate site came online.)
these days it happens so often that it barely rates a mention in the
news. probably every major u.s. government web site was hacked in
1999, as were the web sites of many local and foreign governments. i
listed 65 web site defacements in the first week of march 2000 in chap-
ter 1. sysadmins have become inured to the problem.
denial-of-service attacks
more recently, denial-of-service attacks have become the publicity attack
du jour. this is only because of their massive press coverage, and will
hopefully become old news, too. the idea is simply to stop something
from working. and as anyone who has had to deal with the effects of
striking workers—bus drivers, air traffic controllers, farm laborers, and so
forth—can tell you, these attacks are effective.
there are other denial-of-service attacks in the physical world: boy-
cotts and blockades, for example. these attacks all have analogues in
cyberspace. someone with enough phone connections can tie up all the
modem connections of a local isp. the analog cell phone networks had
trouble freeing connections when a mobile user went from cell to cell; it
was possible to sit on a hill with a directional antenna and, by spinning it
around and around slowly, tie up all the channels in the nearby cells.
denial-of-service attacks work because computer networks are there
to communicate. some simple attack, like saying hello, can be automated
to the point where it becomes a denial-of-service attack. this is basically
the syn flood attack that brought down several isps in 1996.
here’s another denial-of-service attack: in the mid-1980s, jerry fal-
well’s political organization set up a toll-free number for something or
other. one guy programmed his computer to repeatedly dial the number
and then hang up. this did two things: it busied the phone lines so that
legitimate people could not call the number, and it cost falwell’s organi-
zation money every time a call was completed. nice denial-of-service
attack.
denial-of-service attacks can be preludes to criminal attacks. burglars
approach a warehouse at 1:00 a.m. and cut the connection between the
burglar alarm and the police station. the alarm rings, and the police are
38 c h a p t e r  t h r e e
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 38


 
 

____________________________________________

SecretsAndLies.pdf page: 68

____________________________________________

legal attacks
in 1994, in the united kingdom, a man found his bank account emptied.
when he complained about six withdrawals he did not make, he was
arrested and charged with attempted fraud. the british bank claimed that
the security in the atm system was infallible, and that the defendant was
unequivocally guilty. when the defense attorney examined the evidence,
he found (1) that the bank had no security management or quality
assurance for its software, (2) that there was never any external security
assessment, and (3) that the disputed withdrawals were never investigated.
in fact, the bank’s programmers claimed that since the code was written
in assembly language, it couldn’t possibly be the problem (because if there
was a bug, it would cause a system crash). the man was convicted
anyway. on appeal, the bank provided the court a huge security assess-
ment by an auditing firm. when the defense demanded equal access to
their systems in order to evaluate the security directly, the bank refused
and the conviction was overturned.
attacks that use the legal system are the hardest to protect against. the
aim here isn’t to exploit a flaw in a system. it isn’t even to find a flaw in a
system. the aim here is to persuade a judge and jury (who probably aren’t
technically savvy) that there could be a flaw in the system. the aim here is
to discredit the system, to put enough doubt in the minds of the judge and
jury that the security isn’t perfect, to prove a client’s innocence.
here’s a hypothetical example. in a major drug case, the police are
using data from a cellular phone that pinpoints the defendant’s phone at a
particular time and place. the defense attorney finds some hacker expert
who testifies that it is easy to falsify that kind of data, that it isn’t reliable,
that it could have been planted, and should not be counted as evidence.
the prosecution has its own set of experts that say the opposite, and one
possible outcome is that they cancel each other out and the trial goes on
without the cellular-phone evidence.
the same thing can happen to audit data being used to prosecute
someone who broke into a computer system, or signature data that is
being used to try to enforce a contract. “i never signed that,” says the
defendant. “the computer told me to enter my passphrase and then push
this button. that’s what i did.” a jury of the defendant’s peers—probably
just as befuddled by technology as the accused is claiming to be—is likely
to sympathize.
40 c h a p t e r  t h r e e
453803_ch03.qxd:453803_ch03.qxd  4/11/13  3:05 pm  page 40


 
 

____________________________________________

SecretsAndLies.pdf page: 71

____________________________________________

a wealthy adversary is the most flexible, since he can trade his
resources for other things. he can gain access by paying off an insider, and
expertise by buying technology or hiring experts (maybe telling them the
truth, maybe hiring them under false pretenses). he can also trade
money for risk by executing a more sophisticated—and therefore more
expensive—attack.
the rational adversary—not all adversaries are sane, but most are
rational within their frames of reference—will choose an attack that gives
him a good return on investment, considering his budget constraints:
expertise, access, manpower, time, and risk. some attacks require a lot of
access but not much expertise: a car bomb, for example. some attacks
require a lot of expertise but no access: breaking an encryption algorithm,
for example. each adversary is going to have a set of attacks that is
affordable to him, and a set of attacks that isn’t. if the adversary is paying
attention, he will choose the attack that minimizes his cost and maximizes
his benefits. 
hackers
the word hacker has several definitions, ranging from a corporate system
administrator adept enough to figure out how computers really work to
an ethically inept teenage criminal who cackles like beavis and butthead
as he trashes your network. the word has been co-opted by the media
and stripped of its meaning. it used to be a compliment; then it became
an insult. lately, people seem to like “cracker” for the bad guys, and
“hacker” for the good guys. i define a hacker as an individual who exper-
iments with the limitations of systems for intellectual curiosity or sheer
pleasure; the word describes a person with a particular set of skills and not
a particular set of morals. there are good hackers and bad hackers, just as
there are good plumbers and bad plumbers. (there are also good bad
hackers, and bad good hackers . . . but never mind that.)
hackers are as old as curiosity, although the term itself is modern.
galileo was a hacker. mme. curie was one, too. aristotle wasn’t. (aristo-
tle had some theoretical proof that women had fewer teeth than men.
a hacker would have simply counted his wife’s teeth. a good hacker
would have counted his wife’s teeth without her knowing about it, while
adversaries 43
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 43


 
 

____________________________________________

SecretsAndLies.pdf page: 72

____________________________________________

she was asleep. a good bad hacker might remove some of them, just to
prove a point.)
when i was in college, i knew a group similar to hackers: the key
freaks. they wanted access, and their goal was to have a key to every lock
on campus. they would study lockpicking and learn new techniques,
trade maps of the steam tunnels and where they led, and exchange copies
of keys with each other. a locked door was a challenge, a personal affront
to their ability. these people weren’t out to do damage—stealing stuff
wasn’t their objective—although they certainly could have. their hobby
was the power to go anywhere they wanted to.
remember the phone phreaks of yesteryear, the ones who could
whistle into payphones and make free phone calls. sure, they stole phone
service. but it wasn’t like they needed to make eight-hour calls to manila
or mcmurdo. and their real work was secret knowledge: the phone
network was a vast maze of information. they wanted to know the
system better than the designers, and they wanted the ability to modify it
to their will. understanding how the phone system worked—that was
the true prize. other early hackers were ham-radio hobbyists and
model-train enthusiasts.
richard feynman was a hacker; read any of his books.
computer hackers follow these evolutionary lines. or, they are the
same genus operating on a new system. computers, and networks in
particular, are the new landscape to be explored. networks provide the
ultimate maze of steam tunnels, where a new hacking technique becomes
a key that can open computer after computer. and inside is knowledge,
understanding. access. how things work. why things work. it’s all out
there, waiting to be discovered.
today’s computer hackers are stereotypically young (twenty-some-
thing and younger), male, and socially on the fringe. they have their own
counterculture: hacker names or handles, lingo, rules. and like any sub-
culture, only a small percentage of hackers are actually smart. the real
hackers have an understanding of technology at a basic level, and are
driven by a desire to understand. the rest are talentless poseurs and hang-
ers-on, either completely inept or basic criminals. sometimes they’re
called lamers or script kiddies.
hackers can have considerable expertise, often greater than that of the
system’s original designers. i’ve heard lots of security lectures, and the
most savvy speakers are the hackers. for them, it’s a passion. hackers look
44 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 44


 
 

____________________________________________

SecretsAndLies.pdf page: 74

____________________________________________

internet against this type of attack, and to write a research paper about
defending against it. it’s another thing entirely to write a program that
automates the attack.
the trin00 exploit serves no conceivable purpose other than to
attack systems. gun owners can argue self-defense, but internet servers
don’t break into anyone’s house at night. it’s actually much worse,
because once an exploit is written and made available, any wannabe
hacker can download it and attack computers on the internet. he  doesn’t
even have to know how it works. (see why they’re called “script kid-
dies”?) trin00 attacks were popular in early 2000 because the exploit was
available. if it weren’t—even if a research paper were available—none of
the script kiddies would be able to exploit the vulnerability.
certainly the lamers that use trin00 to attack systems are criminals. i
believe the person who wrote the exploit is, too. a fine line exists
between writing code to demonstrate research and publishing attack
tools; between hacking for good and hacking as a criminal activity. i will
get back to this in chapter 22.
most organizations are wary about hiring hackers, and rightfully so.
there are exceptions—the nsa offering scholarships to hackers willing
to work at fort meade, israeli intelligence hiring jewish hackers from the
united states, washington offering security fellowships—and some hack-
ers have gone on to form upstanding and professional security
companies. recently, a handful of consulting companies have sprung up
to whitewash hackers and present them in a more respectable light.
and sometimes this works, but for many people it can be hard to tell the
ethical hackers from the criminals.
lone criminals
in april 1993, a small group of criminals wheeled a fujitsu model 7020
automated teller machine into the buckland hills mall in hartford,
connecticut, and turned it on. the machine was specially programmed
to accept atm cards from customers, record their account numbers and
pins, and then tell the unfortunate consumers that no transactions were
possible. a few days later, the gang encoded the stolen account numbers
and pins onto counterfeit atm cards, and started withdrawing cash
from atms in midtown manhattan. they were eventually caught when
46 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 46


 
 

____________________________________________

SecretsAndLies.pdf page: 75

____________________________________________

the bank correlated the use of the counterfeit atm cards with routine
surveillance films.
it was a shrewd attack, and much higher tech than most banking
crimes. one innovative criminal in new jersey attached a fake night
deposit box to a bank wall, and took it away early in the morning. 
it’s worse elsewhere. a few years ago, an atm was stolen in south
africa . . . from inside police headquarters in broad daylight.
lone criminals cause the bulk of computer-related crimes. sometimes
they are insiders who notice a flaw in a system and decide to exploit it;
other times they work outside the system. they usually don’t have much
money, access, or expertise, and they often get caught be cause of stupid
mistakes. someone might be smart enough to install a fake atm and col-
lect account numbers and pins, but if he brags about his cleverness in a
bar and gets himself arrested before cleaning out all the accounts . . . well,
it’s hard to have any sympathy for him. look at the two public internet
attacks of early 2000. someone manages to gain access to over ten
thousand credit card numbers, with names and addresses. the best crime
he can think of to do: extortion. someone else manages to control a large
number of distributed computers, ready to do his bidding. the best crime
he can think of: irritate major web sites.
lone criminals will target commerce systems because that’s where the
money is. their techniques may lack elegance, but they will steal money,
and they will cost even more money to catch and prosecute. and there
will be a lot of them.
malicious insiders
a malicious insider is a dangerous and insidious adversary. he’s already
inside the system he wants to attack, so he can ignore any perimeter
defenses around the system. he probably has a high level of access, and
could be considered trusted by the system he is attacking. remember the
russian spy aldrich ames? he was in a perfect position within the cia
to sell the names of u.s. operatives living in eastern europe to the kgb;
he was trusted with their names. think about a programmer writing
malicious code into the payroll database program to give himself a raise
every six months. or the bank vault guard purposely missetting the time
lock to give his burglar friends easy access. insiders can be impossible to
adversaries 47
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 47


 
 

____________________________________________

SecretsAndLies.pdf page: 76

____________________________________________

stop because they’re the exact same people you’re forced to trust.
here’s a canonical insider attack. in 1978, stanley mark rifkin was a
consultant at a major bank. he used his insider knowledge of (and access
to) the money transfer system to move several million dollars into a swiss
account, and then to convert that money into diamonds. he also pro-
grammed the computer system to automatically erase the backup tapes
that contained evidence of his crime. (he would have gotten away with
it, except that he bragged to his lawyer, who turned him in.)
insiders don’t always attack a system; sometimes they subvert a system
for their own ends. in 1991, employees at charles schwab in san fran-
cisco used the company’s e-mail system to buy and sell cocaine. a con-
victed child rapist working in a boston-area hospital stole a co-worker’s
password, paged through confidential patient records, and made obscene
phone calls.
insiders are not necessarily employees. they can be consultants and
contractors. during the y2k scare, many companies hired programmers
from china and india to update old software. rampant xenophobia aside,
any of those programmers could have attacked the systems as an insider.
most computer security measures—firewalls, intrusion detection sys-
tems, and so on—try to deal with the external attacker, but are pretty
much powerless against insiders. insiders might be less likely to attack a
system than outsiders are, but systems are far more vulnerable to them.
an insider knows how the systems work and where the weak points
are. he knows the organizational structure, and how any investigation
against his actions would be conducted. he may already be trusted by the
system he is going to attack. an insider can use the system’s own resources
against itself. in extreme cases the insider might have considerable exper-
tise, especially if he was involved in the design of the systems he is now
attacking.
revenge, financial gain, institutional change, or even publicity can
motivate insiders. they generally also fit into another of the categories: a
hacker, a lone criminal, or a national intelligence agent. malicious insid-
ers can have a risk tolerance ranging from low to high, depending on
whether they are motivated by a “higher purpose” or simple greed.
of course, insider attacks aren’t new, and the problem is bigger than
cyberspace. if the e-mail system hadn’t been there, the schwab employ-
48 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 48


 
 

____________________________________________

SecretsAndLies.pdf page: 78

____________________________________________

spying on the competition—but desperate times can bring desperate mea-
sures.
press
think of the press as a subspecies of industrial spy, but with different
motivations. the press isn’t interested in a competitive advantage over its
targets; it is interested in a “newsworthy” story. this would be the wash-
ington city pages publishing the video rental records of judge bork
(which led to the video privacy protection act of 1988), the british
tabloids publishing private phone conversations between prince charles
and camilla parker bowles, or a newspaper doing an exposé on this
company or that government agency.
it can be worth a lot of newspaper sales to get pictures of a presiden-
tial candidate like gary hart with a not-his-wife on his lap. even margin-
ally compromising photographs of princess di were worth over half a
million dollars. some reporters have said that they would not think twice
about publishing national security secrets; they believe the public’s right
to know comes first.
in many countries, the free press is viewed as a criminal. in such
countries, the press is usually not well funded, and generally more the
victim of attack than the attacker. journalists have gone to jail, been tor-
tured, and have even been killed for daring to speak against the ruling
government. this is not what i mean by the press as an attacker.
in industrial countries with reasonable freedoms, the press can bring
considerable resources to bear on attacking a particular system or target.
they can be well funded; they can hire experts and gain access. and if
they believe their motivations are true, they can tolerate risk. (certainly
the reporters who broke the watergate story fall into this category.)
reporters in the united states and other countries have gone to jail to
protect what they believe is right. some have even died for it.
organized crime
organized crime is a lot more than italian mafia families and francis ford
coppola movies. it’s a global business. russian crime syndicates operate
both in russia and in the united states. asian crime syndicates operate
50 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 50


 
 

____________________________________________

SecretsAndLies.pdf page: 80

____________________________________________

benevolent the country is and whether or not they hold occasional
democratic elections, “crimefighting” could cover a whole lot of things
not normally associated with law enforcement. maybe they’re more like
the press, but with better funding and a readership that only cares about
true crime stories. or maybe you can think of them as organized crime’s
industrial competitor.
in any case, police have a reasonable amount of funding and exper-
tise. they’re pretty risk averse—no cop wants to die for his beliefs—but
since they have the laws on their side, things that are risks to some groups
can be less risky to the police. (having a warrant issued, for example, turns
eavesdropping from a risky attack to a valid evidence-gathering tool.)
their primary goal is information gathering, with information that stands
up in court being more useful than information that doesn’t.
but police aren’t above breaking the law. the fundamental assump-
tion is that we trust the individual or some government to respect our
privacy and to only use their powers wisely. while this is true most of the
time, abuses are regular and can be pretty devastating. a spate of illegal
fbi wiretaps in florida and a subsequent cover-up got some press in
1992; the 150 or so illegal wiretaps by the los angeles police department
have gotten more. (drugs were involved, of course; more than one per-
son has pointed out that the war on drugs seems to be the root password
to the u.s. constitution.) j. edgar hoover regularly used illegal wiretaps
to keep tabs on his enemies. and 25 years ago a sitting president used
illegal wiretaps in an attempt to stay in power.
things seem to have improved since the days of hoover and nixon,
and i have many reasons to hope we won’t be back there again. but the
risk remains. technology moves slowly, but intentions change quickly.
even if we are sure today that the police will follow all privacy legislation,
eavesdrop only when necessary, obtain all necessary warrants, follow
proper minimization procedures, and generally behave like upstanding
public servants, we don’t know about tomorrow. the same kind of reac-
tive crisis thinking that led us to persecute suspected communists during
the mccarthy era could again sweep across the country. census data is,
by law, not supposed to be used for any other purpose. even so, it was
used during world war ii to round up japanese americans and put them
in concentration camps. the eerily named “mississippi sovereignty
commission” spied on thousands of civil rights activists in the 1960s. the
52 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 52


 
 

____________________________________________

SecretsAndLies.pdf page: 84

____________________________________________

going to look for reasons why it wasn’t its fault, and none of the “victims”
have said anything in public. still, the possibilities are disturbing.
and this kind of stuff is even worse in cyberspace. echelon is not
the only program that targets the internet. singapore and china eaves-
drop on internet traffic in their countries (china uses its national firewall,
the great wall). internet service providers across russia are helping the
main kgb successor agencies to read private e-mails and other internet
traffic, as part of an internal espionage program called sorm-2.
national intelligence organizations are not above using hacker tools,
or even hackers, to do their work. the israeli and japanese governments
both have programs to bring hackers into their country, feed them pizza
and jolt cola, and have them do intelligence work. other governments
go onto the net and taunt hackers, trying to get them to work for free.
“if you’re so good you’ll have the password to this government com-
puter”—that sort of thing works well if directed against a talented
teenager with no self-esteem. the cuckoo’s egg by clifford stoll is about
the exploits of three hackers who worked for the kgb in exchange for
cash and cocaine.
the techniques of national security agencies are varied and, with the
full weight of a nation behind them, can be very effective. british com-
munications security companies have been long rumored to build
exploitable features into their encryption products, at the request of
british intelligence. in 1997, cia director george tenet mentioned (in
passing, without details) using hacker tools and techniques to disrupt
international money transfers and other financial activities of arab
businessmen who support terrorists. the possibilities are endless.
infowarriors
yes, it’s a buzzword. but it’s also real. an infowarrior is a military
adversary who tries to undermine his target’s ability to wage war by
attacking the information or network infrastructure. specific attacks range
from subtly modifying systems so that they don’t work (or don’t work
correctly) to blowing up the systems completely. the attacks could be
covert, in which case they might resemble terrorist attacks (although a
good infowarrior cares less about publicity than results). if executed via
56 c h a p t e r  f o u r
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 56


 
 

____________________________________________

SecretsAndLies.pdf page: 85

____________________________________________

the internet, the attacks could originate from foreign soil, making detec-
tion and retaliation much more difficult.
this adversary has all the resources of a national intelligence organi-
zation, but differs in two important areas. one, he focuses almost exclu-
sively on the short-term goal of affecting his target’s ability to wage war.
and two, he is willing to tolerate risks that would be intolerable to long-
term intelligence interests. his objectives are military advantage and,
more generally, chaos. some of the particular targets that might interest
an infowarrior include military command and control facilities,
telecommunications, logistics and supply facilities and infrastructure
(think “commercial information systems”), and transportation lines (think
“commercial aviation”). these kinds of targets are called critical infrastruc-
ture.
in 1999, nato targeted belgrade’s electric plants; this had profound
effects on its computing resources. in retaliation, serbian hackers attacked
hundreds of u.s. and nato computer sites. chinese hackers crashed
computers in the department of the interior, the department of energy,
and the u.s. embassy in beijing in retaliation for our accidental bombing
of their embassy in belgrade. china and taiwan engaged in a little cyber-
war through most of 1999, attacking each other’s computers over the
internet (although this was probably not government coordinated on
either side).
in the past, military and civilian systems were separate and distinct:
different hardware, different communications protocols, different every-
thing. over the past decade, this has shifted; advances in technology are
coming too fast for the military’s traditional multiyear procurement cycle.
more and more, commercial computer systems are being used for military
applications. this means that all of the vulnerabilities and attacks that
work against commercial computers may work against militaries. and
both sides of a conflict may be using the same equipment and protocols:
tcp/ip, windows operating systems, gps satellite receivers. the u.s.
air force’s strategic air command (sac) recently switched to windows
nt on its external networks.
militaries have waged war on infrastructure ever since they started
waging war. medieval knights killed serfs, napoleonic armies burned
crops, allied bombers targeted german factories during world war ii.
(ball bearing factories were a favorite.) today, information is infra -
structure. during desert storm, the americans systematically destroyed
adversaries 57
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 57


 
 

____________________________________________

SecretsAndLies.pdf page: 87

____________________________________________

5
security needs
what kinds of security do we need, anyway? before examin-
ing (and often dismissing) specific countermeasures against
the threats we’ve already talked about, let’s stop and talk
about needs. in today’s computerized, international, interconnected,
interdependent world, what kind of security should we expect?
privacy
people have a complicated relationship with privacy. when asked to pay
for it, they often don’t want to. businesses also have a complicated rela-
tionship with privacy. they want it—they know the importance of not
having their dirty laundry spread all over the newspapers—and are even
willing to pay for it: with locks, alarms, firewalls, and corporate security
policies. but when push comes to shove and work needs to get done,
security is the first thing that gets thrown out the window. governments
are comfortable with privacy: they know the importance of not having
their military secrets in the hands of their enemies. they know they need
it, and know that they are going to have to pay dearly for it. and they
accept the burden that privacy puts on them. governments often get the
details wrong, but they grok the general idea.
almost no one realizes exactly how important privacy is in his or her
life. the supreme court has insinuated that it is a right guaranteed by the
constitution. democracy is built upon the notion of privacy; you can’t
59
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 59


 
 

____________________________________________

SecretsAndLies.pdf page: 90

____________________________________________

the few instances of very long privacy requirements i know of are
government related. u.s. census data—the raw data, not the compila-
tions—must remain secret for 72 years. the cia mandates that the
identities of spies remain secret until the spy is dead and all the spy’s
children are dead. canadian census data remains secret forever.
multilevel security
militaries have a lot of information that needs to be kept secret, but some
pieces of information are more secret than others. the locations of navy
ships might be of moderate interest to the enemy, but the launch codes
for the missiles on those ships are much more important. the number of
bedrolls in the supply chain is of marginal interest; the number of rifles is
of greater interest.
to deal with this kind of thing, militaries have invented multiple lev-
els of security classifications. in the u.s. military, data is either unclassi-
fied, confidential, secret, or top secret. rules govern what kind of data
falls into what classification, and different classifications have different
rules for storage, dissemination, and so forth. for example, different
strength safes are required for different classifications of data. top secret
data might only be stored in certain guarded, windowless, rooms without
photocopiers, and might need to be signed out.
people working with this data need security clearances commensurate
with the highest classification of information they are working with.
someone with a secret clearance, for example, can see information that is
unclassified, confidential, and secret. someone with a confidential
clearance can only see unclassified and confidential data. (of course,
clearance is not a guarantee of trustworthiness. the cia’s head russian
counterintelligence officer, aldrich ames, had a top secret security
clearance; he also was a russian spy.)
data at the top secret level or above is sometimes divided by topic,
or compartment. the designation “ts/sci,” for “top secret/special
compartmented intelligence,” indicates these documents. each compart-
ment has a codeword. talent and keyhole, for example, are the
keywords associated with the kh-11 spy satellites. silver, ruff,
teapot, umbra, and zarf are others. (umbra applies to com-
munications intelligence, and ruff applies to imagery intelligence.)
62 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 62


 
 

____________________________________________

SecretsAndLies.pdf page: 91

____________________________________________

compartments are topical access barriers; someone who has a top secret
clearance with an additional keyhole clearance (sometimes called a
“ticket”) is not authorized to see top secret cobra data.
these compartments are a formal codification of the notion of “need
to know.” just because someone has a certain level of clearance doesn’t
mean he automatically gets to see every piece of data at that clearance
level. he only gets to see the data that he needs to know to do his job.
and there are other designations that modify classifications: noforn is
“no foreign nationals,” wnintel is “warning notice, intelligence
sources and methods,” limdis is “limited dissemination.”
other countries have similar rules. the united kingdom has one
additional classification level, restricted, which falls between unclassified
and confidential. the united states has something similar called
fouo—for official use only—which means “unclassified, but don’t
tell anyone anyway.” 
two points are salient here. one, this kind of thing is much easier to
implement on paper than on computer. chapter 8 talks about some of the
multilevel security systems that have been built and used, but none of
them have ever worked on a large scale. and two, this kind of thing is
largely irrelevant outside a military setting. corporate secrets just don’t
work this way; neither do individuals’ secrets. security in the real world
doesn’t fit into little hierarchical boxes.
anonymity
do we need anonymity? is it a good thing? the whole concept of
anonymity on the internet has been hotly debated, with people weighing
in on both sides of the issue.
anyone who works on the receiving end of a crisis telephone line—
suicide, rape, whatever—knows the power of anonymity. thousands of
people on the internet discuss their personal lives in newsgroups for abuse
survivors, aids sufferers, and so on, that are only willing to do so
through anonymous remailers. this is social anonymity, and it is vital for
the health of the world, because it allows people to talk about things they
are unwilling to sign their name to. for example, some people posting
to alt.religion.scientology do so anonymously, and would not do so
otherwise.
security needs 63
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 63


 
 

____________________________________________

SecretsAndLies.pdf page: 97

____________________________________________

to trust, making sense of a complex world. even nonhumans need
authentication: smells, sounds, touch. arguably, life itself is an authenti-
cating molecular pit of enzymes, antibodies, and so on.
people authenticate themselves zillions of times a day. when you log
on to a computer system, you authenticate yourself to the computer. in
1997, the social security administration tried to put people’s data up on
the web; they shut down after complaints that social security number
and mother’s maiden name weren’t good enough authentication means,
that people would be able to see other people’s data. the computer also
needs to authenticate itself to you; otherwise, how to do you know it’s
your computer and not some impostor’s?
consider the average man on the street going to buy a bratwurst. he
examines storefront after storefront, looking for one that sells bratwurst.
or maybe he already knows his favorite bratwurst store, and just goes
there. in any case, when he gets to the store he authenticates that it is the
correct store. the authentication is sensory: he sees bratwurst on the
menu, he smells it in the air, the store looks like the store did the last time
he was there.
our man talks to the deli man and asks for a bratwurst. to some
degree, both authenticate each other. the deli man wants to know if the
customer is likely to pay. if the customer is dressed in rags, the deli man
might ask him to leave (or at least to pay beforehand). if the customer is
wearing a balaclava and brandishing an ak-47, the deli man might
simply run away.
the customer, too, is authenticating the deli man. is he a real deli
man? will he deliver me my bratwurst, or will he just give me a pile of
sawdust on my bun? what about the restaurant? there’s probably some
kind of certificate of cleanliness, signed by the local health inspector, on
the wall somewhere if the customer cares to check. more often, the
customer trusts his instincts. we’ve all walked out of restaurants because
we didn’t like the “feel” of the place.
the deli man hands over the bratwurst, and the customer hands over
a $5 bill. more authentication. is this bill authentic? is this bratwurst-
looking thing food? we’re so good at visual (and olfactory) authentication
that we don’t think about it, but we do it all the time. the customer gets
his change, checks to make sure it is legal tender, and puts it in his pocket.
if the customer paid using a credit card, there would be lot of behind-
the-scenes authentication. the deli man would swipe the card through a
security needs 69
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 69


 
 

____________________________________________

SecretsAndLies.pdf page: 98

____________________________________________

verifone reader, which would dial into a central server and make sure the
account was valid and had enough credit for the purchase. the deli man
would be expected to examine the card to make sure it isn’t a forgery, and
check the signature against the one on the back of the card. (most
merchants don’t bother, especially for low-value transactions.)
if the customer paid by check, there would be another authentication
dance. the deli man would look at the check, and possibly ask the
customer for some identification. then he might write the customer’s
driver’s license number and phone number on the back of the check, or
maybe the customer’s credit card number. none of this will actually help
the deli man collect on a bad check, but it does help him track the
customer down in the event of a problem.
attacking authentication can be very profitable. in 1988, thompson
sanders was convicted of defrauding the chicago board of trade. he
synthesized a nonexistent trader, complete with wig, beard, and fake cre-
dentials. this fake trader would place large risky orders, then claim those
that were profitable and walk away from those that were not. the brokers
on the other side of the losing transactions, unable to prove who they
made the trade with, would be responsible for the losses.
back to the deli. another customer walks in. she and the deli man are
old friends. they recognize each other—authenticating each other by
face. this is a robust authentication system; people recognize each other
even though she has a new hairstyle and he is wearing a new toupee and
glasses. superheroes realize this, and wear masks to hide their secret iden-
tity. that works better in comic books than in real life, because face-to-
face authentication isn’t only face recognition (otherwise the blind would
never recognize anyone). people remember each other’s voice, build,
mannerisms, and so forth. if the deli man called his friend on the phone,
they could authenticate each other without any visual cues at all.
commissioner gordon ought to figure out that bruce wayne is really
batman, simply because they talk on the phone so often.
in any case, our bratwurst-filled customer finishes eating. he says
goodbye to the deli man, sure in the knowledge that he is saying good-
bye to the same deli man who served him his bratwurst. he leaves
through the same door that he came in by, and goes home.
70 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 70


 
 

____________________________________________

SecretsAndLies.pdf page: 99

____________________________________________

easy enough, because everyone involved was there . . . in the deli.
plato (and hume) distrusted writing because you couldn’t know what
was true if the person wasn’t right there in front of you. what would he
say about the world wide web: no handwriting, no voice, no face . . .
nothing but bits.
the same customer who bought the bratwurst is now surfing the
net, and he wants to buy something a little less perishable: a painting of a
bratwurst, for example. he fires up his trusty search engine and finds a few
web sites that sell bratwurst paintings. they all take credit cards over the
internet, or let him mail a check in. they all promise delivery in three to
four days. now what?
how does the poor customer know whether to trust them? it takes
some doing to put up a storefront; on the web, anyone can do it in a few
hours. which of these merchants are honest, and which are scams? the
url might be that of a trusted name in the bratwurst-painting business,
but who’s to say that the url is owned by that same trusted name?
northwest airlines has a web site where you can purchase  tickets:
www.nwa.com. until recently, a travel agent had the web site
www.northwest-airlines.com. how many people bought from the latter,
thinking they were buying from the former? (many companies do not
own their namesake domain name.) some companies embed their com-
petitors’ names in their web site (usually hidden) in an effort to trick
search engines to point to them instead of their competitors. internic.net,
which is where you go to register domain names, is not the same as inter-
nic.com. the latter started out as a spoof, morphed into internic soft-
ware, and now registers domain names as well. they probably get a
considerable business from the confused. and there’s an even more
sinister thought: who’s to say that some illicit hacker hasn’t convinced the
browser to display one url while pointing to another?
the customer finds a web site that looks reasonable and chooses a
bratwurst painting. he then has to pay the merchant. if he’s buying any-
thing of value, we are going to need some serious authentication here.
(if he’s spending 25 cents for a virtual newspaper, it’s a little easier to let
this slide.) is this digital cash valid? is this credit card valid, and is the
customer authorized to use it? is the customer authorized to write a digi-
tal check? some face-to-face merchants ask to see a driver’s license before
security needs 71
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 71


 
 

____________________________________________

SecretsAndLies.pdf page: 101

____________________________________________

think of a merchant checking a $100 bill to make sure it’s not counter-
feit, or comparing the signature on a credit card with the signature on the
sales slip.
integrity
sometimes when we think of authentication, we really mean integrity.
the two concepts are distinct but sometimes confused. authentication
has to do with the origin of the data: who signed the license to practice
medicine, who issued the currency, who authorized this purchase order
for 200 pounds of fertilizer and five gallons of diesel fuel? integrity has to
do with the validity of data. are these the correct payroll numbers? has
this environmental test data been tampered with since i last looked at it?
integrity isn’t concerned with the origin of the data—who created it,
when, or how—but whether it has been modified since its creation.
integrity is not the same as accuracy. accuracy has to do with a
datum’s correspondence to the flesh-and-blood world; integrity is about a
datum’s relation to itself over time. they are often closely related.
in any society where computerized data are going to be used to make
decisions, the integrity of the data is important. sometimes it is important
on an aggregate scale: if that faulty statistic about children below the
poverty line is accepted as fact, it could change the amount of federal aid
spent. someone who fiddles with the closing prices for a handful of nas-
daq stocks could make a killing on the resultant confusion. sometimes
it is important to an individual: you can really mess up someone’s day
tampering with his dmv records and marking his license as suspended.
(this was accidentally done in 1985 in anchorage, alaska, to 400 people,
at least one of whom had to spend the night in jail. think of the fun
someone could have doing it on purpose.)
there have been several integrity incidents regarding stocks. in 1997,
a company called swisher that makes toilet bowl deodorizers got a big
boost to its stock prices because the news services kept mixing up its stock
symbol with that of another company called swisher, which makes cigars.
swisher(1) was a much smaller company than swisher(2), so when you
plugged in the mistaken earnings figures, it looked like an incredibly
undervalued stock. some guys on the motley fool web site figured out
security needs 73
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 73


 
 

____________________________________________

SecretsAndLies.pdf page: 102

____________________________________________

what had happened and sold swisher(1)’s stock short, figuring it would
come back down when investors realized their mistake.
in 1999, an employee of pairgain technologies posted fake takeover
announcements designed to look like they came from the bloomberg
news service, running the stock up 30 percent before the hoax was
exposed.
these attacks are not about authentication—it doesn’t matter who
collected the census data, who compiled the closing stock prices, or who
input the motor vehicle records—they’re about integrity. there are many
other databases where integrity is important: telephone books, medical
records, financial records, and so on.
if there’s a mystery writer in the audience, i always thought that a
cool way to murder someone would be to modify the drug dosage data-
base 
in a hospital. if the physician isn’t paying close enough attention—he’s
tired, the drug is an obscure one, some macguffin is distracting him—
he might just prescribe what the computer tells him to. this might be far-
fetched today—there’s still a lot of reliance on hard-copy documentation
like the physician’s desk reference and ahfs drug information—but it
won’t be soon. millions of people are getting medical information on line.
for example, drugemporium.com queries another site, drkoop.com, 
to search for any harmful drug interactions among the products in your
order (which can include prescription drugs). users are admonished not
to rely on this information alone, but most of them probably will anyway.
someone playing with the integrity of that data can cause a lot of harm.
and even if no malice is involved, any online system that deals with
prescriptions and treatments had better implement integrity checking
against random errors: no one wants a misplaced byte to result in an acci-
dental hospital death, neither the patient nor the software company who
is going to have to deal with the lawsuits.
in the physical world, people use the physical instantiation of an
object as proof of integrity. we trust the phone book, the physician’s desk
reference, and the u.s. statistical abstracts because they are bound books
that look real. if they are fake, someone is spending a lot of money mak-
ing them look real. if you pull a dickens novel off the shelf and start read-
ing it, you don’t think twice about whether it is real or not. the same
with a clipping from business week; it’s just a piece of paper, but it looks
74 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 74


 
 

____________________________________________

SecretsAndLies.pdf page: 103

____________________________________________

and feels like a page from the magazine. if you get a photocopy of the
clipping, then it just looks like a page from the magazine. if someone
retypes the article (or downloads it from lexis-nexis) and e-mails it to
you . . . then who knows.
on august 1, 1997, i received an e-mail from a friend; in it was a
copy of kurt vonnegut’s 1997 mit commencement address. at least, i
assumed it was vonnegut’s 1997 mit commencement address. my friend
mailed it to me in good faith. but it wasn’t kurt vonnegut’s 1997 mit
commencement address. vonnegut didn’t deliver the 1997 commence-
ment address at mit. he never wrote the speech, or delivered it any-
where. the words were written by mary schmich, and published in her
june 1, 1997, chicago tribune column.
contrast that with another piece of alleged vonnegut writing i
received, about 15 years previous. this was before the world wide web,
before i even had an e-mail address (but not before the internet). this was
an essay entitled “a dream of the future (not excluding lobsters)”; a
friend sent a photocopy in the mail. the copy was clearly from a publica-
tion. yes, it could have been faked, but it would have been a lot of work.
this was before the era of desktop publishing, and making something
look like it was photocopied out of esquire magazine was difficult and
expensive. today it’s hard to tell the difference between the real thing and
a canard.
i’ve been e-mailed articles from magazines and newspapers many
times. what kind of assurance do i have that those articles are really from
the newspapers and magazines they are claimed to be from? how do i
know that they haven’t been subtly modified, a word here and a sentence
there? what if i make this book available online, and some hacker comes
in and changes my words? maybe you’re reading this book online; did
you ever stop to think that these might not be my actual words, that
you’re trusting the server you downloaded the book from? is there a
mechanism that you can use to verify that these are my words? if enough
years go by, more people will have read the altered version of the book
than my original words. will anyone ever notice? how long before the
modified version becomes the “real” version? when will vonnegut’s
denial be forgotten and his commencement address become history?
the temptation to falsify, or modify, data remains. a rune-covered
stone discovered in minnesota supposedly described a visit by the vikings
in 1362; never mind that it contained a word only found in modern
security needs 75
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 75


 
 

____________________________________________

SecretsAndLies.pdf page: 104

____________________________________________

swedish. paul schliemann (heinrich schliemann’s grandson) claimed to
have discovered the secret of atlantis in the ancient mayan troano
codex, which he read in the british museum. never mind that no one
could read mayan, and that the codex was stored in madrid. bismarck’s
rewrite of the 1870 ems telegram effectively started the franco-prussian
war. in 1996, when david selbourne tried to pass off his translation of a
thirteenth-century italian traveler’s visit to china (beating marco polo by
three years); he used the “owner of the manuscript allowed him to
translate it only if he swore himself to secrecy” trick to avoid having to
produce a suitable forgery.
the problem is that the digital world makes this kind of thing easier,
because it is so easy to produce a forgery and so hard to verify the accu-
racy of anything. in may 1997, a 13-year-old brooklynite won a national
spelling bee. when the new york post published the associated press
photo of her jumping for joy, it erased the name of her sponsoring news-
paper, the new york daily news, from a sign around her neck. video, too:
when cbs covered the 2000 new year celebration, they digitally
superimposed their own logo over the 30-by-40-foot nbc logo in
times square. and fake essays and speeches, like the vonnegut speech,
are posted on the internet all the time.
images can have powerful effects on people. they can change minds
and move foreign policy. desert storm pictures of trapped iraqis being
shot up by coalition airpower played a large part in the quick cease-fire:
americans didn’t like seeing the lopsided carnage. and remember soma-
lia? all it took was a 30-second video clip of a dead marine being dragged
through the streets of mogadishu to undermine the american will to
fight. information is power. and next time, the video clip could be a fake.
it sounds spooky, but unless we pay attention to this problem we will
lose the ability to tell the real thing from a fake. throughout human
history, we’ve used context to verify integrity; the electronic world has no
context. in the movie the sting, newman and redford hired a cast of
dozens and built an entire fake horseracing-betting parlor in order to con
one person. a more recent movie, the spanish prisoner, had a similar big
con. cons this involved were popular around the time of the depression;
for all i know it’s still done today. the mark is taken because he can’t
imagine that what he’s seeing—the rooms, the people, the noise, the
action—is really only a performance enacted solely for his benefit.
76 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 76


 
 

____________________________________________

SecretsAndLies.pdf page: 114

____________________________________________

tography is a boatload of acronyms that accomplish various security tasks.
ipsec, for example, secures ip traffic across the internet. it secures virtual
private networks (vpns). secure sockets layer (ssl) secures www
connections. pretty good privacy (pgp) and s/mime secure e-mail;
they prevent others from reading e-mail that isn’t addressed to them, and
from forging e-mail to look like it came from someone else. set secures
internet credit card transactions. these are all protocols. there are proto-
cols for digital content protection (music, movies, etc.), cell phone
authentication (to stop fraud), electronic commerce, and just about every-
thing else. to build these protocols, cryptographers use different algo-
rithms: encryption algorithms, digital signature algorithms, and so forth.
symmetric encryption
historically, cryptography has been used for one thing: to keep secrets.
written language itself has been used as a form of cryptography—in
ancient china only the upper classes were allowed to learn to read and
write—but the first documented use of cryptography was around 1900
b.c. in egypt: a scribe used nonstandard hieroglyphs in an inscription.
there were other examples: a mesopotamian tablet from 1500 b.c. con-
taining an enciphered formula for making pottery glazes, the hebrew
atbash cipher from 500–600 b.c., the greek skytale from 486 b.c.,
and julius caesar’s simple substitution cipher from 50–60 b.c. the kama
sutra of vatsyayana even lists secret writing as the 44th, and secret talking
as the 45th, of 64 arts (yogas) men and women should know and practice.
the main idea behind cryptography is that a group of people can use
private knowledge to keep written messages secret from everyone else.
there is a message, sometimes called the plaintext, that someone wants to
keep secure. maybe the someone (we’ll call her alice) wants to send it to
someone else (we’ll call him bob); maybe she wants to be able to read it
herself at some later date. what she doesn’t want is for anyone other than
(possibly) bob to be able to read the message.
so alice encrypts the message. she invents some transformation,
called an algorithm, of the plaintext message into a ciphertext message.
this ciphertext message is gibberish, so that an eavesdropper (we’ll call
her eve) who gets her hands on this ciphertext cannot figure out the
86 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 86


 
 

____________________________________________

SecretsAndLies.pdf page: 116

____________________________________________

yourself (you wouldn’t trust anyone else to do it), and hope you’re
smarter than everyone who might try to break your algorithm.
not bloody likely.
such is the beauty of a key. your front door lock is mass-produced by
some faceless company that hasn’t the faintest idea how valuable your
vintage pez collection is, but you don’t have to trust them. they don’t
say: “remember, anyone else who has the same brand lock can open the
lock.” you have a key. the pin settings inside your lock, which match
your key, make your lock different from all the other locks in the neigh-
borhood, even though they might be exactly the same make and model
number. (actually, the example is simplistic. you do have to trust them
to install the lock correctly, and not to pocket an extra copy of the key.
but never mind that.)
this is the same security model that leon battista alberti, the famous
italian renaissance architect, brought to cryptography in 1466 when he
invented the cryptographic key. everyone can have the same brand lock,
but everyone has a different key. the design of the lock is public—lock-
smiths have books with detailed diagrams, and most of the good designs
are described in public patents—but the key is secret. you have a key, so
you can get in your front door. if you give a key to your friend, he can
get in your front door. someone without a key cannot. (the locksmiths
are the cryptanalysts; we’ll get to them later.)
applying this model to cryptography solves both of the preceding
problems. algorithms, like locks, can be standardized. the data encryp-
tion standard (also called des) has been a standard cryptographic algo-
rithm, worldwide, since 1977. it’s been used in thousands of different
products for all sorts of applications. the innermost workings of des
have been public from day one; they were published even before it was
adopted as a standard. the public nature of the algorithm doesn’t affect
security, because each different group of users chooses its own secret key.
alice and bob share the same key, so they can communicate. eve doesn’t
know the key, so she can’t read their communications—even though she
has a copy of the exact same encryption software that alice and bob have.
keys solve the problem of people moving in and out of a private
group. if alice and bob share a key, and they want to let kim philby join
their conversations, they just give him a copy of the key. if they later learn
that philby is passing secrets to the soviet union, they can simply agree
88 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 88


 
 

____________________________________________

SecretsAndLies.pdf page: 117

____________________________________________

on a new key and not tell philby. from that moment forward, he is cut
out of the system and can no longer read newly encrypted messages. (of
course, he can still read the old ones.)
this is the way conventional cryptography works today. the algo-
rithms are designed for computers instead of pencil and paper—they
operate on binary bits instead of alphabetic characters, they’re designed
with the efficiencies of microprocessors and integrated circuits in mind—
but the philosophy is the same. the algorithm is public, and the commu-
nicating parties agree on a shared secret key to use with the algorithm.
these algorithms are called symmetric because the sender and receiver
must share the same key. the key is a string of random bits of some
length: in the year 2000, 128 bits is a good key length. different symmet-
ric algorithms have different key lengths.
symmetric algorithms can be found in encryption systems all over the
computerized world. common algorithms are des and triple-des, rc4
and rc5, idea, and blowfish. aes is the advanced encryption
standard; it will soon be the u.s. government standard encryption algo-
rithm. these algorithms secure private e-mail, personal computer files,
electronic banking transactions, and nuclear launch codes. they protect
privacy.
but they’re not perfect.
the problem is distributing the keys. for this system to work, alice
and bob need to agree on a secret key before exchanging any secret
messages. if alice and bob are smart, they are going to change their key
routinely: daily, perhaps. they need to agree on these daily keys in some
secure manner, since anyone who eavesdrops on the key can eavesdrop
on all communications encrypted with that key. and assuming you want
pairwise security, the number of keys needed grows with the square of the
number of users: two users need just one key, but a ten-user network
needs 45 keys to allow every pair of users to communicate securely. and
a 100-user network needs 4,950 different keys. in the 1980s, u.s. navy
ships would often sail with a forklift-full of nsa- distributed keys—each
printed on paper tape or punch cards or whatever—enough for all of their
communications circuits for the entire length of their missions.
and it isn’t enough to disseminate these keys securely: they have to
be stored securely, used securely, and then destroyed securely. alice and
bob need to keep their keys secret until they need to talk with one
cryptography 89
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 89


 
 

____________________________________________

SecretsAndLies.pdf page: 118

____________________________________________

another and they need to make sure that no one gets their keys, either
before they use them, while they are using them, or after they have used
them.
this means that destruction is critical. alice and bob can’t just toss
their key in the dumpster in the back and hope no one finds it. eaves-
droppers are not above storing encrypted communications that they can’t
read, hoping that they will find the key at some later date. the nsa’s
decryption of the russian venona traffic (look up the story; it’s cool
beans) was possible only because the soviets reused keys that should have
been thrown away, and because the nsa stored the soviet encrypted
messages for over a decade.
there are many historical examples of poor key management break-
ing otherwise strong encryption. john walker was in the u.s. navy, but
he had a second career photocopying u.s. navy key material before it
was used and then mailing it to the russians—and he was a security
officer entrusted with keeping the keys secure. the japanese death cult
aum shinrikyo encrypted their computer records, but they were careless
enough to leave a copy of the key on a floppy disk for the police to find.
and this was in 1995; you’d think death cults would have learned a thing
or two by then.
types of cryptographic attacks
what does it mean to break an algorithm? obviously, it means that some-
one can read the message without the key. but it’s more complicated than
that.
if an attacker can take a ciphertext message and recover the plaintext,
this is called a ciphertext-only attack. this almost never occurs anymore;
modern algorithms are just too good to fall to this kind of attack.
a known-plaintext attack is more likely: the analyst has a copy of the
plaintext and the ciphertext, and can then recover the key. this might
sound useless, but it in fact can be very useful. if other texts are encrypted
with the same key, the attacker can take the key and read more plaintext
encrypted with it. for example, almost all computer files have known
headers. all microsoft word files, for example, start with the same
hundreds of bytes. (these are not the characters you see; these bytes are
90 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 90


 
 

____________________________________________

SecretsAndLies.pdf page: 121

____________________________________________

integrity. they ensure that the message came from the person from
whom it purports to have come from (authentication), and that the
message was not altered in transit (integrity).
you can think of a mac as a tamperproof coating on a message.
anyone can read the message; the coating doesn’t provide privacy. but
someone who knows the mac key can verify that the message has not
been altered. more specifically, a mac is a number that is appended to a
digital message.
macs use a shared secret key, just like symmetric encryption algo-
rithms. first, alice shares a key with bob. then, when she wants to send
a message to bob, she computes the mac of the message (using the secret
key) and appends it to the message. every message has a unique mac for
each possible key.
when bob receives the message, he computes its mac (again, using
the same shared secret key) and compares it with the mac he received
from alice. if they match, then he knows two things: the message really
does come from alice (or someone who knows the secret of the shared
key), because only that key could be used to compute the mac; and that
the message is complete and unaltered, because the mac could only be
computed from the entire and exact message. if eve (remember our
eavesdropper?) was listening in on the communications, she could read
the message. however, if she tried to modify either the message or the
mac, then bob’s calculated mac would not equal the mac he
received. eve would have to modify the message and then modify the
mac to be correct for the new message, but she can’t do that because she
doesn’t know the key. banks have used this simple authentication system
for decades.
alice can use this same trick to authenticate information stored in a
database. when she adds the information to the database, she calculates
the mac and stores it with the information. when she retrieves the
information, she again calculates the mac and compares it with the
mac stored in the database. if they match, she knows that no one has
modified the information.
macs are used on the internet all the time. they’re used in the ipsec
protocol, for example, to ensure that ip packets have not been modified
between when they are sent and when they reach their final destination.
they’re used in all sorts of interbank transfer protocols to authenticate
messages. most macs are constructed using symmetric algorithms or
cryptography 93
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 93


 
 

____________________________________________

SecretsAndLies.pdf page: 123

____________________________________________

so they can use a symmetric encryption algorithm or a mac function?
public-key cryptography (a.k.a. asymmetric encryption) solves this. it allows you
to send secret messages to people you haven’t met yet, and with whom
you haven’t agreed on a secret key. it allows two people to engage in a
publicly visible data exchange and, at the end of that exchange, compute
a shared secret that someone listening in on the discussion can’t learn. in
real-world terms, it allows you and a friend to shout numbers at each
other across a crowded coffeehouse filled with mathematicians so that
when you are done, both you and your friend know the same random
number, and everyone else in the coffeehouse is completely clueless.
if this sounds ridiculous, it should. it sounds impossible. if you were
to survey the world’s cryptographers in 1975, they would all have told
you it was impossible. so you can imagine the surprise in 1976, when
whitfield diffie and martin hellman explained how to do it. or the sur-
prise in the british intelligence community when james ellis, clifford
cocks, and m.j. williamson figured out the same thing a few years
before.
the basic idea is to use a mathematical function that is easy to com-
pute in one direction and hard to compute in the other. integer factoriza-
tion is one. given two prime numbers, it’s easy to multiply them together
to find the product. but given a single product, it can be impracticable to
factor the number and recover the two factors. this is the kind of math
that can be used to create public-key cryptography; it involves modular
arithmetic, exponentiation, and large prime numbers thousands of bits
long, but you can elide the details. today, there are a good half-dozen
algorithms, with names like rsa, elgamal, and elliptic curves. (algo-
rithms based on something called the knapsack problem were another
early contender, but over the course of about 20 years they were broken
every which way.) the mathematicals are different for each algorithm,
but conceptually they are all the same.
instead of a single key that alice and bob share, there are two keys:
one for encryption and the other for decryption. the keys are different,
and it is not possible to compute one key from the other. that is, if you
have the encryption key, you can’t figure out what the decryption key is.
now, here’s the cool part. bob can create a pair of these keys. he can
take the encryption key and publish it. he can send it to his friends, post
it on his web site, publish it in a phone book, whatever. alice can find
this key. she can take it and encrypt a message to bob. then, she can send
cryptography 95
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 95


 
 

____________________________________________

SecretsAndLies.pdf page: 124

____________________________________________

the message to him. bob can use his decryption key (which he astutely did
not post on his web site) to decrypt and read alice’s message. notice that
alice did not have to meet bob in some dark alley and agree on 
a shared secret. bob doesn’t even have to know alice. actually, alice
 doesn’t even have to know bob. if alice can find bob’s public key, she
can send him a secret message that can’t be read by anyone but bob. this
happens to pgp users all the time; one of their keys is uploaded to a server
somewhere, and then a perfect stranger sends them an encrypted message.
even if you understand the mathematics, it can be startling.
the particulars are a whole lot more subtle. for example, i left out
how bob creates his public and private keys, and how bob keeps his
private key secret. (he can’t remember it; it’s over a thousand random
digits long.) and i skipped over the incredibly complicated problem of
how alice knows that she has bob’s key and not some old key or, worse
yet, some impostor’s key. we’ll get back to this later.
for now, i just want to point out that no one uses public key encryp-
tion to encrypt messages. all operational systems use a hybrid approach
that uses both kinds of cryptography. the reason is performance. what
alice really does, when she wants to send a message to bob, is to use a
symmetric algorithm to encrypt the message with a random key that she
creates out of thin air (called a session key). she encrypts that random key
with bob’s public key, and then sends both the encrypted key and the
encrypted message to bob. when bob receives the encrypted message
and key, he does the reverse. he uses his private key to decrypt the ran-
dom symmetric key, and then uses the random symmetric key to decrypt
the message.
this might sound weird, but it isn’t. it’s perfectly normal. nobody
uses public-key cryptography to directly encrypt messages. everyone uses
this hybrid approach. it’s in every e-mail security program: pgp, pem,
s/mime, whatever. it’s how encryption works with web security,
tcp/ip security, secure telephones, and everything else.
digital signature schemes
public-key encryption was amazing enough, but digital signatures are
even more splendiferous—and more important. digital signatures
provide a level of authentication for messages, similar to macs. and in
96 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 96


 
 

____________________________________________

SecretsAndLies.pdf page: 126

____________________________________________

that the message came from either bob or alice; after all, both of them
knew the mac key. macs can be used to convince the receiver that the
message came from the sender, but it cannot be used to convince a third
party. digital signatures can be used to convince a third party, which
solves the nonrepudiation problem: alice cannot send a message to bob,
and then later deny ever sending it.
the unfortunate reality is that this stuff about signatures is not as black
and white as the math implies. digital signature laws are on the books in
many states and countries, but i worry that they won’t survive litigation.
digital signatures are not analogues of handwritten signatures. i will talk
more about this in chapter 15.
random number generators
random numbers are the least-talked-about cryptographic primitive, but
are no less important than the others. almost every computer security sys-
tem that uses cryptography needs random numbers—for keys, unique val-
ues in protocols, and so on—and the security of those systems is often
dependent on the randomness of those random numbers. if the random
number generator is insecure, the entire system breaks.
depending on who you talk to, generating random numbers from a
computer is either trivial or impossible. theoretically, it’s impossible.
john von neumann, the father of computers, said: “anyone who consid-
ers arithmetic methods of producing random digits is, of course, in a state
of sin.” what he means is that it is impossible to get something truly
random out of a deterministic beast like a computer. this is true, but
luckily we can get by anyway. what we really need out of a random
number generator is not that the numbers be truly random, but that they
be unpredictable and irreproducible. if we can get those two things, we
can get security.
on the other hand, if we mess those two things up, we get insecurity.
in 1994, the casino montreal used a computer’s random number gener-
ator for its keno drawings. one observant gambler who spent way too
much time in the casino noticed that the winning numbers were the same
every day. he successfully picked three successive jackpots and won
$600,000. (after much wringing of hands, gnashing of teeth, and investi-
gations, the casino paid up.)
98 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 98


 
 

____________________________________________

SecretsAndLies.pdf page: 131

____________________________________________

keting literature proclaims that “we use rsa,” or triple-des, or
whatever cryptographic algorithms are in vogue. it’s like advertising a
house as completely safe just because it has a certain brand of door lock.
it’s just not enough.
key length and security
despite what i said last chapter, key length has almost nothing to do with
security.
the lock on the front door of your house has a series of pins in it.
each of the pins has multiple possible positions. when someone inserts a
key into the lock, the pins are each moved to specific positions. if the
positions dictated by the key are the ones that the lock needs to open, it
does. otherwise, it doesn’t.
most residential locks have five pins, each of which can be in one of
ten different positions. that means that there are 100,000 possible keys. a
burglar with a gargantuan key ring can try every possible key, one after
the other, and eventually he will get in. he had better be patient, because
even if he can try a new key every five seconds, it will take him an
average of 69 hours to find the correct key—and that doesn’t include
bathroom, meal, or sleep breaks.
one day a salesman knocks on your door, and offers to sell you a new
lock. his lock has seven pins with twelve positions each. a burglar, he
tells you, will have to try different keys for almost three years, nonstop,
before he will be able to open your door. do you feel more secure with
this lock?
probably not. no burglar would ever stand at your doorstep for 69
hours anyway. he’s more likely to pick the lock, drill it out, kick the door
down, break a window, or just hide in the bushes until you saunter up the
front walk. a lock with more pins and positions won’t make your house
more secure, because the specific attack it makes more difficult—trying
every possible key—isn’t one you’re particularly worried about. as long
as there are enough pins to make that attack infeasible, you don’t have to
worry about it.
the same is true for cryptographic keys. if they are long enough,
brute-force attacks are simply beyond the capabilities of human engineer-
ing. but there are two worries. the first is the quality of the encryption
cryptography in context 103
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 103


 
 

____________________________________________

SecretsAndLies.pdf page: 132

____________________________________________

algorithm, and the second is the quality of the keys. how long is “long
enough” is more complicated than a simple number; it depends on both
of these things.
but first i need to explain about entropy.
entropy is a measure of disorder; or, more specifically in the context
of cryptography, it is a measure of uncertainty. the more uncertain some-
thing is, the more entropy in that thing. for example, if a random person
from the general population is either male or female, the variable “gen-
der” has one bit of entropy. if a random person prefers one of the four
beatles, and each is equally likely, that corresponds to two bits of entropy.
the sex of someone on a women’s olympic running team has no
entropy; everyone is female. the entropy of the beatle-preference at a
john lennon fan club meeting has much less than two bits, because it is
more likely that a random person will prefer john. the more certainty in
the variable, the less the entropy.
the same is true for cryptographic keys. just because an algorithm
accepts 128-bit keys does not mean it has 128 bits of entropy in the key.
or, more exactly, the best way to break a given implementation of a 128-
bit encryption algorithm might not be to try every possible key. the “128
bits” is simply a measure of the maximum amount of work required to
break the algorithm and recover the key; it says nothing about the mini-
mum.
the first worry is the source of the keys. all the key-length calcula-
tions i just made assume that each key has maximum entropy when it is
generated. in other words, i assumed that each key is equally likely: that
the random number generator that created the keys was perfect. this just
isn’t true.
many keys are generated from passwords or passphrases. a system that
accepts 10-character ascii passwords might require 80 bits to represent,
but has much less than 80 bits of entropy. high-order ascii bits won’t
appear at all, and passwords that are real words (or close to real words) are
much more likely than random character strings. i’ve seen entropy esti-
mates of standard english at less than 1.3 bits per character; passwords
have less than 4 bits of entropy per character. this means that an 8-char-
acter password is about the same as a 32-bit key, and if you want a
128-bit key, you are going to need a 98-character english passphrase.
you see, a smart brute-force password-cracking engine isn’t going to
try every possible key in order. it’s going to try the most likely ones first,
104 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 104


 
 

____________________________________________

SecretsAndLies.pdf page: 134

____________________________________________

it has a particular key length. but does the algorithm actually deliver the
entropy that it claims to? it might take years of analysis before we trust that
it does. and even then we could easily be wrong; new mathematics could
be invented that reduce the algorithm’s entropy and break it. this is also
why products that advertise thousand-bit keys are hard to take seriously;
their promoters don’t understand how keys and entropy work.
a similar issue exists with physical keys and locks. i used to know a
locksmith who would carry large key rings around in his truck. it might
require 10,000 keys to open all the locks, but in reality a few dozen keys
would open all the locks of a particular manufacturer. sometimes he
would have to slide the keys around a bit—note the combination of
analysis and a brute-force attack—but it would work. tedious yes, but
nowhere near as tedious as trying all 10,000 possible keys (older cars have
four-pin locks). the actual security of door locks was nowhere near the
theoretical maximum.
it’s the same with combination locks. you can try every possible
combination—and there are brute-force safecracking machines that do
that—or you can be smarter about it. modern safecracking machines use
a microphone to listen to the dials as they turn, and can open a safe much
faster than brute force.
this makes choosing an encryption algorithm very important. i
discuss this in more detail at the end of the chapter.
one-time pads
one-time pads are the simplest of all algorithms, and were invented early
on in the 20th century. the basic idea is that you have a pad of key let-
ters. you add one key letter to each plaintext letter, and never repeat the
key letters. (that’s the “one-time” part.) for example, you add b (2) to
c (3) to get e (5), or t (20) to l (12) to get f (6). 20 + 12 = 6 mod 26.
this system works with any alphabet, including a binary one. and it’s the
only provably secure algorithm we’ve got.
recall the concept of unicity distance. the unicity distance grows
with the length of the key. as the key length approaches the length of the
message, the unicity distance approaches infinity. this means that it is
impossible to recognize plaintext, and why a one-time pad is provably
106 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 106


 
 

____________________________________________

SecretsAndLies.pdf page: 136

____________________________________________

want to build a secure telephone? use public-key cryptography to
generate a random session key, and then use symmetric cryptography and
that session key to encrypt the conversation. a hash function provides
added security against man-in-the-middle attacks. (more about those
later.) to secure e-mail, use public-key cryptography for privacy and dig-
ital signature schemes for authentication. electronic commerce? usually
nothing more than digital signatures and sometimes encryption for
privacy. a secure audit log: combine a hash function, encryption, maybe
a mac, and stir.
what we’re doing here is building protocols. a protocol is nothing
more than a dance. it’s a series of predetermined steps, completed by two
or more people, designed to complete a task. think of the protocol used
by a merchant and a customer for purchasing a tangerine. here are the
steps:
1. the customer asks the merchant for a tangerine.
2. the merchant gives the customer a tangerine.
3. the customer gives the merchant money.
4. the merchant gives the customer change.
everyone involved in the protocol must know the steps. for exam-
ple, the customer knows he has to pay for the tangerine. all steps must be
unambiguous; neither the merchant nor the customer can reach a step
where they don’t know what to do. and the protocol has to terminate;
there can be no endless loops.
there’s also a certain amount of processing by the parties. for exam-
ple, step 2 won’t work properly unless the merchant understands the
semantic content of step 1. the merchant won’t complete step 4 unless
she recognizes the money as real in step 3. try buying a tangerine in the
united states with polish zlotny and see how far you get.
the particular protocols we’re concerned about are secure protocols.
in addition to the preceding requirements, we don’t want either the cus-
tomer or the merchant to be able to cheat (whatever “cheat” means in this
context). we don’t want the merchant to be able to peek into the
customer’s wallet in step 3. we don’t want the merchant to be able to not
give the customer change in step 4. we don’t want the customer to be
able to shoot the merchant dead in step 3 and walk away with a stolen
108 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 108


 
 

____________________________________________

SecretsAndLies.pdf page: 137

____________________________________________

tangerine. these cheats are possible in the physical world, and the
anonymity of cyberspace exacerbates the risks.
even in the physical world, more complex protocols have been
designed to mitigate the risks of different types of fraud. think of the basic
car-purchase protocol:
1. alice gives the title and keys to bob.
2. bob gives a check for the purchase price to alice.
3. alice deposits the check.
in this protocol, bob can easily cheat. he can give alice a bad check.
she won’t know the check is bad, and won’t find out until the bank tells
her that the check bounced. by then, bob is long gone with alice’s car.
when i sold my car a few years ago, i used this modified protocol to
prevent that attack:
1. bob writes a check and gives it to the bank.
2. after putting enough of bob’s money on hold to cover the check, the bank
“certifies” the check and gives it back to bob.
3. alice gives the title and keys to bob.
4. bob gives the certified check for the purchase price to alice.
5. alice deposits the check.
what’s going on here? the bank is acting as a trusted third party in
this scrap of street commerce. alice trusts the certification on the check,
that the bank will honor the check for its full amount. bob trusts that the
bank will keep the money for the check on hand, and not spend it on
risky loans in third world countries. alice and bob can complete their
transaction, even though they don’t trust each other, because they both
trust the bank.
this system works not because the bank is a solid institution backed
by impressive-looking buildings and a solid advertising campaign, but
because the bank has no interest in alice and bob’s transaction and has a
reputation to uphold. it will follow the protocol for a certified check no
matter what. if bob has enough money in his account, the bank will issue
the check. if alice presents the check for payment, the bank will pay. if it
did abscond with the money, there wouldn’t be much of a bank left. (this
is the essence of reputation.)
cryptography in context 109
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 109


 
 

____________________________________________

SecretsAndLies.pdf page: 138

____________________________________________

this protocol works to protect alice, but the bank does not protect
bob against buying a forged title and a stolen car. for that, we need
another protocol:
1. alice gives the title and keys to a lawyer.
2. bob gives the check to the lawyer.
3. the lawyer deposits the check.
4. after waiting a specified time period for the check to clear and for bob to
register the car, the lawyer gives the title to bob. if the check does not
clear within a specified time period, the lawyer returns the title to alice. if
bob cannot get a clean title for the car (because alice gave him a bad title),
bob shows proof of this to the lawyer and gets his money back.
as in the previous protocol, a trusted third party gets involved. in this
case, the trusted third party is a lawyer. alice does not trust bob and bob
does not trust alice, but both trust the lawyer to act fairly in the final step.
the lawyer is completely disinterested in the transaction; he does not care
whether he gives the title to bob or alice. he will keep the money in
escrow and do whatever is required, based on the agreement between
alice and bob.
other protocols are more mundane, and might not involve compli-
cated exchanges. for example, here’s a protocol a bank can use to verify
that a check was signed by alice:
1. alice signs the check.
2. the bank compares the signature on the check with the signature it has on
file for alice.
3. if they match, the bank gives alice her money. if they don’t match, the
bank doesn’t.
in theory, the protocol is secure against bob cheating and getting
alice’s money, but of course reality is more complicated. bob could learn
forgery. the bank could make risky loans in paraguay and go under. alice
could pull a gun. there are probably hundreds of ways to break this pro-
tocol, but given a reasonable set of assumptions on people’s behavior, the
protocol works.
protocols in the digital world are much the same as the preceding
examples. digital protocols use cryptography to do the same sorts of
110 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 110


 
 

____________________________________________

SecretsAndLies.pdf page: 139

____________________________________________

things: keep secrets, authenticate things, enforce fairness, provide audit,
whatever.
the internet is full of security protocols, which i discuss in the next
section. other digital networks have their own security protocols. the
cell phone industry uses a bunch of protocols, both for privacy and fraud
prevention, with varying degrees of success. set-top television boxes have
security protocols. smart cards do, too.
protocols involving digital signatures can be particularly useful in
different authentication situations. for example, digital signature schemes
can produce signatures that only the designated recipient can authenticate.
this is useful for informants or whistle-blowers, since the receiver of the
message can verify who sent it, but cannot prove this fact to a third party.
(think of a secret whispered in your ear. you know who said it, but
there’s nothing you can do to prove to someone else who said it.) digital
signature protocols can be used to sign software so that only a person who
buys the software package legitimately can verify the signature and know
that it is authentic; anyone who pirates a copy can’t be sure of this. we
can create group signatures, so that outside the group each signature
appears to come from the group as a whole, but people inside the group
can determine who signed what.
more complex protocols can make cryptography jump through all
sorts of hoops. we can do something called zero-knowledge proofs,
where alice can prove to bob that she knows something without reveal-
ing to him what it is. cryptographic protocols can also support a system
for simultaneous contract signing over the internet, such that neither
party is bound by the contract unless the other is. we can create the
digital equivalent of certified mail, where alice can’t read the mail unless
she sends back a receipt.
using a protocol called secret sharing, we can enforce requirements
for collusion in access: secrets that cannot be revealed unless multiple people
act in concert. this is a really neat notion. think of a nuclear missile silo.
in order to launch the missile, two people have to simultaneously turn
keys and unlock the system. and the keyholes (or in this case, the digital
equivalent) are far enough apart that a single rogue soldier can’t kill every-
one else and turn all the keys himself: at least two people must act in
concert to launch the missile. or think of a corporate checking account
that requires two signatures on high-value checks: any two of the five
cryptography in context 111
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 111


 
 

____________________________________________

SecretsAndLies.pdf page: 144

____________________________________________

cryptography is difficult, too. it combines several branches of math-
ematics with computer science. it requires years of practice. even smart,
knowledgeable, experienced people invent bad cryptography. in the
cryptographic community, people aren’t even all that embarrassed when
their algorithms and protocols are broken. that’s how hard it is.
the problem is this: anyone, no matter how unskilled, can design a
cryptographic primitive that he himself cannot break. this is an important
point. what this means is that anyone can sit down and create a crypto-
graphic primitive, try to break it and fail, and then announce: “i have
invented a secure algorithm/protocol/whatever.” what he is really saying
is: “i cannot break this; therefore it is secure.” the first question to ask in
response is: “well, who the hell are you?” or in more detail: “why
should i believe something is secure because you can’t break it? what
credentials do you have to support the belief that your inability to break
something means that no one else can break it either?”
what the cryptographic community has found is that no one person
has those sorts of credentials. (maybe there’s someone inside the nsa, but
that person’s not talking.) there’s no way to prove the security of
a primitive; it’s only possible to either demonstrate insecurity or fail try-
ing. this is called proving the null hypothesis. the best any security com -
pany can say is: “i don’t know how to break this algorithm/protocol/
whatever, and neither does anyone else.” peer review, long periods of
peer review, are the only evidence of security that we have.
even worse, it doesn’t do any good to have a bunch of random peo-
ple review the primitive; the only way to tell good cryptography from bad
cryptography is to have it examined by experts. analyzing cryptography
is hard, and there is a paucity of people who can do it competently. before
a primitive can really be considered secure, it needs to be examined by
many experts over the course of years.
this is why cryptographers prefer the old and public over the new
and proprietary. public cryptography is what cryptographers study, and
write papers about. older primitives have more papers written about
them. if there were flaws there, they would have been found already (or
so the reasoning goes). the new is riskier precisely because it is new, and
not enough people have studied it.
look at these three alternatives for ip security:
ipsec. beginning in 1992, it was designed in the open by committee and was
the subject of considerable public scrutiny from the start. everyone knew it
116 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 116


 
 

____________________________________________

SecretsAndLies.pdf page: 146

____________________________________________

• triple-des, which has been analyzed by pretty much everyone in the
cryptographic community since the mid-1970s.
• aes, which (when it is chosen) will be the result of a three-year public
selection process that involved pretty much everyone in the cryptographic
community.
• algorithm x, which was published at an academic conference two years
ago; there’s been one analysis paper published that seems to imply that it is
strong.
• algorithm y, which someone recently posted on the internet and assures
you is strong.
• algorithm z, which a company is keeping secret until the patent issues;
maybe they paid a couple of cryptographers to analyze it for three weeks.
this isn’t a hard choice. there may be performance constraints that
prevent you from choosing the algorithm you want (the primary reason
aes exists is that triple-des is too slow for many environments), but the
choice is acutely clear.
it continuously amazes me how often people don’t make the obvious
choice. instead of using public algorithms, the digital cellular companies
decided to create their own proprietary ones. over the past few years, all
the algorithms have become public. and once they became public, they
have been broken. every one of them. the same thing has happened to
the dvd encryption algorithm, the firewire encryption algorithm, vari-
ous microsoft encryption algorithms, and countless others. anyone who
creates his or her own cryptographic primitive is either a genius or a fool.
given the genius/fool ratio for our species, the odds aren’t very good.
the counter-argument you sometimes hear is that secret cryptogra-
phy is stronger because it is secret, and public cryptography is riskier
because it is public. this sounds plausible, but when you think about it for
a minute, the dissonance becomes obvious. public primitives are designed
to be secure even though they are public; that’s how they’re made. so
there’s no risk in making them public. if a primitive is only secure if it
remains secret, then it will only be secure until someone reverse engineers
and publishes it. proprietary primitives that have been “outed” include all
the algorithms in the preceding paragraph, various smart card electronic-
commerce protocols, the secret hash function in securid cards, and the
protocol protecting motorola’s mobile mdc-4800 police data termi-
nal.
118 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 118


 
 

____________________________________________

SecretsAndLies.pdf page: 147

____________________________________________

this doesn’t mean that everything new is lousy. what it does mean is
that everything new is suspect. new cryptography belongs in academic
papers, and then in demonstration systems. if it is truly better, then even-
tually cryptographers will come to trust it. and only then does it make
sense to use it in real products. this process can take five to ten years for
an algorithm, less for protocols or source-code libraries.
choosing a proprietary system is like going to a doctor who has no
medical degree and whose novel treatments (which he refuses to explain)
have no support by the american medical association. sure, it’s possible
(although highly unlikely) that he’s discovered a totally new branch of
medicine, but do you want to be the guinea pig? the best security meth-
ods leverage the collective analytical ability of the cryptographic commu-
nity. no single company (outside the military) has the financial resources
necessary to evaluate a new cryptographic algorithm or shake the design
flaws out of a complex protocol.
in cryptography, security comes from following the crowd. a home-
grown algorithm can’t possibly be subjected to the hundreds of thousands
of hours of cryptanalysis that des and rsa have seen. a company, or
even an industry association, can’t begin to mobilize the resources that
have been brought to bear against the kerberos authentication protocol
or ipsec. no proprietary e-mail encryption protocol can duplicate the
confidence that pgp or s/mime offers. by following the crowd, you
can leverage the cryptanalytic expertise of the worldwide community, not
just a few weeks of some unnoteworthy analyst’s time.
it’s hard enough making strong cryptography work in a new system;
it’s just plain lunacy to use new cryptography when viable, long-studied
alternatives exist. yet most security companies, and even otherwise smart
and sensible people, exhibit acute neophilia and are easily blinded by
shiny new pieces of cryptography.
and beware the doctor who says, “i invented and patented this totally
new treatment that consists of pulverized pretzels. it has never been tried
before, but i’m sure it is much better.” there’s a good reason new cryp-
tography is often called snake oil.
cryptography in context 119
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 119


 
 

____________________________________________

SecretsAndLies.pdf page: 148

____________________________________________

120
8
computer security
computer security is different from cryptography. it often uses
cryptography, but its scope is much broader. general computer
security includes such diverse things as controlling authorized
(and unauthorized) computer access, managing computer accounts and
user privileges, copy protection, virus protection, software metering, 
and database security. more generally, it also includes defenses against
computers across network connections, password sniffers, and network
worms, but we’ll discuss those sorts of things in the chapters on network
security. in the age of the internet, computer security and network secu-
rity have blurred considerably; but for the purposes of this book, i’ll draw
the somewhat arbitrary line between computer and network security as
“whether or not the security problem affects any computer, as opposed to
just a computer attached to networks.” general computer security, which
can be defined as the prevention and/or detection of unauthorized actions
by users of a computer system, seems a whole lot harder than the simple
mathematics of cryptography. and it is.
philosophically, the problem is that the defender doesn’t have math-
ematics on his side. the mathematics of cryptography gives the defender
an enormous advantage over the attacker. add one bit to the key, double
the work to break the algorithm. add ten bits, multiply the work by a
thousand. computer security is more balanced: attackers and defenders
can get similar advantages from technology. what this means is that if
you can rely on cryptography for security, you’re in great shape.
unfortunately, most of the time you can’t.
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 120


 
 

____________________________________________

SecretsAndLies.pdf page: 151

____________________________________________

access control is really a problem much bigger than computers: how
do i limit access to something? how do i control access to a shared
resource? how do i limit the type of access that different people have? it’s
a hard problem to solve in a large building—locks on outside doors and
inside offices and keys given to specific people, badges worn by everyone
and guards to check the badges, and so forth—and it’s a hard problem to
solve on a computer system.
it’s also a problem that’s waxed and waned over the years. in the
beginning, computers didn’t need access control because everyone trusted
each other. as more people started using the large mainframes, access
control was required both to protect privacy and to audit usage for billing.
access control was easy in a batch-processing world.
when personal computers appeared, they didn’t need to provide
access control: every person had his own computer. if someone wanted
to prevent others from accessing his files, he just locked his office door.
now we’re back using shared systems, shared network resources, remote
systems, and the like. access control is a big deal for almost everyone,
whether they’re using a shared computer at work or an account on a web
site. and access control is difficult to do properly.
before talking about different types of access control, we need a
couple of definitions. first, there is some “subject” that has access to some
“object.” often the subject is a user and the object is a computer file, but
not always. the subject could be a computer program or process, and the
object another computer program: a plug-in, for example. the object
could be a database record. the object could be a certain resource, maybe
a piece of computer hardware, or a printer, or a chunk of computer mem-
ory. depending on the circumstance, the same computer program can be
a subject in one access-control relationship and an object in another.
there are two ways to define access control. you can define what dif-
ferent subjects are allowed to do, or you can define what can be done to
different objects. really these are two ways of looking at the same thing,
but they have their pluses and minuses. traditionally, operating systems
managed resources and files, so access control was defined in terms of
these objects. more modern systems are application-oriented. these offer
services to end users, like large database management systems. often these
systems have access-control mechanisms that control subjects.
computer security 123
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 123


 
 

____________________________________________

SecretsAndLies.pdf page: 154

____________________________________________

bell-lapadula has two main security rules: one regarding the reading
of data and the other regarding the writing of data. if users have secret
clearance, they can read unclassified, confidential, and secret docu-
ments, but not top secret documents. if users are working with secret
data, they can create secret or top secret documents, but not confiden-
tial or unclassified ones. (this is important. imagine someone—a person
or maybe a computer virus—trying to steal documents. his computer, of
course, prevents him from e-mailing confidential documents outside the
computer. but if he can take a confidential document and copy the text
into an unclassified document, he can then e-mail the new document.
controls were put in place to prevent this kind of thing.) basically, users
cannot read documents higher than their clearance, nor can they write
documents lower than the clearance of their sessions. and yes, it is theo-
retically possible for users to write documents that they cannot read.
these are mandatory access controls in the language of bell-lapadula,
because they are required by the system. this is in contrast to the “discre-
tionary access controls” in operating systems like unix or nt, described
in the previous section, that allow the users to make their own decisions
about who can read or write to what file. (although most unix versions
can have some mandatory access controls: someone with root access has
mandatory read, write, and execute access to all files on the computer.)
the bell-lapadula model was a big deal, but it had limitations. one,
it concentrates on confidentiality at the expense of pretty much every-
thing else, and that confidentiality is based on a military model of security
classifications. two, it ignores the problem of how to manage classifica-
tions. the model assumes that someone, magically, gives every piece of
data a classification, and that classification never changes. in the physical
world, classifications change: someone notices that it is important and
classifies it, then someone else declassifies it. data sometimes have a higher
classification in aggregate than each datum does individually: an individ-
ual telephone number at the nsa is unclassified, but the entire nsa
phone book is classified confidential. what this means is that data natu-
rally migrates up toward higher classifications, requiring trusted down-
grades. and three, sometimes users need to work with data that they are
not authorized to see. the fact that an aircraft is carrying a cargo of q
bombs might be classified at a level above a dispatcher, but the dispatcher
still needs to know the weight of the cargo.
126 c h a p t e r  e i g h t
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 126


 
 

____________________________________________

SecretsAndLies.pdf page: 157

____________________________________________

disk, how do we prevent one user reading what another user writes?
what if one user wants another user to read what she writes? is it possible
for a user to use interrupts to do something he shouldn’t? how can we
secure access to the printer? can one person eavesdrop on another via the
keyboard? what if the trusted computing base crashes? how do you
implement a disk defragmenter if you can only access your own files?
the historical example that got this the most nearly correct is an oper-
ating system called multics, developed in the late 1960s by mit, bell
labs, and honeywell. multics implemented the bell-lapadula model
from the ground up. (in fact, the multics project was the impetus for the
bell-lapadula model.) the designers used the mathematical formalism of
the model to show the security of the system, and then mapped the con-
cepts of the model into the operating system. no code was ever written
until specifications had been approved. multics worked, although the
security was way too cumbersome. by now, almost everyone has forgot-
ten multics and the lessons learned from that project.
one of the lessons people have forgotten is that the kernel needs to
be simple. (even the multics kernel, with only 56,000 lines of code, was
felt to be too complex.) the kernel is defined as the software that is
trusted. chapter 13 talks about software reliability, the moral being that it
is unreasonable to expect software not to have security bugs. the simpler
the software is, the fewer bugs it will have.
unfortunately, modern operating systems are infected with a disease
known as “kernel bloat.” this means that a lot of code is inside the ker-
nel instead of outside. when unix was first written, it made a point of
pushing nonessential code outside the kernel. since then, everyone has
forgotten this lesson. all current flavors of unix have some degree of
kernel bloat: more commands inside the kernel, inexplicable utilities
running with root permissions, and so forth.
windows nt is much worse. the operating system is an example of
completely ignoring security lessons from history. things that are in the
kernel are defined as secure, so smart engineering says to make the kernel
as small as possible, and make sure everything in it is secure. windows
seems to take the position that since things in the kernel are defined as
secure, than you should put everything in the kernel. when they can’t
figure out how to secure something, they just put it into the kernel and
define it as secure. obviously, this doesn’t work in the long run.
computer security 129
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 129


 
 

____________________________________________

SecretsAndLies.pdf page: 162

____________________________________________

more general type than the formal models discuss. this mandatory secu-
rity mechanism enforces a policy that is controlled by a policy administra-
tor, who is not necessarily the user. moreover, this policy must control
the use of both access and encryption. that is, the policy must enforce
who (person or process) is allowed to access what data (or other process),
and what kinds of encryption controls must be placed on that data. this
kind of policy cannot prevent covert channels (nothing can), but will go
a long way toward stopping the kinds of abuses we’re seeing today.
the second key component is a trusted path. this is a mechanism by
which a user (or a process) can interact with a piece of trusted software,
which can be initiated by either the user or the trusted software, and can-
not be impersonated by another piece of software. for example, would-
n’t it be nice if when a user saw a login screen he could be sure that it was
a real login screen, and not a trojan horse trying to capture his password?
mechanisms for implementing a trusted path will also go a long way
toward limiting the damage malicious software can do.
there are secure oeprating systems on the market that implement
some of these components, but they are still niche products. i would like
to see more of these ideas flow into mainstream operating systems such as
microsoft windows. it doesn’t look like it will happen anytime soon.
134 c h a p t e r  e i g h t
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 134


 
 

____________________________________________

SecretsAndLies.pdf page: 165

____________________________________________

in chapter 7, where i talked about key length and security, i
discussed the problems of user-generated and user-remembered keys. a
password is a form of user-remembered key, and dictionary attacks against
passwords are surprisingly effective.
how does this attack work? think about an access control system for
a computer or web site. the computer has a file of usernames and pass-
words. if an attacker got her hands on that file, she would learn every
password. in the mid-1970s, computer security experts came up with a
better solution: instead of storing all the passwords in a file, they would
store a cryptographic hash of the password. now, when alice types her
password into the computer or web site, the software computes the hash
of the password and compares that hash with the hash stored in a file. if
they match, alice is allowed in. now there is no file of passwords to steal;
there is only a file of hashed passwords to steal. and since a hash function
prevents someone from going backward, the attacker can’t recover the
passwords from the hashed passwords.
here’s where dictionary attacks come in. assume that an attacker has
a copy of the hashed password file. he takes a dictionary, and computes
the hash of every word in the dictionary. if the hashed word matches any
of the password entries, then he has found a password. after he tries all
words, he tries reversed dictionary words, dictionary words with some
letters capitalized, and so forth. eventually he tries all character combina-
tions shorter than some length.
dictionary attacks used to be hard, because computers were slow.
they’re much easier now, because computers are a lot faster. l0phtcrack
is an example of a password recovery hacker tool that is optimized for
windows nt passwords. windows nt contains two password func-
tions: a stronger one designed for nt, and a weaker one that is backward-
compatible with older networking login protocols. the weaker one is
case-insensitive, and passwords can’t be much stronger than seven charac-
ters (even though they may be longer). l0phtcrack makes easy work of
this password space. on a 400-mhz quad pentium ii, l0phtcrack can
try every alphanumeric password in 5.5 hours, every alphanumeric pass-
word with some common symbols in 45 hours, and every possible
keyboard password in 480 hours. this is not good.
some have dealt with this problem by requiring stronger and stronger
passwords. what this means is that the password is harder to guess, and less
likely to appear in a password dictionary. the old racf mainframe sys-
tem required users to change passwords monthly, and wouldn’t permit
identification and authentication 137
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 137


 
 

____________________________________________

SecretsAndLies.pdf page: 166

____________________________________________

words. (microsoft windows has no such controls, and helpfully offers to
remember your passwords for you.) some systems generate passwords
randomly for users, by concatenating random syllables to create pro-
nounceable passwords (e.g., “talpudmox”) or mixing in numbers or sym-
bols and changing case: for example, “fot78hif#elf.” pgp uses
passphrases, which are recommended to be complex sentences with non-
sense thrown in: for example, “33333telephone,, it must be you
speaking sweetly to me1958???!telephone.” (admittedly, that’s not as easy
to remember and type as you might want.)
these techniques are becoming less and less effective. over the past
several decades, moore’s law has made it possible to brute-force larger and
larger entropy keys. at the same time, there is a maximum to the entropy
that the average computer user (or even the above-average computer
user) is willing to remember. you can’t expect him to memorize a 32-
character random hexadecimal string, but that’s what has to happen if he
is to memorize a 128-bit key. you can’t really expect him to type the
pgp passphrase in the previous paragraph. these two numbers have
crossed; password crackers can now break anything that you can reason-
ably expect a user to memorize.
there are exceptions to this, of course. you could imagine high-
security applications—nuclear launch computers, secure diplomatic chan-
nels, systems that communicate with spies living deep in enemy
territory—where users will take the time to memorize long and compli-
cated passphrases. these applications have nothing to do with modern
computer networks and passwords for commodity e-commerce applica-
tions. the problem is that the average user can’t, and won’t even try to,
remember complex enough passwords to prevent dictionary attacks.
attacking a basic password-protected system is often easier than attacking
a cryptographic algorithm with a 40-bit key. passwords are insecure,
unless you can stop dictionary attacks.
as bad as passwords are, users will go out of the way to make it worse.
if you ask them to choose a password, they’ll choose a lousy one. if you
force them to choose a good one, they’ll write it on a post-it and stick it
on their computer monitor. if you ask them to change it, they’ll change
it back to the password they changed it from last month. one study of
actual passwords found that 16 percent of them were three characters or
less, and 86 percent of them were easily crackable. other studies have
confirmed these statistics.
138 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 138


 
 

____________________________________________

SecretsAndLies.pdf page: 167

____________________________________________

and they’ll choose the same password for multiple applications. want
to steal a bunch of passwords? put up a web site with something interest-
ing on it: porn, hockey scores, stock tips, or whatever will appeal to the
demographic you’re after. don’t charge for it, but make people register a
username and password in order to see the information. then, sit back
and collect usernames and passwords. most of the time you’ll get the same
username and password that the user chose last time, maybe the one that
lets you into his bank or brokerage accounts. save incorrect passwords as
well; people sometimes enter the password for system a into system b by
mistake. make the user fill out a little questionnaire during registration:
“what other systems do you use regularly? bank x? brokerage firmy?
news service z?” a researcher i know did something like this in 1985;
he got dozens of system administrator passwords.
and even when they choose good passwords and change them regu-
larly, people are much too willing to share their passwords with others in
and out of the organization, especially when they need help to get the
work done. clearly this represents one of the greatest security risks of all,
but, in people’s minds, the risk is minimal and the need to get work done
imperative.
this is not to say that there are not better or worse passwords. the
preceding example pgp passphrase is still secure against dictionary attacks.
generally, the easier a password is to remember, the worse it is. dictio-
nary attacks generally try common passwords before uncommon ones:
dictionary words, reversed dictionary words, dictionary words with some
letters capitalized, dictionary words with minor modifications—like the
number “1” instead of the letter “l”—and so forth.
unfortunately, many systems are only as secure as the weakest pass-
word. when an attacker wants to gain entry into a particular system, she
might not care which account she gets access to. in operational tests,
l0phtcrack recovers about 90 percent of all passwords in less than a day,
and 20 percent of all passwords in a few minutes. if there are 1,000
accounts, and 999 users choose amazingly complicated passwords that
l0phtcrack just can’t possibly recover, it will break the system by recov-
ering that last weak ordinary password.
on the other hand, from the user’s point of view this can be an exam-
ple of “not having to outrun the bear; only having to outrun the people
you’re with.” any dictionary attack will succeed against so many accounts
whose passwords are “susan” that if your password is “hammerbutterfly,”
identification and authentication 139
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 139


 
 

____________________________________________

SecretsAndLies.pdf page: 168

____________________________________________

while it’s pretty vulnerable to dictionary attacks, it’s not likely to actually
succumb to one.
depending on the type of attacker you’re worried about, a system
with long and strong passwords can be secure. but this is changing all the
time; moore’s law means that today’s strong password is tomorrow’s
weak password. in general, if a system is based on passwords and an
attacker can mount a dictionary attack, then the system is vulnerable.
period.
there are fixes. this is all predicated on the attacker stealing the file
of hashed passwords. prevent dictionary attacks, and passwords are again
good. this is possible, although not easy, for general-access machines.
the unix password file, for example, is world readable. these days,
unix has something called a shadow password file; it contains the actual
hashed passwords, and the world-readable password file contains nothing
useful. the hashed password file in nt is well-protected and difficult to
steal; you either need administrator access to sniff the hashed passwords
across the network (although the latest nt version and windows 2000
prevent this); or you need to pick up the passwords when they are used
by other network applications. 
systems can also lock up after some number of bad passwords, for
example, ten. what this means is that after someone fails to log in ten
times, the system freezes the account. so if someone tries to log in to
alice’s account and starts guessing passwords, he only gets ten guesses
before the system freezes. this will, of course, annoy alice, but it’s better
than compromising alice’s account. and the exact definition of “freeze”
can depend on the circumstance. maybe it will freeze alice’s account for
five minutes, or 24 hours. maybe it will freeze alice’s account until she
talks with some administrator. high-security devices might freeze perma-
nently, destroying the information inside, after a certain number of incor-
rect passwords.
another solution is to require a noncomputer interface. your atm
cash card is protected by a four-digit pin. that would be trivial for a
computer to break—it would take a few milliseconds to try all 10,000
possible pins—but it’s hard for a computer to attach itself to the user
interface. a person has to stand at the atm and try pins, one after the
other. at a brisk ten seconds per attempt it would take 28 hours, nonstop,
to try 10,000 pins.
140 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 140


 
 

____________________________________________

SecretsAndLies.pdf page: 170

____________________________________________

who signed it. your photograph identifies you as the person who owns a
particular passport.
for most applications, biometrics need to be stored in a database like
passwords. alice’s voice only works as a biometric identification on the
telephone if you already know who she is; if she is a stranger, it doesn’t
help. it’s the same with alice’s handwriting; you can recognize it only if
you already know it. to solve this problem, banks keep signature cards on
file. alice signs her name on a card when she opens her account, and it is
stored in the bank (the bank needs to maintain its secure perimeter in
order for this to work right). when alice signs a check, the bank verifies
alice’s signature against the stored signature to ensure that the check is
valid. (in practice, that rarely happens. manual signature checking is so
costly that the bank doesn’t bother checking for amounts less than about
$1,000. if there is a problem, they assume, someone will complain. and
making good on the occasional problem is cheaper than paying someone
to do the checking.) you could do the same thing with alice’s voice—
compare her voiceprint to the one stored in some central database.
the exceptions are situations where the biometric is only verified as
part of an involved and uncommon protocol. when alice signs a con-
tract, for example, bob does not have a copy of her signature on file. the
protocol still works because bob knows that he can verify the signature at
some later time, if necessary.
there are many different types of biometrics. i’ve mentioned hand-
writing, voiceprints, face recognition, and fingerprints. there is also hand
geometry, typing patterns, retinal scans, iris scans, signature geometry (not
just the look of the signature, but the pen pressure, signature speed, and
so forth), and others. the technologies behind some of them are more
reliable than others—fingerprints are much more reliable than face recog-
nition—but that may change as technology improves. some are more
intrusive than others; one failed technology was based on lip pattern, and
required the user to kiss the computer. as a whole, biometrics will only
get better and better.
“better and better” means two different things. first, it means that it
will not incorrectly identify an impostor as alice. the whole point of the
biometric is to prove that the claimant alice is the actual alice, so if an
impostor can successfully fool the system, it isn’t working very well. this
is called a false positive. second, it means that the system will not incor-
rectly identify alice as an impostor. again, the point of the biometric is to
142 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 142


 
 

____________________________________________

SecretsAndLies.pdf page: 172

____________________________________________

ify that the picture is of your face, only that it matches the picture of
alice’s face on file, we can fool it.
similarly, we can fool a signature biometric using a photocopier or a
fax machine. it’s hard to forge the vice president’s signature on a letter
giving you a promotion, but it’s easy to cut his signature out of another
letter, paste it on the letter giving you a promotion, and fax it to the
human resources department. they won’t be able to tell that the signature
was cut from another document.
the moral is that biometrics work great only if the verifier can verify
two things: one, that the biometric came from the person at the time of
verification, and two, that the biometric matches the master biometric on
file. if the system can’t do both, it is insecure.
here’s another possible biometric system: thumbprints for remote
login authorizations. alice puts her thumbprint on a reader embedded
into the keyboard (don’t laugh, a lot of companies want to make this hap-
pen, and the technology already exists). the computer sends the digital
thumbprint to the host. the host verifies the thumbprint and lets alice in
if it matches the thumbprint on file. this won’t work because it’s so easy
to steal alice’s digital thumbprint, and once you have it, it’s easy to fool
the host, again and again.
tamper-resistant hardware helps (within the limitations of chapter
14), as long as the tamper-resistant hardware includes both the biometric
reader and the verification engine. it doesn’t work if a tamper-resistant
fingerprint reader sends the fingerprint data across an insecure network.
encryption can help, too, though.
anyway, this brings us to the second major problem with biometrics:
it doesn’t handle failure well. imagine that alice is using her thumbprint
as a biometric, and someone steals it. now what? this isn’t a digital cer-
tificate (we’ll get to those in chapter 15), where some trusted third party
can issue her another one. this is her thumb. she only has two. once
someone steals your biometric, it remains stolen for life; there’s no getting
it back.
this is why biometrics don’t work as cryptographic keys (even if you
could solve the fuzzy biometric logic versus absolute mathematical logic
problem). occasionally i see systems that use cryptographic keys gener-
ated from biometrics. this works great, until the biometric is stolen. and
i don’t mean that the person’s finger is physically cut off, or the fingerprint
is mimicked on someone else’s finger; i mean that someone else steals the
144 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 144


 
 

____________________________________________

SecretsAndLies.pdf page: 173

____________________________________________

digital fingerprint. once that happens, the system does not work any-
more. (well, maybe it will work until all ten fingers are stolen. . . .)
biometrics can be good authentication mechanisms, but they need to
be used properly.
access tokens
the third solution to proving identity is to use something you have: a
physical token of some sort. this is an old form of access control: a phys-
ical key restricted access to a chest, a room, a building. possession of the
king’s seal authorized someone to act on his behalf. more modern systems
can be automated—electronic hotel room keys—or manual—corporate
badges that allow access into buildings. the basic idea is the same; a phys-
ical token serves to authenticate the holder of it.
there are several basic ways this can be done. most simply, the holder
can simply prove that he is holding the token. computers that require a
physical key to turn them on work in this manner; so do computers that
require a smart card. the basic idea is that you insert the token into some
slot somewhere, and then the computer verifies that it is really there. if it
is, you’re in.
the most serious problem with this system is that tokens can be
stolen. if someone steals your house keys, for example, she can unlock
your house. so the system doesn’t really authenticate the person; it
authenticates the token. most computer systems combine access tokens
with passwords—sometimes called pins—to overcome this vulnerability.
you can think of bank atm cards. the atm authenticates the card, and
also asks for a pin to authenticate the user. the pin is useless without the
access token. some cellular phone systems work the same way: you need
the physical phone and an access code to make calls on a particular cellu-
lar account.
in addition to stealing a token, someone can copy it. some tokens can
be easily copied—physical keys, for example—so they can be stolen,
copied, and replaced without the owner knowing about it.
another problem is that there needs to be some authenticated way of
determining that the token is really there. think of a token as a remov-
able, changeable biometric, and you’ve got all the problems of a secure
identification and authentication 145
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 145


 
 

____________________________________________

SecretsAndLies.pdf page: 174

____________________________________________

verification path from the previous section. at least the token can be
changed if necessary.
this problem can be illustrated using credit cards. it’s difficult to forge
a physical credit card, which makes it risky to use a forged credit card to
purchase things at a store. the clerk might notice that the card is forged.
it’s far easier to use a forged credit card over the telephone, however. at
the store, the clerk authenticates both the account number on the credit
card and the credit card itself—the token. over the phone, the operator
cannot authenticate the physical token, only the account number.
there’s another, relatively minor, problem that shows up with some
tokens. if users can leave the token in the slot, they often do. if the users
need to have a smart card inserted in a slot before it will boot, they’re
likely to leave the smart card there all day and night . . . even when
they’re not there. so much for authentication.
all of this discussion assumes that there’s some kind of reader associ-
ated with the token, and the user can insert the token into the reader. this
often isn’t the case: most computers don’t have the required reader, or the
system might have to work for mobile users who could be sitting some-
where other than at their normal computers. two different technologies
deal with this situation.
the first is challenge/reply. the token is a pocket calculator, with a
numeric keypad and small screen. when the user wants to log in, the
remote host presents him with a challenge. he types that challenge into
his token. the token calculates the appropriate reply, which he types into
the computer and sends to the host. the host does the same calculation;
if they match, he is authenticated. the second technology is time-based.
this token is the same pocket calculator, with just a screen. the numbers
on the screen change regularly, generally once per minute. the host asks
the user to type in what is showing on his screen. if it matches what the
host expects, he is authenticated. the securid token works this way.
of course, the full system also includes a password—the challenge/
reply token might even require a second password to get it working—and
there are other, ancillary, security measures. the basic idea, though, is that
some secret calculation is going on inside the token that can’t be imper-
sonated. an attacker can’t pretend to have the token, because she doesn’t
know how to calculate replies based on challenges, or doesn’t know how
146 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 146


 
 

____________________________________________

SecretsAndLies.pdf page: 175

____________________________________________

to calculate values based on the time. the only way to do this is to actu-
ally have the token.
this works, more or less. cryptographic techniques, encrypting or
hashing, provide the security. the host knows how to do the calculations,
so the system is only as secure as the host’s source code. anyone who can
reverse engineer the token can figure out how to do the calculations, so
the system is only as secure as the tokens (see chapter 14). but it’s pretty
good, and certainly a lot better than passwords alone. the security prob-
lems arise in the network, and the authenticating computer.
one last token needs discussion: the password, written down. there
is a knee-jerk reaction to writing passwords down in the security com-
munity, but if done properly this can improve security considerably.
someone who writes his password down turns something he knows (the
password) into something he has (the piece of paper). this trick does
allow him to use longer passwords, which can make passwords actually
secure again. it does have all the problems of a simple token: it can be
copied or stolen. it doesn’t work if alice writes her password on a yellow
sticky attached to her monitor. much better is for her to put her pass-
words in her wallet; this can be secure. probably the best solution is to
have two parts to the password: one part remembered by alice, and the
other part written down in her wallet.
similarly, there are systems of one-time passwords. the user has a list
of passwords, written down, and uses each one once. this is certainly a
good authentication system—the list of passwords is the token—as long as
the list is stored securely.
authentication protocols
authentication protocols are cryptographic ways for alice to authenticate
herself across a network. the basic authentication protocol is pretty sim-
ple:
1. alice types in her username and password on the client. the client sends
this information to the server.
2. the server looks up alice’s username in a database and retrieves the corre-
sponding password. if that password matches the password alice typed,
alice is allowed in.
identification and authentication 147
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 147


 
 

____________________________________________

SecretsAndLies.pdf page: 177

____________________________________________

cator” that she will use to prove to bob that she is alice.
5. alice sends bob both the ticket and the authenticator.
6. bob validates everything. if it all checks out, he lets alice in. (bob also
shares a long-term key with the kerberos server. the ticket is a message
from the server encrypted in bob’s long-term key.) 
this protocol is secure in the same way that physical ticket protocols are
secure. the kerberos server prints tickets. it gives alice a ticket that she
can present to bob. bob can validate the ticket, so he knows that alice
received it from the kerberos server.
this protocol has some nice properties. the long-term secrets of
alice and bob, which are kind of like passwords, are never sent through
the network. on the minus side, this system needs a kerberos server to
operate; the kerberos server is a trusted third party. this can mean a bot-
tleneck in the system at 9:00 in the morning, when everyone is trying to
log on to their computer.
kerberos was invented at mit in 1988, and has been used in the
unix world ever since. kerberos is part of windows 2000, but
microsoft’s implementation differs from the standard and is incompatible
with the rest of the kerberos world. i can only assume this was done for
deliberate marketing reasons (at this writing, microsoft only allowed you
to open the file with the modification details if you first clicked on a
screen agreeing to treat the information as proprietary, so third-party
developers can’t build interoperable systems), but it makes for bad secu-
rity. you can’t just modify a security protocol and assume that the modi-
fied protocol is also secure.
other, more byzantine, login authentication protocols use public-
key cryptography. ipsec and ssl, for example, use public-key authenti-
cation protocols. some systems use simple, but esoteric, protocols. the
protocol by which a cell phone proves that it should be allowed to make
telephone calls in a particular network is one of these.
single sign-on
one thing that has annoyed computer users in large secure environments
is the large number of passwords. users might have to type in one pass-
word to log on to their computers, another to log on to the network, a
third to log on to a particular server on the network, and so on and on and
identification and authentication 149
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 149


 
 

____________________________________________

SecretsAndLies.pdf page: 182

____________________________________________

print on a file, they know it has been infected. these programs can then
disinfect the file by removing the viral code. fingerprint scanning only
works after the antivirus company has isolated the virus in its lab and
updated its software to include the new fingerprint: hence the brisk busi-
ness in antiviral software updates.
in some ways we’ve been fortuitous with respect to computer viruses;
all the ones we’ve seen are targeted against large computers, not periph-
erals or embedded systems. it’s possible to write a virus in the postscript
printing language. it could propagate from document to document. it
could affect printers. it’s possible to write a virus that infects cell phones,
and propagates via the cellular network. it’s possible to write a virus that
affects almost any computerized system; we’ve seen one that’s specific to
webtv devices. if we haven’t seen it yet, it’s be cause no one with the
requisite knowledge and lack of morals has bothered making one.
to catch unknown viruses, polymorphic viruses (which mutate with
every infection), and encrypted viruses (which use cryptography to hide
their footprints), some antiviral products monitor the computer system
looking for “suspicious” virus-like behavior. (normal virus checkers are
pretty brain-dead; sometimes just changing variable names is enough to
fool them.) these systems work moderately well, although they rely on
users to make security decisions: is this a virus or a false alarm?
viruses have no “cure.” it’s been mathematically proven that it is
always possible to write a virus that any existing antivirus program can’t
stop. (even the bell-lapadula model does not prevent virus attacks.) i’ll
elide the details, but the basic idea is that if the virus writer knows what
the antivirus program is looking for, he can always design his virus not to
be noticed. of course, the antivirus programmers can always create an
update to their software to detect the new virus after the fact.
worms
a worm is a piece of malware particular to networked computers. it’s a
self-replicating program that does not hide in another program, like a
virus does. instead it exists on its own, meandering through computer
networks as best it can, doing whatever damage it is programmed to do.
robert t. morris released the most famous worm in 1988. it was an
internet worm, and crashed about 6,000 computers: 10 percent of the
154 c h a p t e r  t e n
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 154


 
 

____________________________________________

SecretsAndLies.pdf page: 183

____________________________________________

internet’s computers. the worm started out on one machine. then it
tried breaking into other machines on the network, using a couple of
basic techniques. when it was successful, it sent a copy of itself to the new
machine. and then the copy replicated the process, trying to break into
yet more machines. this is the way a worm works. the worm would
have been more devastating had it not been for a lucky bug. it was not
supposed to crash 6,000 computers; it was supposed to quietly infect
them. a bug in the worm program caused it to crash computers it
infected. i’ll talk more about the details of how it infected and the bug in
chapter 13.
prettypark is another worm. it’s a windows executable that arrives as
an attachment to an e-mail message. (its name comes from the fact that
the program’s icon is a south park character named kyle.) if you run the
program, it sends itself to everyone in your outlook express address
book. it also attempts to connect to an internet relay chat (irc) server
and send messages to chat users. the author of the worm can then use the
connection to collect information from your computer. iloveyou
and all its variants are worms, too.
trojan horses
a trojan horse is a piece of malware embedded in some “normal” piece
of software, designed to fool the user into thinking that it is benign.
remember the original trojan horse? the greeks besieged troy for ten
years, and it was showing no sign of falling. out of desperation—and
probably boredom—odysseus had the greek soldiers build a large
wooden horse and put some of them inside. he left it for the trojans as
an admission of defeat and then told his army to pretend to sail away, try-
ing not to giggle as they did. the trojans took the wooden horse inside
the walls—every artist’s rendition puts the horse on a wheeled platform—
despite the better judgment of one of their priests. that night, the greeks
crept out of the horse, opened the gates, and let the rest of the greek army
inside. the greeks then massacred the trojans, looted their wealth, and
burned the city. (at least, that’s the story. no one knows if it’s true or not.
troy itself was considered a myth until heinrich schliemann discovered
it in the late 1800s.)
following that analogy, a digital trojan horse is code deliberately
placed in your system, that does things you don’t expect or want while
networked-computer security 155
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 155


 
 

____________________________________________

SecretsAndLies.pdf page: 184

____________________________________________

pretending to do something useful. (technically, a trojan horse is code
that you deliberately place on your system, while a logic bomb is code that
someone else places on your system.) it’s a piece of code that a program-
mer writes into a large software application that starts misbehaving if, for
example, the programmer is ever deleted from the payroll file. timothy
lloyd, a network manager at omega engineering, set a logic bomb in
1996 that crippled his former employers’ manufacturing capabilities and
cost them more than $12 million in damages.
a trojan horse, on the other hand, is a program that secretly installs
itself in your machine, watches your keyboard buffer until it detects what
appears to be a credit card number—right number of digits, checksum
matches—and sends that number via tcp/ip to someone. it’s a java
application that disconnects your modem connection and connects you to
a 900 number in moldavia (this trojan horse actually happened).
a trojan horse is a particularly insidious attack because you may not
know what it’s doing. back orifice is a popular trojan horse for
microsoft windows. if it is installed on your computer, a remote user can
effectively take it over across the internet. he can upload and download
files, delete files, run programs, change configurations, take control of the
keyboard and mouse, see whatever is on the server’s screen. he can also
do more subversive things: reboot the computer, display arbitrary dialog
boxes, turn the microphone or camera on and off, capture keystrokes (and
passwords). and there is an extensible plug-in language for others to write
modules. (i’m waiting for someone to disseminate a module that auto-
matically sniffs for, and records, pgp private keys or web login
sequences.) 
in addition to back orifice and other hacker-written tools, many
remote administration programs can serve as trojan horses. dirt (data
interception by remote transmission) is a trojan horse developed by
the u.s. government and available to police.
these are the swiss army knives of trojan horses, but others are
much subtler. several trojan horses collect usernames and passwords, and
send them back to the creator. trojans can also subtly modify your
encryption program to choose keys from a small random pool, effectively
weakening the keyspace. (i have seen trojaned versions of pgp that do
this.) they can drop a fake certificate into your computer and fool you
into trusting someone. (lab demonstrations of attacks against microsoft’s
156 c h a p t e r  t e n
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 156


 
 

____________________________________________

SecretsAndLies.pdf page: 191

____________________________________________

sive; there’s no real middle ground. (java 2 has fine-grained control, but it
isn’t used very well.)
code signing. think of a private party. the host decides who to let in and
who to keep out, based only on some unforgeable document they have (a
driver’s license, for example). that way, only friends of the host are allowed
in his home. code signing is the same thing. the programmer signs com-
ponents. the user decides, based on the signatures, which components to
allow on his computer and which not to. (activex uses code signing as its
primary security against hostile code.) code signing, as it is currently done,
sucks. there are all sorts of problems. first, users have no idea how to
decide if a particular signer is trusted or not. second, just because a compo-
nent is signed doesn’t mean that it is safe. third, just because two compo-
nents are individually signed does not mean that using them together is safe;
lots of accidental harmful interactions can be exploited. fourth, “safe” is not
an all-or-nothing thing; there are degrees of safety. and fifth, the fact that
the evidence of attack (the signature on the code) is stored on the computer
under attack is mostly useless: the attacker could delete or modify the sig-
nature during the attack, or simply reformat the drive where the signature is
stored. code signing makes less and less sense the more you think about it.
nascent technologies gestating in university laboratories may someday
result in better solutions, but they are some years away. in the meantime,
modular code is likely to become an even bigger security problem. more
and more software packages are building in live update features, allowing
them to download new modules regularly. for example, internet
explorer 4.0 and later versions have a “subscription” feature that, if the
user turns it on, will automatically update itself with new modules from
microsoft’s web page. this is a fine feature, unless you turn it on acci-
dentally. then, in the middle of the night, you can find your computer
automatically dialing the internet. one reaction from a news report:
“i had my head in the refrigerator very early in the morning and discovered
my computer had connected itself to the internet,” said one beta tester who
requested anonymity for fear that his working relationship with microsoft
would be damaged. “i was completely freaking out. i pulled the phone plug
right out of the wall.” 
there was nothing nefarious here; the user just didn’t realize what was
going on. but most computer users have no idea what is going on inside
networked-computer security 163
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 163


 
 

____________________________________________

SecretsAndLies.pdf page: 193

____________________________________________

rare, and finding people who can answer those questions is even more
rare.
javascript, java, and activex
javascript, java, activex, and downloadable plug-ins all have different
models for securing themselves. i’ll talk about them each in turn.
javascript is netscape’s scripting language that allows bits of code to
be embedded in web pages, and all major browsers support it. it is simi-
lar to java only in its first four letters. javascript code can be used for
simple things: opening and closing windows, manipulating forms on web
pages, adjusting browser settings, and so forth. all of those annoying
things that some web sites do when you try to close their pages: that’s
javascript.
javascript is basically pretty tame, but all sorts of javascript-based
attacks have appeared over the past few years. these bugs have all been
fixed. a few random examples: 1997, monitor what sites the user visits;
1998, read arbitrary files on the user’s machine; 1998, intercept the user’s
e-mail address. a lot of these attacks depend on fooling the user into
doing something marginally stupid, but that’s not hard. these sorts of
security flaws show up in browsers, and are fixed pretty quickly. but new
ones are regularly discovered.
activex uses a code-signing defense. basically, every piece of
activex code, called a “control,” is checked for a digital signature.
(microsoft has defined something called authenticode to do this.) then
the browser puts up a dialog box, and shows the user the name of the pro-
grammer or company that signed the control. if the user agrees to accept
the control, it is downloaded to the browser. otherwise, it is not.
any teenager who’s let the wrong sorts of guests into his party knows
the problem: the system is only as good as the judgment of the user.
once an activex control is on a user’s machine, it can do anything it
wants: reformat your hard drive, change all your $1 spreadsheet entries to
$100, collect all your steamy love letters and send them to a movie pro-
ducer in los angeles, whatever.
microsoft has countered that the signatures will identify authors, but
knowing who wrote the malicious control is little consolation to some-
one who just had his computer trashed. it’s like forcing criminals to wear
name badges and then not bothering to put locks on people’s doors: “i’m
networked-computer security 165
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 165


 
 

____________________________________________

SecretsAndLies.pdf page: 196

____________________________________________

all revolve around the certificates and how they are used (see chapter 15
for an explanation). basically, some web sites give you the option of set-
ting up an ssl-secured browsing session. (the web page has to have the
option; the browser cannot demand to use ssl if the server is not set up
for it.) the browser and the web server use public-key cryptography to
exchange a key and then symmetric cryptography to encrypt the data
going back and forth. a green key or a yellow padlock appears on the
bottom of the browser, and the user feels much better.
the main problem is that unless the user manually checks the certifi-
cate the server sent, he has no idea whom he went secure with. let me
repeat this. ssl establishes a secure connection between the browser and
whomever is at the other end of the connection. if the user does not ver-
ify who is at the other end of the connection, he has no idea who he is
speaking securely with. it’s as if two strangers enter a pitch-black sound-
proof room. the two people know that their conversation is secure, that
no one is eavesdropping. but who would tell his secrets to the stranger?
this is only one problem with ssl certificates as they are used.
also, ssl does nothing to protect the data at the server. in early 2000,
there were many cases of hackers breaking into web sites and stealing
information: credit card numbers, personal account information, and
more. ssl does nothing to prevent this.
url hacking
a bunch of attacks target urls, some relying on user error and some just
on user ignorance. the first class of attacks consists of ways different
servers steal traffic from each other. you might not think this is a big
deal—why would a web site that sells plumbing supplies want to steal
traffic from a financial news web site—but some sites, like porn sites, just
want people to look at their home pages.
one of the ways to try to get traffic is to try to fool the search engines.
search engines are mostly pretty stupid: ask for sites on plumbing supplies,
and they respond with all the web pages that have the words “plumbing
supplies” somewhere in the text. (actually, the newer search engines are
a smidge smarter, but that’s the general idea.) what some sites do is to put
text on the pages to act as bait for the search engines. this text is not
shown on the screen—sometimes it’s hidden by other things (white text
on a white background, for example) and sometimes it’s in the form of
168 c h a p t e r  t e n
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 168


 
 

____________________________________________

SecretsAndLies.pdf page: 198

____________________________________________

one else. for example, someone other than me owns applied-cryptogra-
phy.com and applied-cryptography.com, the title of my first book.
web spoofing is kind of an internet con game. by manipulating the
url addresses on a client’s site, an attacker can force a victim to do all its
browsing through a particular site. this site, owned by the attacker, can
eavesdrop on the victim’s entire browsing session. the attacker can keep
records of where the victim visits, what his different account names and
passwords are, anything. the attacker can also subtly modify different
pages—maybe change the “ship to” address for products that the victim
buys.
this attack works even if the victim has an ssl connection. as i
mentioned previously, ssl only guarantees that the user is secure with
someone. in the case of this attack, the user has a secure connection with
the attacker—not very helpful. several other tricks facilitate the attack;
turning off javascript provides some defense. some web sites—askjeeves
is an example—exacerbate the problem by putting other people’s web
pages in their own frames, and present that information as their own. at
the time of writing, this attack has not been reported in the wild.
cookies
cookies are an inventive programming trick built into www browsers.
basically, a cookie is a scrap of data that a web server gives to a browser.
the browser stores the data on the user’s computer, and returns it to the
server whenever the browser returns to the server. cookies can do all
sorts of useful and good things. unfortunately, they can also do all sorts of
useful and bad things. first, i’ll explain how they work; then i’ll talk about
the problems.
http is basically a stateless protocol. this means that the server
doesn’t know who you are from one click to the next. all the server does
is serve up web pages. a browser asks for a web page; the server gives it
to it. the server has no idea if this is the same browser as before or a
different browser, nor does it care. this works great for simple, static,
web sites that just contain informational pages.
complex web sites are dynamic. retail web sites have shopping
carts, which travel with you as you browse the site. paid-access informa-
tional sites have usernames and passwords, which travel with you as you
go from page to page. (i would find it annoying to have to type my user-
170 c h a p t e r  t e n
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 170


 
 

____________________________________________

SecretsAndLies.pdf page: 204

____________________________________________

176
11
network security
network security goes hand in hand with computer security,
and these days it’s hard to separate the two. everything, from
electronic hotel door locks to cellular telephones to desktop
computers, is attached to networks. as difficult as it is to build a secure
stand-alone computer, it is much more difficult to build a computer that
is secure when attached to a network. and networked computers are
even more pregnable; instead of an attacker needing to be in front of the
computer he is attacking, he can be halfway across the planet and attack
the computer using the network. a networked world may be more con-
venient, but it is also much more insecure.
these days it’s pretty much impossible to talk about computer secu-
rity without talking about network security. even something as special-
ized as the credit card clearing system works using computer networks. so
do cellular telephones and burglar alarm systems. slot machines in casinos
are networked, as are some vending machines. the computers in your
kitchen appliances will soon be networked, as will the ones in your car.
all computers will eventually be networked.
lots of different types of networks are out there, but i’m going to
spend the most time talking about the internet protocol: tcp/ip. net-
working protocols seem to be converging on the internet, so it makes the
most sense to talk about the internet. this is not to imply that the inter-
net protocols are more insecure than others—although certainly they
were never designed with security in mind—only that there are more
good examples. later in the book, i talk about the fundamental dilemma
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 176


 
 

____________________________________________

SecretsAndLies.pdf page: 205

____________________________________________

of choosing a common protocol that is widely attacked by hackers, and
hence whose security is constantly improving, or one that is obscure and
little-known, and is possibly even less secure. keep that question in mind
while reading this chapter.
how networks work
computer networks are bunches of computers connected to each other.
that is, either physical wires run between computers—wires in an office
lan, dedicated phone lines (possibly isdn or dsl), dial-up connec-
tions, fiber optic, or whatever—or there is an electromagnetic connec-
tion: radio links, microwaves, and so forth.
simply, when one computer wants to talk to another, it creates a
message, called a packet, with the destination computer’s name on it and
sends it to the computer over this network. this is fundamentally unlike
telephone conversations. when alice wants to call bob, she tells the
phone company’s computer network bob’s network name (commonly
known as his telephone number) and the network hooks up different
communications circuits—copper wire, satellite, cellular, fiber, what-
ever—to make an unbroken connection. alice and bob talk through this
circuit until one of them hangs up. then, the telephone network disas-
sembles this connection and lets other people use the same pieces for
other phone calls. the next time alice calls bob, they will be connected
through a completely different set of links. (well, mostly different; the
line between the telephones and the first switches will be the same.)
computers don’t use circuits to talk to each other. they don’t have
conversations like people do—they send short data packets back and
forth. these packets are broken-up pieces of anything: e-mail messages,
gifs of naked ladies, streaming audio or video, internet telephone calls.
computers divide large files into packets for easier transmission. (think of
a ten-page letter being divided up and mailed in ten different envelopes.
at the recipient’s end, someone opens all the envelopes and reassembles
the letter in its proper order. the packets don’t have to arrive in order,
and they don’t have to travel along the same route to their destination.)
these packets are sent through the network by routers. there are
bunches of protocols—ethernet, tcp, whatever—but they all work
basically (for large values of “basically”) the same way. routers look at the
network security 177
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 177


 
 

____________________________________________

SecretsAndLies.pdf page: 206

____________________________________________

addresses on packets, and then send them toward their destination. they
may not know where the destination is, but they know something about
where it should go. it’s sort of like the postal system. a letter carrier visits
your house, takes all of your outgoing mail, and brings it to the local post
office. the post office might not know where 173 pitterpat lane,
fingerbone, id, is, but it knows that it should put the envelope on the
truck to the airport. the airport postal workers don’t know either, but
they know to put the letter on a plane to chicago. the chicago post
office knows to put the letter on a plane to boise. the boise post office
knows to put the letter on a train to fingerbone. and finally, the local
fingerbone post office knows where the address is, and a letter carrier
delivers it.
ip security
it’s not hard to see that any network built on this model is terribly inse-
cure. consider the internet. as those packets pass from router to router,
their data, sometimes called their payload, is open to anyone who wants
to read it. the routers are only supposed to look at the destination address
in the packet header, but there’s nothing to stop them from peeking at the
contents. most ip packets in the world go over just a handful of high-
speed connections between lightning-fast routers, known as the internet
backbone. all packets between distant points, the united states and
japan, for example, go through only a few routers.
it’s hard for an individual hacker to monitor the entire internet, but
it’s easy for him to monitor a small piece of it. all he has to do is to gain
access to some computer on the network. then he can watch all the
packets going through, looking for interesting ones. if he gets access to a
machine close to company a, he will probably be able to monitor all the
traffic in and out of that company. (of course, by “close to” i mean “near
on the network,” and not necessarily physically near.) if he gets a machine
nowhere near company a, he might see little (or none) of that com-
pany’s traffic. if he’s a quintessential hacker and doesn’t care what com-
pany he eavesdrops on, then it doesn’t really matter.
packets with passwords in them are particularly interesting. password
sniffing is easy, and a common internet attack. an attacker installs a packet
178 c h a p t e r  e l e v e n
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 178


 
 

____________________________________________

SecretsAndLies.pdf page: 207

____________________________________________

sniffer designed to steal usernames and passwords. all the program does is
collect the first two dozen (or so) characters of every session that requires
a login and save them for the attacker. these characters almost certainly
contain the username and password (usually the unencrypted password).
then the attacker runs a password cracker on the encrypted passwords,
and uses those passwords to break into other computers. it’s difficult to
spot because password sniffers are small and inconspicuous. and it can
snowball. once you’ve broken into one machine, you can install a pass-
word sniffer on it and get even more passwords. maybe you can use those
passwords to break into other machines. and so on.
not only is eavesdropping possible, but active attacks are also possible
. . . easier, actually. in most communications systems, it is far easier to pas-
sively eavesdrop on a network than it is to actively insert and delete mes-
sages. on the internet, it is reversed. it’s difficult to eavesdrop. however,
it’s easy to send messages; any self-respecting hacker can do that. because
communications are packet-based, and they travel along many different
paths and are reassembled at the destination, it’s easy to slip another packet
in with the rest of them. many, many attacks are based on blindly insert-
ing packets into existing communications channels.
it’s called ip spoofing, and it’s easy. packets have source and destination
information, but an attacker can modify them at will. an attacker can cre-
ate packets that seem to come from one site, but don’t really. computers
on the internet assume that the “from” and “to” information is accurate,
so if a computer sees a packet from a computer it trusts, it assumes that the
packet is trusted. an attacker can take advantage of this trusting relation-
ship to break into a machine: he sends a packet purporting to come from
a trusted computer in the hope that the target computer will trust the
packet.
there are routing attacks, where an attacker tells two points on the
internet that the shortest route between them goes through his comput-
ers. this makes eavesdropping on a particular node easier. this section
could go on and on; whole books have been written about attacks against
the internet.
the solutions to these problems are obvious in theory, but harder in
practice. if you encrypt packets, no one can read them in transit. if you
authenticate packets, no one can insert packets that pretend to come from
somewhere else, and deleted packets will be noticed and reacted to.
network security 179
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 179


 
 

____________________________________________

SecretsAndLies.pdf page: 208

____________________________________________

in fact, several solutions encrypt packets on the internet. programs
like ssh encrypt and authenticate shell connections from a user on one
machine to a computer across the network. protocols like ssl can
encrypt and authenticate web traffic across the internet. protocols like
ipsec promise to be able to encrypt and authenticate everything.
dns security
the domain name service (dns) is basically a large distributed database.
most computers on the internet—nodes, routers, and hosts—have a
domain name like “brokenmouse.com” or “anon.penet.fi”. these names
are designed to be remembered by people, and are used to build things
like urls and e-mail addresses. computers don’t understand domain
names; they understand ip addresses like 208.25.68.64. ip addresses are
then used to route packets around the network.
among other things, the dns converts domain names to ip
addresses. when a computer is handed a domain name, it queries a dns
server to translate that domain name into an ip address. then it knows
where to send the packet.
the problem with this system is that there’s no security in the dns
system. so when a computer sends a query to a dns server and gets a
reply, it assumes that the reply is accurate and that the dns server is hon-
est. in fact, the dns server does not have to be honest; it could have been
hacked. and the reply that the computer gets from the dns server might
not have even come from the dns server; it could have been a faked
reply from somewhere else. if an attacker makes changes in the dns
tables (the actual data that translates domains to ip addresses and vice
versa), computers will implicitly trust the modified tables.
it’s not hard to imagine the kinds of attacks that could result. an
attacker can convince a computer that he is coming from a trusted com-
puter (change the dns tables to make it look like the attacker’s computer
is a trusted ip address). an attacker can hijack a network connection
(change the dns tables so that someone wanting to connect to legiti-
mate.company.com actually makes a connection with evil.hacker.com).
an attacker can do all sorts of things. and dns servers have a viral update
procedure; if one dns server records a change, it tells the other dns
180 c h a p t e r  e l e v e n
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 180


 
 

____________________________________________

SecretsAndLies.pdf page: 209

____________________________________________

servers and they believe it. so if an attacker can make a change at a few
certain points, that change can propagate across the internet.
in one attack in 1999, someone hacked the dns system so that 
traffic to network solutions—they’re one of the companies that register
domain names—was redirected to other domain-name registration com-
panies. a similar attack, from 1997, was a publicity attack. this was before
domain registration was opened up for competition. eugene kashpureff,
owner of the alternative alternic, redirected network solutions traffic
to his site as a protest. he was arrested and convicted, and received two
years’ probation.
in 2000, rsa security’s homepage was hijacked by spoofing the
dns tables. this is not the same as breaking into the web site and defac-
ing the page. the attacker created a fake home page, and then redirected
legitimate traffic to that faked page by manipulating the dns records.
the hacker did this not by cracking rsa’s dns server, but the dns
server upstream in the network. clever, and very easy. dns record
spoofing is a trivial way to spoof a real web site crack. and to make mat-
ters worse for the hijacked site, the hijacking misleads people into
thinking intruders cracked the web site at company a, when intruders
actually cracked the dns server at company b.
these problems are serious, and cannot easily be fixed. cryptographic
authentication will eventually solve this problem, because no longer will
computers implicitly trust messages that claim to come from a dns
server. currently people are working on a secure version of the dns
system that will deal with these issues, but it’s going to be a long wait.
denial-of-service attacks
in september 1996, an unknown hacker or group of hackers attacked the
computers of public access networks corporation (a.k.a. panix), a new
york isp. what they did was to send hello messages (syn packets) to the
panix computers. what’s supposed to happen is for a remote computer to
send panix this hello message, for panix to respond, and then for the
remote computer to continue the conversation. what the attackers did
was to manipulate the return address of the remote computers, so panix
ended up trying to synchronize with computers that essentially did not
network security 181
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 181


 
 

____________________________________________

SecretsAndLies.pdf page: 210

____________________________________________

exist. the panix computers waited 75 seconds after responding for the
remote computer to acknowledge the response before abandoning the
attempt. the hackers flooded panix with as many as 50 of these wake-up
messages per second. this was too much for the panix computers to han-
dle, and they caused the computers to crash. this is called syn flooding.
this was the first publicized example of a denial-of-service attack
against an internet host. since then, there have been many others. denial
of service is a particularly noxious attack against communication systems,
because communications systems are designed for communications. on
the net, flooding a computer with requests to communicate is a good
way to bring it crashing down. and often the technology doesn’t exist to
trace who originated the attack.
here’s a denial-of-service attack against someone’s paper mail: an
attacker signs the victim up for every mail-order catalog, credit card solic-
itation, and everything else he can think of. the victim gets so much mail,
maybe 200 pieces a day, that the real mail gets lost among the junk mail.
theoretically, this attack will work. the only thing preventing this attack
is the limit of the amount of junk mail in the world. on the internet,
though, the mail system always delivers the mail. in 1995, the internet
liberation front (it’s just a made up name; they’ve never been heard from
since) sent a flood of e-mail messages to author joshua quittner and wired
magazine. the flood was so great the computers just crashed.
this is known as mail bombing, and is an effective attack. send enough
mail to someone and that person’s system will fill until the computer
crashes. the easiest way to do this is to subscribe the victim to thousands
of mailing lists. victims’ disks might run out of space, their network con-
nections might go down, or their computers might crash. and if you dis-
guise the origin of the e-mail, no one will catch you.
there are other denial-of-service attacks. some target computers, like
the preceding mail-server attack. some target routers. some target web
servers. the basic idea is the same: flood the target with so much stuff that
it shuts down. winnuke can crash older windows 95 computers; some-
one, in a single attack, brought down 6,000 windows 95 computers on
the internet in april 1999. denial-of-service attacks against web sites are
common, and remote-cache services like akamai will make them easier
to mount and harder to detect.
182 c h a p t e r  e l e v e n
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 182


 
 

____________________________________________

SecretsAndLies.pdf page: 212

____________________________________________

by closing the vulnerability. but if the attacker has a bigger fire hose than
you do, he can flood your connection.
denial-of-service attacks are not intrusions. they do not affect the
data on the web sites. these attacks cannot steal credit card numbers or
proprietary information. they cannot transfer money out of bank
accounts or trade stocks in someone else’s name. attackers cannot directly
profit from these attacks. (they can sell the stock short and then attack the
company.)
this is not to say that denial-of-service attacks are not real, or not
important. for most big corporations, the biggest risk of a security breach
is loss of income or loss of reputation, either of which is achieved
elegantly by a conspicuous denial-of-service attack. and for companies
with more mission- or life-critical data online, a denial-of-service attack
can literally put a person’s life at risk.
distributed denial-of-service attacks
distributed denial-of-service attacks are just a virulent strain of denial-of-
service attacks. the first automatic tools for these attacks were released in
1999—the university of minnesota was the first public target in august
1999—but the spate of high-profile attacks in early 2000 put them on the
front pages of newspapers everywhere.
these attacks are the same as traditional denial-of-service attacks, only
this time there is no single source of the attack. the attacker first breaks
into hundreds or thousands of insecure computers, called zombies, on the
internet and installs an attack program. then he coordinates them all to
attack the target at the same time. the target is attacked from many places
at once; its traditional defenses just don’t work, and it falls over dead.
it’s much like the pizza delivery attack: alice doesn’t like bob, so she
calls a hundred pizza delivery parlors and, from each one, has a pizza
delivered to bob’s house at 11:00 p.m. at 11, bob’s front porch is filled
with 100 pizza deliverers all demanding their money. it looks to bob like
the pizza mafia is out to get him, but the pizza parlors are victims, too.
the real attacker is nowhere to be seen.
these attacks are incredibly difficult, if not impossible, to defend
against. in a traditional denial-of-service attack, the victim computer
might be able to figure out where the attack is coming from and shut
184 c h a p t e r  e l e v e n
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 184


 
 

____________________________________________

SecretsAndLies.pdf page: 213

____________________________________________

down those connections. but in a distributed attack, there is no single
source. the computer should shut down all connections except for the
ones it knows to be trusted, but that doesn’t work for a public internet
site.
there have been several academic conferences on distributed denial-
of-service attacks in recent years, and the consensus is that no general
defense exists. continuously monitoring your network connections
helps, as does the ability to switch to backup servers and routers. some-
times the particular bugs exploited in the attacks can be patched, but
many cannot. the internet was not designed to withstand this class of
attacks.
these attacks are likely to get worse. current distributed denial-of-
service tools require the attacker to break into a large number of
machines, install the zombie programs, keep those zombie programs from
being discovered, and coordinate the attack . . . all without getting
caught. neoteric tools are likely to use a virus, worm, or trojan horse
program to propagate the zombie tools, and then to automatically launch
the attack with some code word from a public forum.
there has already been one denial-of-service attack that worked this
way. in 1999, someone posted a fake internet explorer update from
microsoft. it was really a trojan horse that caused the infected computer
to send packets to hosts belonging to the bulgarian telecommunications
company, causing denial-of-service problems for them for a long time.
tracing the attacker is also incredibly difficult. returning to the pizza
delivery example, the only thing the victim could do is to ask the pizza
parlors to help him catch the attacker. if everyone coordinated their
phone logs, maybe they could figure out who ordered all the pizzas in the
first place. something similar is possible on the internet, but it is unlikely
that the intermediate sites kept good logs. additionally, it is easy to dis-
guise your location on the internet. and if the attacker is in some eastern
european country with minimal computer crime laws, a bribable police,
and no extradition treaties, there’s nothing you can do anyway. 
the real problem is the hundreds of thousands, possibly millions, of
nescient computer users who are vulnerable to attack. they’re using dsl
or cable modems, they’re always on the internet with static ip addresses,
and they can be taken over and used as launching pads for these (and
other) attacks. the media is focusing on the mega e-corporations that are
under attack, but the real story is the individual systems.
network security 185
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 185


 
 

____________________________________________

SecretsAndLies.pdf page: 218

____________________________________________

attacker will just go around the firewall and attack some undefended
connection.
and fourth point: castles need gates. it’s futile and absurd to build a
castle that can’t be penetrated by anyone under any circumstances: even
kings need to go outside and perambulate sometimes. merchants, mes-
sengers, even common townsfolk need to be able to go in and out regu-
larly. hence, castles had gatekeepers whose job it was to admit or turn
away people who wanted to enter the castle.
the great wall of china didn’t impress genghis khan. “the
strength of a wall depends on the courage of those who defend it,” he
supposedly said. letting the good stuff in while keeping the bad stuff out
is the central problem that any computer firewall needs to solve. it has to
act as gatekeeper. it has to figure out which bits are harmful and deny
them entry. it has to do this without unreasonably delaying traffic. (and
to your average internet user, an unreasonable delay is defined as one that
is noticeable.) it has to do this without irritating legitimate users. (your
average internet user will not tolerate not being able to do something, like
downloading a new internet game from suspicious software™ or con-
necting remotely and reading e-mail from an untrusted machine.) but if
the firewall’s gatekeeper makes a mistake, some hacker can sneak in and
own the network.
there are three basic ways to defeat a firewall. the first i talked about:
go around it. a large network has lots of connections. large photocopiers
often come with internet connections, and some network equipment
comes with dial-up maintenance ports. companies often hook their net-
works to the networks of suppliers, customers, and so forth; sometimes
those networks are much less protected. employees will hook personal
modems up to their computers so they can work at home. there’s a story
of a married couple in silicon valley who occasionally worked from
home. he was checking his e-mail while his wife was doing some pro-
gramming, both of them on their small home network. suddenly, his
company’s computers started showing up on her company’s network and
vice versa.
the second, and more complicated attack, is to sneak something
through the firewall. to do this, you have to fool the firewall into think-
ing you are good, honorable, and authorized. depending on how good
the firewall is and how well it has been installed, this is either easy,
difficult, or next to impossible. 
190 c h a p t e r  t w e l v e
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 190


 
 

____________________________________________

SecretsAndLies.pdf page: 219

____________________________________________

the basic idea is to create a piece of code that the firewall lets inside
the network. the code is designed to exploit some kind of bug in the
computer system that will open a connection between the hacker outside
the firewall and the computer inside the firewall. if it all works, the hacker
gets inside.
the third attack is to take over the firewall. this is akin to bribing or
blackmailing the gatekeeper. since he is now in your employ, he’ll do
what you want. again, how easy this is depends on the firewall. some
firewalls run buggy software, which helps. some run on top of insecure
operating systems, which helps a lot.
anyway, firewall design today is all about designing smart gatekeep-
ers. at the simplest level, a firewall is a router with a consistent rule set that
it tests network traffic against, and then passes traffic that meets the rules
and drops all other traffic. examples might be to restrict traffic based on
source or destination address or protocol type.
this was relatively easy in early networks, but today’s firewalls have
to deal with multimedia traffic, downloadable programs, java applets, and
all sorts of weird things. a firewall has to make decisions with only partial
information: it might have to decide whether or not to let a packet
through before seeing all the packets in a transmission.
early firewalls were something called packet filters. the firewall would
look at each packet and either admit or drop it, depending on a bunch of
rules about the packet header. the first packet filters were pretty dumb,
and let a whole lot of things in that were better left out. eventually they
got smarter. today they are stateful: instead of looking at each packet indi-
vidually, the firewall keeps information about the state of the network and
what types of packets are expected. still, firewalls only have so long a
memory, and slow attacks can often get through.
some good packet-filtering firewalls are out there, but they still dis-
play a number of weaknesses. first and foremost, they are a pain to con-
figure properly, and improper configuration often leads to security
vulnerabilities. lots of things are allowed in by default that should be
blocked. and the firewall doesn’t modify packets, so if a packet gets
through, it can do whatever it wants. and there are a bunch of more eso-
teric attacks against packet filters; just imagine fooling a guard who tries to
stop the flow of dangerous letters into a castle by looking at the envelopes.
network defenses 191
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 191


 
 

____________________________________________

SecretsAndLies.pdf page: 226

____________________________________________

taken this idea even further, and suggested that when a security vulnera-
bility is patched in a product, it should also be alarmed. 
honey pots are more involved: entire dummy computers and sub-
networks designed to look inviting to attackers. you can have fun with
these; name the computers something like transactions.bigcompany.com
or accounting.bank.com, dress them up with impressive-sounding
accounts and files, and protect them on your network. when an attacker
breaks into the network, he gravitates toward the honey pot because it
looks like an interesting place to explore. then an alarm goes off, and the
honey pot monitors the hacking activity and gathers data for prosecution.
some companies sell premade honey pots; just add enticing names.
what’s interesting about both of these measures is that they exploit
the one advantage the network administrator has over an attacker: knowl-
edge of the network. the administrator knows how the network is
supposed to look and what is supposed to happen. he can set burglar
alarms—just as a homeowner can set window alarms because he knows
that no one is supposed to open the windows, and motion sensors because
he knows that no one is supposed to be walking around in the living
room—using that knowledge. he can deploy honey pots with the
knowledge that no legitimate user will ever access those systems. he can
set up all sort of burglar alarms, turn them on and off at different times of
the day, move them around once in a while, do anything he wants. these
measures are effective precisely because the attacker doesn’t know if they
are there or where they would be. unlike a firewall or ids—an attacker
often knows what brand firewall is installed—burglar alarms and honey
pots are tailored specifically for the network being alarmed.
vulnerability scanners
the intent of vulnerability scanning is to have an automated program scan
your network (or computer) for a huge laundry list of known weaknesses.
it does the work, and then you get a tidy report of which weaknesses the
network has. then it’s up to you to fix them (or, i suppose, exploit them).
the reality of vulnerability scanners is not nearly so clean, and all vul-
nerability scanners on the market are massively flawed. if they worked the
way you expect them to work, they would all crash your computers and
198 c h a p t e r  t w e l v e
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 198


 
 

____________________________________________

SecretsAndLies.pdf page: 227

____________________________________________

damage your network. no one would use such a tool, so they all fake it.
imagine a vulnerability scanner for your house. one of the things it
checks is whether your windows are vulnerable to attack by a rock. the
obvious way to test this is to throw a rock against the window and watch
the results. but this would cause damage to the house, so the scanner fakes
it. it looks at the glass to see if it is single pane or double pane. maybe it
taps on it, to see if it is actually glass or a stronger plastic. maybe it tries to
read the part number on the window, and makes some assumptions about
the glass based on that. this is the same sort of thing that network vul-
nerability scanners have to do.
it gets worse. sometimes it’s hard to tell whether or not a particular
attack is successful. the same home vulnerability scanner now tests the
power reliability by trying to cut the power lines into the house. it cuts
the power line, and the lights stay on. does this mean that the scanner
failed to cut the power line, and the house is not vulnerable, or does it
mean that the house has a backup power system? or maybe the scanner
cuts the power lines and the power goes off. does this mean that the scan-
ner cut the power line, or that it did something else that, through some
contorted chain of events, resulted in the power being shut off? the scan-
ner doesn’t know, and most of the time has no way of figuring it out.
networks are unreliable; they don’t fail in neat ways.
even though vulnerability scanners can’t actually scan for vulnerabil-
ities, nor can they accurately measure the effects of their actions when
they can scan for vulnerabilities, they are not useless. they can scan for,
or at least fake scanning for, some vulnerabilities. they do produce a list
of vulnerabilities that a conscientious system administrator will close (and
a nefarious attacker will exploit). they work okay.
satan (security administrator tool for analyzing networks)
made a big press splash when it was released in 1995. it was portrayed in
the media as worse than its namesake, and its author was fired from his job
at sgi. since then vulnerability scanners have achieved respectability as a
component of a security administrator’s toolkit. several commercial prod-
ucts, with respectable names, are in the marketplace. think of these tools
as another audit technique: a private investigator that reports on your
security vulnerabilities. you can hire the p.i. to examine your own
system, but an attacker can hire the same p.i. to examine a target system.
but understand the limitations of the technology.
network defenses 199
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 199


 
 

____________________________________________

SecretsAndLies.pdf page: 228

____________________________________________

e-mail security
these days, e-mail is everywhere. anyone who has any presence at all 
in cyberspace has an e-mail address, and probably receives far too many
e-mail messages every day. e-mail has no built-in security.
like any network packet, any machine between the source and the
destination can read e-mail. (you can even see the names of some of those
machines in the headers of your received mail.) the common metaphor
used for internet e-mail is postcards: anyone—letter carriers, mail sorters,
nosy delivery truck drivers—who can touch the postcard can read what’s
on the back. and there’s no way of verifying the signature on a letter or
the return address (you do know that the “from” field in your mail
header can easily be forged?), so there’s no way of knowing where a mes-
sage really came from. (spammers use this feature to hide the origin of
their mass mailings.) if an attacker wants to be subtle, he can actually con-
nect (without an account) to the forged machine of origin and send the
mail from there. if he doesn’t care, he can just forge the “from” line.
we want two things for e-mail. one, we want to make sure that no
one other than the intended recipient can read the message. two, we
want to make sure that an e-mail message came from the person it
purports to have come from, and that no one can forge e-mail messages.
the cryptography to protect e-mail is simple and straightforward, and
dozens of products on the market deal with the problem. here’s the basic
protocol:
1. alice gets bob’s public key.
2. alice signs her message with her private key.
3. alice encrypts her message with bob’s public key.
4. alice sends the encrypted and signed message to bob.
5. bob decrypts the message using his private key.
6. bob verifies alice’s signature using her public key.
where you’re going to see difficulties is in the public keys: how you
get them, store them, verify them. i’ll talk about this a lot more in
chapter 15.
200 c h a p t e r  t w e l v e
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 200


 
 

____________________________________________

SecretsAndLies.pdf page: 234

____________________________________________

a more recent example is the java security model. java has a complex
security model to shield computers from malicious java applets. a pro-
gramming error anywhere in the protection mechanisms can potentially
render them all useless, and since its inception, a steady stream of imple-
mentation-specific java attacks have exploited a variety of different flaws.
what makes all these examples more troublesome than the ariane
flaw (although less incandescent) is that the bugs that were used to break
security did not affect performance. they were there, undetected, until
they were found and exploited. this is a big deal, and why security is
harder than reliability. the ariane bug is one that affects performance.
once a performance bug is found—and beta testing can find them—it
can be fixed. security bugs don’t affect performance, and don’t show up
in beta test results. i’ll talk more about testing security in chapter 22, but
the moral is that while people can sometimes stumble onto security flaws,
only experienced experts can reliably discover them.
this kind of thing happens all the time. when someone skilled per-
forms a security analysis of a piece of security software, he always finds
random flaws that compromise security. always. the more complex the
code, the more security flaws.
security problems, once discovered, will be exploited until they are
fixed. assume an attacker finds a security flaw in a commerce protocol
that allows him to steal credit card numbers or, even worse, money. if he’s
in it for the publicity, he’ll announce his exploit to the press and it will be
fixed. (hopefully, he’ll alert the company first.) if he’s in it for the money,
he will make use of the flaw, again and again. he’ll steal as much as he can
until someone else notices the flaw and fixes it. this is an important dif-
ference: flaws that affect performance are noticed, while security flaws
can remain invisible for a long time.
these flaws are not necessarily in the security portion of the code,
either. they can be anywhere in the code: the user interface, the error-
handling routines, anything. and as we saw in chapter 10, even programs
that don’t have anything to do with computer security can affect the secu-
rity of networked computers. flaws in your word processor, your printer
driver, or your multimedia player can all compromise the security of your
computer.
the other moral is that software bugs (and therefore, security flaws)
are inevitable. just as it is inconceivable that the ariane 5 software could
be completely bug-free—the unfortunate accident is that the bug had
206 c h a p t e r  t h i r t e e n
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 206


 
 

____________________________________________

SecretsAndLies.pdf page: 236

____________________________________________

networked computers work the same way. the computer has a set of
protocols that it follows—logon procedures, access restrictions, password
protections—that it uses to figure out who can come in and who can’t.
someone who follows the protocols correctly can get in. someone who
doesn’t, can’t.
one way to defeat a protocol like this is to modify the actual com-
puter program. or, back to our analogy, it’s like slipping a page into the
clerk’s employee manual. imagine that the manual is written for people
who are none too bright. each page is a step, kind of like a “choose your
own adventure” novel: “if the customer gives you a credit card, go to the
next page. if the customer gives you cash, go to page 264.” the dealing
with a federal express driver” steps might look like this:
page 163: take the package. if the driver has one, go to the next page. if the
driver doesn’t have one, go to page 177.
page 164: take the signature form, sign it, and return it. go to the next page.
page 165: ask the driver if he or she would like to purchase something. if the
driver would, go to page 13. if not, go to the next page.
page 166: ask the driver to leave. if he or she does . . . and so on.
there’s one last piece of setup. whenever the 7-11 clerk gets some-
thing, she puts it on top of the open page in her manual. she can’t look at
the new thing any other way.
here’s the attack: we’re going to dress up like a fedex driver, and
then slip a page into the clerk’s manual when we give her the signature
form. what we’ll do is give the clerk two pages instead of one. top page
will be a signature form. the bottom page will be a fake employee-man-
ual page:
page 165: give the driver all the money in the cash register. go to the next
page.
this will work. the clerk takes the package on page 163. she goes to
page 164 and takes the signature form (and our fake page). she puts them
both on top of the open manual. she signs and returns the form (leaving
the fake page on top of the manual), and when she returns to the manual
she gets our fake page instead. she gives us all the money in the register
and turns to the next page (the real page 165). we can tell her we don’t
want to buy anything, and leave. if the 7-11 clerk is really as dumb as a
208 c h a p t e r  t h i r t e e n
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 208


 
 

____________________________________________

SecretsAndLies.pdf page: 237

____________________________________________

computer system, we can get away with it. we can use this trick to
persuade the 7-11 clerk to let us into the stockroom or to do whatever
else we want. by slipping a page into her employee manual, we can give
her arbitrary instructions.
essentially, this is the way to exploit a buffer overflow bug in a com-
puter system. computers store everything, programs and data, in mem-
ory. if the computer asks a user for an 8-character password and receives
a 200-character password, those extra characters may overwrite some
other area in memory. (they’re not supposed to—that’s the bug.) if it is
just the right area of memory, and we overwrite it with just the right char-
acters, we can change a “deny connection” instruction to an “allow
access” command or even get our own code executed.
the morris worm is probably the most famous overflow-bug exploit.
it exploited a buffer overflow in the unix fingered program. it’s
supposed to be a benign program, returning the identity of a user to
whomever asks. this program accepted as input a variable that is supposed
to contain the identity of the user. unfortunately, the fingered program
never limited the size of the input. input larger than 512 bytes overflowed
the buffer, and morris wrote a specific large input that allowed his rogue
program to execute as root and install itself on the new machine. (this
particular bug has, of course, been fixed.)
what makes this worm especially relevant for this section is that it
itself had a programming bug. it was supposed to hop between comput-
ers on the internet, copy itself onto each server, and then move on. but a
typo in the code made the worm copy itself not once, but indefinitely, on
each computer. the result was that computers infected by the worm
crashed. over 6,000 servers crashed as a result; at the time that was about
10 percent of the internet.
skilled programming can prevent this kind of attack. the program
can truncate the password at 8 characters, so those extra 192 characters
never get written into memory anywhere. it’s easy to do, but it’s hard to
do everywhere. the problem is that with any piece of modern, large,
complex code, there are just too many places where buffer overflows 
are possible (and they’re not all as simple as this example) that it is difficult
to squash them all. it’s very difficult to guarantee that there are no over-
flow problems, even if you take the time to check. the larger and more
complex the code is, the more likely the attack.
software reliability 209
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 209


 
 

____________________________________________

SecretsAndLies.pdf page: 240

____________________________________________

212
14
secure hardware
this is an ancient idea. it began when the first person drew a line
across his cave entrance, proclaimed that what was on one side 
of the line was his, and then proceeded to defend his cave against
all who disagreed with him. the notion covers a lot of different things:
computer rooms behind locked doors and armed guards, tamper- resistant
set-top boxes for pay-tv, secure tokens for access control, smart card
chips for electronic commerce applications, and a bomb that blows up if
you try to defuse it. the physical instantiation of the secure perimeter is
different in each of these cases, but the fundamental benefit of the idea is
the same: “it’s a whole lot easier to design a computer security system if
we can leverage the innate physical security of a device, and assume that
parts of the system cannot be accessed by large classes of people.”
and that’s true. it’s easier to design a secure pay-for-parking system if
you assume that crooks can’t empty the parking meters into their pockets.
it’s easier to design a secure library if you assume that people can’t sneak
books out of the building inside their overcoats. and it’s easier to design
an electronic wallet if you assume that people can’t arbitrarily modify the
amount of money they have.
here’s a perfect cashless monetary system: everyone carries around a
piece of paper with a number on it representing the number of ducats in
his wallet. when someone spends money, he crosses out the number and
writes the lower number. when he receives money, he does the oppo-
site. if everyone is honest, this system works. as soon as someone notices
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 212


 
 

____________________________________________

SecretsAndLies.pdf page: 241

____________________________________________

that he can write whatever number he wants on the paper, the system falls
apart.
however, this was almost exactly the system that precomputer banks
used to keep track of depositors’ accounts. each depositor had a bank-
book stored in a file cabinet in the bank, and another in his possession.
the bankbooks had a number that represented the amount of money the
person had stored in the bank. when he deposited or withdrew money,
the bank wrote a new number in both books. the system  didn’t fall apart,
because one of the books was kept within the secure perimeter of the
bank. and that was the real book; the book the depositor got was just a
copy for his mollification. if a depositor forged a line in his bankbook, it
would not match with the book stored in the bank. the bank teller
would notice the discrepancy, presumably check other records to make
sure there actually was attempted fraud, and prosecute accordingly. the
customer could not modify the book in the bank because he could not get
through the secure perimeter. (the teller, of course, had many more
opportunities to commit fraud.)
this example illustrates the benefit of a secure perimeter; the security
wouldn’t work without one.
we can build an anonymous cash card system the same way. cus-
tomers walk around with smart cards in their wallets. the smart card con-
tains a memory location with a dollar amount stored in it, much the same
as the bankbook. smart cards talk to each other through some kind of
point-of-sale terminal. when a customer buys something, her smart card
subtracts the amount of purchase from the amount in memory and writes
the lower number back into memory. when a merchant sells something,
his smart card adds the amount of purchase into the memory location.
the cards only do this in pairs (secret keys in the cards can easily enforce
this), so that everything balances out at the end. and to stop someone
from just going into the card and changing his balance, the cards are tam-
perproof.
wasn’t that easy? the secure perimeter around the card—secrets
within the card stay within the card, and people outside the card can’t
affect those secrets—makes a lot of security problems go away. without
it, the only way to make a system like this work is through a tedious back-
end processing system.
checks work rather like the first example i talked about: people
keeping a paper in their wallet listing their current account balance. peo-
secure hardware 213
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 213


 
 

____________________________________________

SecretsAndLies.pdf page: 244

____________________________________________

defeating it or how much money it would cost to buy the equipment they
used, but someone at a lab across town could use different techniques and
come up with a different figure. and remember the publicity attack:
some grad student somewhere could borrow equipment and defeat your
tamper resistance just for fun. or maybe a criminal could buy the equip-
ment and expertise. this is nowhere near as straightforward as estimating
the time and money it would cost to implement a brute-force attack
against a cryptographic algorithm.
and even if it were possible to figure out how effective a tamper-
resistance technique is today, that says nothing about how effective it
would be tomorrow, or next year, or five years from now. advances in
this field happen all the time. advances come from a variety of technolo-
gies, and they interact in really interesting ways. what was difficult to
defeat one year might be trivial to defeat the next. it’s naïve to rely on
tamper resistance for any long-term security.
another option is to make the system tamper evident. this is easier to
do than making it tamper resistant: we don’t care if someone can tamper
with the system, we just care that he can’t do it undetectably. imagine a
tamper-evident hand-held gambling device. a player can take it home
with him and win or lose money. because we are going to let the player
take the device home with him, and we know that he can potentially win
thousands of dollars, we do our best to make it tamper resistant. but
because we know that true tamper resistance is impossible, we actually
rely on tamper evidence. when he returns the gambling device to collect
his winnings, we are going to inspect it up one side and down the other.
we’re going to install seals that have to be broken, coatings that have to
be removed, wires that have to be cut. sure, the best attackers can do all
of that, but they can’t do it all and then undo it all after they’re done.
better, but still not good enough. i believe that no system can be
absolutely tamper evident, although there are different degrees. relying
on it as a sole security measure is a mistake.
none of this stops the physical world from using these concepts.
many systems make use of antitampering devices, from aspirin bottles to
nsa-designed cryptographic chips. this is not necessarily a bad thing:
tamper resistance protects systems from most people and most attacks. i
worry when systems rely on tamper resistance for security, instead of
using it as just one aspect of a more comprehensive security system.
216 c h a p t e r  f o u r t e e n
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 216


 
 

____________________________________________

SecretsAndLies.pdf page: 245

____________________________________________

one system that uses tamper resistance effectively as part of a larger
control mechanism is the u.s. system for controlling nuclear weapons.
the risk is real: some rogue commander could launch weapons without
permission, or tactical nuclear weapons could be stolen or (if they were
stored at an american base overseas) seized by an ally during a crisis.
there was a need to ensure that nuclear weapons could only be launched
in the event of a directive from washington. the solution uses something
called a pal, a permissive action link, details of which are still secret. we
do know that pals are only considered useful if they are buried deep
within a large and complex weapon system. simpler weapons are stored
in special containers, paps (prescribed action protective systems), that
provide an extra tamper-resistant barrier.
the tamper resistance in nuclear weapons includes various booby
traps: chemicals that render the nuclear material useless, small explosives
that destroy critical components of the weapon and the attacker, and so
forth. only cryptographic codes transmitted from washington will disarm
these tamper-resistance mechanisms and arm the nuclear weapon itself.
these protection mechanisms are extreme, but this is an extreme sit-
uation. there are extreme situations in the commercial world—root ca
keys (see chapter 15), keys used by banks to secure interbank wire trans-
fers—but the security measures come from carefully crafted systems, not
mass-produced products. in the normal commercial world, the protection
measures are much more pedestrian.
and there is a fundamental difference of control. the nuclear weapon
is under extreme physical control; this makes tamper-resistance measures
more effective.
think of a slot machine. a slot machine has a secure perimeter. if you
can open up the slot machine, you can take all the money out or, more
dangerously, modify the roms so that it pays a jackpot. but that slot
machine is on a casino floor. there are lights, cameras, guards, people . .
. if someone goes anywhere near that slot machine with a drill or a screw-
driver, he is going to get arrested. now imagine the casino says something
like this: “here’s a slot machine. take it home. play all you want. bring
it back in a few months. whatever is on the pay line, we’ll pay.”
this is now a different situation. the attacker can take the slot
machine home to his basement lab. he can study the machine all he
wants. he can x-ray it. he can even buy several identical machines from
secure hardware 217
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 217


 
 

____________________________________________

SecretsAndLies.pdf page: 246

____________________________________________

the manufacturer and take them apart. in the end, he is much more likely
to be able to attack the system in his basement than the one sitting on the
casino floor. and this holds true not only for slot machines, but atms,
bank safe deposit boxes, and anything with a similar security model.
(this is not to say that slot machines on a casino floor are invulnera-
ble. dennis nikrasch made a good living—about $16 million total—rip-
ping off slot machines. he practiced on slot machines at home, and
learned how to open a machine up on the casino floor—without setting
off the alarms—and swap firmware chips. blockers stood between him
and the cameras. then he would leave, and an accomplice would play the
rigged machines for the jackpot.)
the morals of this section are simple. one, tamper resistance is largely
a myth, but it does provide a barrier to entry. two, tamper resistance
should be augmented by other countermeasures. and three, any system
where the device and the secrets within the device are under the control
of different people has a fundamental security flaw. it’s possible to design
a secure system that includes this flaw, but it must be recognized as a flaw.
side-channel attacks
in the last few years, new kinds of cryptanalytic attacks have begun to
appear in the literature: attacks that target specific implementation details.
the timing attack made a big press splash in 1995: rsa private keys could
be recovered by measuring the relative times cryptographic operations
took. this attack has been successfully implemented against smart cards
and other security tokens, and against electronic commerce servers across
the internet.
researchers have generalized these methods to include attacks on a
system by measuring power consumption, radiation emissions, and other
side channels, and have implemented them against a variety of public-key
and symmetric algorithms in tamper-resistant tokens. related research has
looked at fault analysis: deliberately introducing faults into cryptographic
processors in order to determine the secret keys. the effects of this attack
can be devastating.
let’s assume that an attacker wants to learn the secret keys inside a
tamperproof module: a smart card, a pcmcia card, or something like
218 c h a p t e r  f o u r t e e n
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 218


 
 

____________________________________________

SecretsAndLies.pdf page: 247

____________________________________________

that. he can’t cryptanalyze the algorithms or protocols (they’re too good),
and he can’t defeat the tamper resistance. but the attacker is clever; instead
of just looking at the inputs and outputs, he’s going to look at the speed
in which the module does things. the critical observation in the timing
attack is that many implementations of cryptography do things at different
speeds for different keys. knowing what speed a certain operation takes
yields information about the key. knowing a lot of different speeds for
different operations can yield the entire key.
imagine the attack working against a stockroom; you want to know
about its contents. you can’t look in the stockroom to see how things are
arranged. however, you can ask the clerk to get stuff for you. by timing
how long it takes him to get different things, you can learn a lot about the
stockroom. does he always take a long time to get toner cartridges? then
they must be in the back of the room. does he take longer to get reams
of paper every ten requests? then they must come in boxes of ten. does
he take longer to get pencils if you’ve just asked him to get erasers? that
tells you something about what boxes get stacked on top of each other.
here’s a timing attack against a password checker. try a random pass-
word, but vary the first character. so if there are 26 letters, capital and
lowercase, ten numbers, and a handful of punctuation marks, try about 70
passwords. just possibly, one will take longer to be rejected than the
others. just possibly, this is the first correct character. repeat with the rest
of the characters. if you are attempting to attack an eight-character pass-
word, you only have to try 560 passwords and measure their timings.
the attacker doesn’t have to limit himself to timing. he can look at
how much power is dissipated for different operations. (the module can
use different amounts of power to do the same operation, depending on
the key.) he can look at how much heat is radiated, and even where on
the module it radiates from. for example, power attacks have been used to
pry secrets out of almost all smart cards on the market.
these attacks are possible because the module is in the attacker’s
hands. if the module were sitting in a locked vault, he couldn’t perform
these kinds of attacks. (although he might be able to attack another copy
of the same product, which might provide some interesting information.)
but precisely because the system’s designers relied on tamperproof hard-
ware and were willing to give the attacker a copy of the module, he can
perform these systemic attacks.
secure hardware 219
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 219


 
 

____________________________________________

SecretsAndLies.pdf page: 249

____________________________________________

open; there’s a lot to be learned while it is still working properly.
cutting it open is always interesting, though, especially if you can cut
it open without killing it. if we defeat the tamper resistance and do this to
the module, we can learn a lot about its security.
fault analysis is another powerful attack, because cryptography is sen-
sitive to small changes. in chapter 7, i talked about how easy it is to
incorrectly implement cryptography, destroying its security in the
process. in fault analysis, an analyst purposely introduces flaws into the
cryptographic implementation—in specific places designed to maximize
the amount of information leaked. combining this with defeating the
tamper resistance—cutting a lead here and a lead there (not at random,
but specific ones)—is a devastating attack against secure modules.
systemic attacks are not low-budget attacks. you aren’t likely to see
them carried out by lone criminals or common terrorists. they are attacks
for well-funded adversaries: organized crime, some industrial competitors,
military intelligence organizations, and academic laboratories. they work,
and work well. systems such as smart cards would do well to assume that
systemic attacks are possible, and ensure that even if successful they can-
not defeat the security of the system.
side-channel attacks don’t necessarily generalize to other systems. a
fault-analysis attack just isn’t possible against an implementation that does-
n’t permit an attacker to create and exploit the required faults. but these
attacks can be much more powerful than standard cryptanalytic attacks
against algorithms. for example, a published differential-fault-analysis
attack against des requires between 50 and 200 ciphertext blocks (no
plaintext) to recover a key. it only works on certain tokens implementing
des in a certain way. contrast this with the best non-side-channel attack
against des, which requires just under 64 terabytes of plaintext and
ciphertext encrypted under a single key.
some researchers have claimed that this is cheating. true, but in real-
world systems, attackers cheat. their job is to recover the key, not to fol-
low some arbitrary rules of conduct. prudent engineers of secure systems
anticipate this and adapt to it. it is our belief that most operational crypt-
analysis makes use of side-channel information. sound as a side channel—
listening to the rotation of electromechanical rotor machines—was
alluded to in david kahn’s book the codebreakers. the u.s. military has
long made a big deal about tempest. and in his book spycatcher, peter
secure hardware 221
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 221


 
 

____________________________________________

SecretsAndLies.pdf page: 254

____________________________________________

this will become important later in this chapter. but first, let’s talk
about trusted third parties.
trusted third parties
cryptographers define a trusted third party as someone trusted by every-
one involved in a protocol to help complete the protocol fairly and
securely. a friend at the nsa once said (with remarkable perspicuity):
“someone whom you know can violate your security policy without get-
ting caught.” oddly enough, these definitions are basically the same.
remember the various trusted third party protocols from chapter 7?
all commerce, with the exception of direct barter, uses trusted third par-
ties in some way. even cash transactions: the seller is trusting the gov-
ernment to back the currency he is accepting. when the transaction
involves an interesting financial instrument—a check, a credit card, a
debit card, a traveler’s check—both the buyer and the seller are relying on
the bank or financial company to behave properly. the merchant and the
customer don’t necessarily trust each other, but the trusted third party is
able to successfully mediate a transaction between them. things would
fall apart pretty quickly if a credit card company started capriciously refus-
ing to accept merchant slips for certain cardholders.
lawyers act as trusted third parties in more personal roles: executors
of wills, that sort of thing. when someone announces to her captors, “if
you kill me, my lawyer will mail a copy of the evidence to the fbi,
cnn, and the new york times,” she is using her attorney as a trusted
third party. lawyer jokes aside, the profession makes a pretty good trusted
third party.
the entire civil court system can be viewed as a trusted third party,
ensuring that contracts are fulfilled and that business is conducted prop-
erly. here’s the fair contract protocol: alice and bob negotiate and sign a
contract. if one of them feels that the other is not upholding his or her end
of the contract, he or she calls in the trusted third party: the judge. the
judge listens to the evidence from both sides, and then makes a ruling.
this works because both alice and bob believe that the judge will be
fair. in jurisdictions where the legal system is corrupt or incompetent, you
see a much smaller reliance on contracts and a radically different set of
rules for conducting commerce.
226 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 226


 
 

____________________________________________

SecretsAndLies.pdf page: 258

____________________________________________

remember chapter 6 and public-key cryptography? alice uses pub-
lic-key cryptography to digitally sign things. she signs documents with
her private key, and sends the signed document to bob. now bob needs
alice’s public key to verify the signature. where does he get it and how
is he sure it’s alice’s?
in the early days of public-key cryptography, people envisioned vast
databases of public keys, kind of like telephone books. bob could look
alice’s name up in the online database of public keys, and then retrieve
the public key associated with that name.
well, if everyone’s public key is going to be stored in a vast database
somewhere, what about the security of that database? an attacker can do
lots of malicious things if he can substitute one public key for another. he
can create a new public key, sign a bunch of checks with it, and then slip
it in the database next to alice’s name. suddenly, alice signed all of those
checks. if bob is using alice’s public key to encrypt a message to her, the
attacker can swap his public key for alice’s; now bob’s secret message to
alice can be decrypted by the attacker, and not by alice.
we might be able to secure the public-key database, but the whole
idea was to have public keys freely and widely available. this just isn’t
going to work.
certificates were the solution. a certificate is a binding between a
public key and an identity. a mutually trusted entity—call him god for
now—takes alice’s name and alice’s public key, sticks them together,
and then signs the whole mess. now bob has no worries. he gets alice’s
public-key certificate from somewhere—he doesn’t much care where—
and verifies god’s signature on it. bob trusts god, so if the signature is
valid he knows that the public key belongs to alice and not to some
imposter. problem solved; the world is now safe for electronic commerce.
well, not exactly. note that we haven’t actually solved the problem.
all we’ve done is taken the original problem, “how does bob know that
alice’s public key is really hers?” and changed it to: “how does bob
know that god’s public key is really his?” bob has to verify god’s signa-
ture on the certificate before he can use alice’s key, so he needs god’s
public key. and where is he going to get that?
but we did solve something. bob presumably wants to communicate
with a lot of people, not just with alice. and if god has signed everyone’s
certificate, we’ve reduced bob’s problem from verifying everyone’s pub-
230 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 230


 
 

____________________________________________

SecretsAndLies.pdf page: 259

____________________________________________

lic key to verifying just one public key: god’s. but let’s save that problem
for later.
a real certificate is a little more complicated. it contains information
about the person (his name, possibly his job title, possibly his e-mail
address, and other things about him), information about the certificate
(when it was issued, when it expires), information about the issuer or
signer (who he is, what algorithm he used to sign the certificate), and
information about the public key (what algorithm it is for) . . . as well as
the public key itself.
the basic idea is that alice gets a public key certificate signed by god
somehow. either she generates her own public-key/private-key key pair
and sends the public key to god, who returns the public-key certificate,
or god generates a public-key/private-key key pair for alice and sends
her both the private key and the public-key certificate. (now we have the
problem of securing this exchange, but never mind that for now.)
this all works great, until alice loses her private key. maybe some-
one stole it. maybe she just forgot it. (or, more likely, her computer
crashed and didn’t have a backup.) bob is going to try to send her
encrypted 
e-mail in that lost key. or, worse yet, bob is going to try to verify signa-
tures created after someone stole the key. what do we do now?
we tell god, and he revokes alice’s certificate. he declares it no
longer valid, no longer good, no longer correct. how does he do this? he
can’t go through every nook and cranny of the net and erase every copy
of the certificate. (well, maybe god can, but this is only an analogy.) he
probably doesn’t even know bob has a copy of it.
so, god puts alice’s certificate on the certificate revocation list, or crl.
the crl is a list of revoked certificates. (remember 20 years ago when
merchants had newsprint books listing bad credit card numbers? that’s a
crl.) god issues a crl at regular intervals (the credit card companies
did it once a week), and it is bob’s job to make sure that alice’s certificate
is not on the current crl before he uses it. he should also make sure that
it hasn’t expired, and that the certificate really does belong to alice.
how does he do that last one? he compares alice’s name with the
name on the certificate. if they match, then the certificate is hers. it
sounds simple, except that it doesn’t work.
this idea has several problems. first, there is no one to act as god.
or, more properly, there is no one organization or entity that everyone
certificates and credentials 231
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 231


 
 

____________________________________________

SecretsAndLies.pdf page: 260

____________________________________________

can agree on and whose judgment is unassailable. the second is that alice
has no single name that everyone can agree on.
first problem first. remember, for this whole system to work, alice
has to have her certificate issued by someone that both she and bob trust.
in reality, we use hierarchies of trust to establish the validity of certificates.
a military organization is probably the best example of this. the platoon
leader signs the certificates of everyone in his platoon. the division com-
mander signs the certificate of every platoon leader under him. the army
general signs the certificates of his divisional commanders. and so on, up
to the commander-in-chief.
alice now has a chain of certificates, from the commander-in-chief to
the army general to the divisional commander to the platoon leader to
her. she keeps them all, and presents them to bob. if she and bob are in
the same platoon, then bob also has the platoon leader’s certificate. he
knows that it is valid, so he can verify alice’s certificate directly. if bob is
in the same division as alice but in a different platoon, they share the same
divisional commander certificate. bob can use it to verify alice’s platoon
leader’s certificate, and then alice’s certificate. since alice and bob are in
the same military, someone is in both of their chains of command. it
might even be the commander-in-chief, who is “god” in this example.
this system works great in the military, but less well in the civilian
world. the internet uses certificates to fuel a lot of protocols: ipsec and
various vpn systems, ssl, a few electronic commerce protocols, some
login protocols. these certificates are issued to users by someone called a
certificate authority (ca). a ca can be a corporate security office. it can be
a government. it can be a private company that is in the business of issu-
ing certificates to internet users.
these cas also need certificates. (remember, there’s a hierarchy
here.) these ca certificates are issued by other cas (probably verisign).
eventually you get to the god in this system, or in reality a pantheon of
gods. the highest-level cas have what are known as root certificates;
they are not signed by anyone else. these certificates are embedded in the
software you buy: your browser, your vpn software, and so forth. this
is all called a public-key infrastructure (pki). it works, but only sort of.
second problem: alice’s name.
back in ancient times (the mid-1980s), someone dreamed about a
world where every individual, every process, every computer, every
communications device—anything connected to digital communica-
232 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 232


 
 

____________________________________________

SecretsAndLies.pdf page: 262

____________________________________________

problems with traditional pkis
pkis and cas have a raft of other problems. for example, what does it
mean when a ca claims that it is trusted? in the cryptographic literature,
this only means that it handles its own private keys well. this  doesn’t
mean you can necessarily trust a certificate from that ca for a particular
purpose: making a small payment or signing a million-dollar purchase
order.
who gave the ca the authority to grant such authorizations? who
made it trusted? many cas sidestep the question of having no authority
to delegate authorizations by issuing identity certificates. anyone can
assign names. we each do that all the time. this leaves the risk in the
hands of the verifier of the certificate, if he uses an identity certificate as if
it implied some kind of authorization. basically, certificates only protect
you from those that the pki vendor refuses to do business with.
and “authority” has several meanings. the ca may be an authority
on making certificates, but is it an authority on what the certificate con-
tains? for example, an ssl server certificate contains two pieces of data of
potential security interest: the name of the keyholder (usually a corporate
name) and the dns name for the server. there are authorities on dns
name assignments, but none of the ssl cas listed in the popular browsers
is such an authority. that means that the dns name in the certificate is
not an authoritative statement. there are authorities on corporate names.
these names need to be registered when one gets a business license.
however, none of the ssl cas listed in the browsers is such an author-
ity. in addition, when some server holds an ssl server certificate, it has
permission to do ssl. who granted the authority to an ssl ca to con-
trol that permission? is the control of that permission even necessary?
what harm would be done if an uncertified server were allowed to use
encryption? none.
some cas, in response to the fact that they are not authorities on the
certificate contents, have created a two-part certification structure: a reg-
istration authority (ra), run by the authority on the contents. the idea is
that the ra is responsible for validating what’s in the certificate, and the
ca is responsible for issuing it.
the ra+ca model is categorically less secure than a system with a
ca at the authority’s (i.e., the ra’s) desk. the ra+ca model allows
some entity (the ca) that is not an authority on the contents to forge a
234 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 234


 
 

____________________________________________

SecretsAndLies.pdf page: 263

____________________________________________

certificate with those contents. of course, the ca would sign a contract
promising not to do so, but that does not remove the capability. mean-
while, since security of this model depends on the security of both pieces
and the interaction between them (they have to communicate somehow),
the ra+ca is less secure than either the ra or the ca, no matter how
strong the ca or how good the contract with the ca. of course, the
model with a ca at the authority’s desk (not at the vendor’s site) violates
some pki vendors’ business models.
another problem involves the protection of the private key. re -
member, for the whole digital-signature system to work, you have to be
sure that only you know your private key. okay then, how do you pro-
tect it? you almost certainly don’t own a secure computing system with
physical access controls, tempest shielding, “air wall” network secu-
rity, and other protections; you store your private key on a conventional
computer. there, it’s subject to attack by viruses and other malicious pro-
grams. even if your private key is safe on your computer, is your com-
puter in a locked room, with video surveillance, so that you know no one
but you ever uses it? if it’s protected by a password, how hard is it to guess
that password? if your key is stored on a smart card, how attack-resistant
is the card? if it is stored in a truly attack-resistant device, can an infected
computer convince the trustworthy device to sign something you didn’t
intend to sign?
this matters mostly because of the term nonrepudiation. like
“trusted,” this term is taken from the literature of academic cryptography.
there it has a specific meaning: that the digital-signature algorithm is not
breakable, so a third party cannot forge your signature. pki vendors have
latched onto the term and used it in a legal sense, lobbying for laws to the
effect that if someone uses your private signing key, then you are not
allowed to repudiate the signature. in other words, under some digital sig-
nature laws (e.g., utah’s and washington’s), if your signing key has been
certified by an approved ca, then you are responsible for whatever that
private key does. it does not matter who was at the computer keyboard
or what virus did the signing; you are legally responsible.
the way it’s supposed to work is that when you know your key is
compromised, you put it on a crl. anything signed after that time is
automatically repudiated. this sounds plausible, but the system is funda-
mentally flawed. bob wants to know that alice’s key hasn’t been com-
promised before he accepts her digital signature. the attacker is not going
certificates and credentials 235
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 235


 
 

____________________________________________

SecretsAndLies.pdf page: 264

____________________________________________

to announce the compromise to alice. so, alice’s first clue that her key
was compromised will come when she gets some notice from bob show-
ing evidence of the fraudulent signature. in most schemes, this will hap-
pen only after bob accepts the signature.
contrast this with the practice regarding credit cards. under mail-
order/telephone-order (moto) rules, if you object to a line item on
your credit card bill, you have the right to repudiate it—to say you  didn’t
buy that—and the merchant is required to prove that you did.
there are similar vulnerabilities in the computer that does the verifi-
cation. certificate verification does not use a secret key, only public keys.
but to verify a certificate, you need one or more “root” public keys: the
public keys of the cas. if the attacker can add his own public key to that
list, then he can issue his own certificates, which will be treated exactly
like the legitimate certificates. they can even match legitimate certificates
in every other field except that they would contain a public key of the
attacker instead of the correct one.
some pki vendors claim that these keys are in root certificates, and
hence secure. such a certificate is self-signed and offers no increased secu-
rity. the only answer is to do all certificate verification on a computer sys-
tem that is invulnerable to penetration by hostile code or to physical
tampering.
and finally, how did the ca identify the certificate holder? whether
a certificate holds just an identifier or some specific authorization, the ca
needs to identify the applicant before issuing the certificate.
several credit bureaus thought they would get into the ca business.
after all, they had a vast database on people, so, the thinking ran, they
should be able to establish someone’s identity online with ease. if you
want to establish identity online, you can do that provided you have a
shared secret with the subject and a secure channel over which to reveal
that secret. ssl provides the secure channel.
the trouble with a credit bureau serving this role is that they don’t
have a secret shared only with the subject. in other words, there isn’t a
secure offline id that can be used to bootstrap the process. this is because
credit bureaus are in the business of selling their information to people
other than the subject. worse, because credit bureaus do such a good job
at collecting and selling facts about people, others who might have infor-
mation about a subject are probably hard pressed to find any datum shared
with the subject that is not already available through some credit bureau.
236 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 236


 
 

____________________________________________

SecretsAndLies.pdf page: 265

____________________________________________

this puts at risk commercial cas that use credit bureau information to
verify identity online; the model just doesn’t work.
meanwhile, having identified the applicant somehow, how did the
ca verify that the applicant really controlled the private key correspond-
ing to the public key being certified? some cas don’t even consider that
to be part of the application process. others might demand that the appli-
cant sign some challenge right there on the spot, while the ca watches.
certificates aren’t like some magic security elixir, where you can just
add a drop to your system and it will become secure. certificates must be
used properly if you want security. are these practices designed with solid
security reasons, or are they just rituals or imitations of the behavior of
someone else? many such practices and even parts of some standards are
just imitations which, when carefully traced back, started out as arbitrary
choices by people who didn’t try to get a real answer.
how is key lifetime computed? does the vendor use one year, just
because that’s common? a key has a cryptographic lifetime. it also has a
theft lifetime, as a function of the vulnerability of the subsystem storing it,
the rate of physical and network exposure, attractiveness of the key to an
attacker, and so forth. from these, one can compute the probability of loss
of key as a function of time and usage. does the vendor do that compu-
tation? what probability threshold is used to consider a key invalid?
does the vendor support certificate or key revocation? crls are built
into some certificate standards, but many implementations avoid them.
but if crls are not used, how is revocation handled? if revocation is sup-
ported, how is compromise of a key detected in order to trigger that revo-
cation? can revocation be retroactive? that is, can a certificate holder
deny having made some signature in the past? if so, are signatures dated so
that one knows good signatures from suspect ones? is that dating done by
a secure timestamp service?
how long are the generated public keys and why was that length
chosen? does the vendor support shorter, and weaker, rsa keys just
because they’re fast or longer keys because someone over there in the cor-
ner said he thought it was secure?
does the proper use of these certificates require user actions? do users
perform those actions? for example, when you establish an ssl connec-
tion with your browser, there’s a visual indication that the ssl protocol
worked and the link is encrypted. but who are you talking securely with?
unless you take the time to read the certificate that you received, you
don’t know.
certificates and credentials 237
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 237


 
 

____________________________________________

SecretsAndLies.pdf page: 273

____________________________________________

steganography
steganography is the science of hiding messages in messages. herodotus
talks of the ancient greek practice of tattooing a secret message on the
shaved head of a messenger, and letting his hair grow back before sending
him through enemy territory. (the latency of this communications sys-
tem was measured in months.) invisible ink is a more modern technique.
microdots were invented by the germans during world war i, and
stayed in vogue for many years. spies would photograph an image such
that the image on the negative was small enough to cut out and place over
a period of a book. the spy would carry the book around, secure that no
one would find the microdot hidden on one of its many pages.
in the computer world, steganography has come to mean hiding
secret messages in graphics, pictures, movies, or sound. the sender hides
the message in the low-order bits of one of these file types—the quality
degrades slightly, but if you do it right it will hardly be noticeable—and
the receiver extracts it at the other end. several commercial and freeware
programs offer steganography, either by themselves or as part of a com-
plete communications security package.
steganography offers a measure of privacy beyond that provided by
encryption. if alice wants to send bob an e-mail message securely, she
can use any of several popular e-mail encryption programs. however, an
eavesdropper can intercept the message and, while she might not be able
to read it, she will know that alice is sending bob a secret message.
steganography allows alice to communicate with bob secretly; she can
take her message and hide it in a gif file of a pair of giraffes. when the
eavesdropper intercepts the message, all she sees is a picture of two
giraffes. she has no idea that alice is sending bob a secret message. alice
can even encrypt it before hiding it, for extra protection.
so far, so good. but that’s not how the system really works. the
eavesdropper isn’t stupid; as soon as she sees the giraffe picture she’s going
to get suspicious. why would alice send bob a picture of two giraffes?
does bob collect giraffes? is he a graphics artist? have alice and bob been
passing this same giraffe picture back and forth for weeks on end? do they
even mention the picture in their other correspondence?
the point here is that steganography isn’t enough. alice and bob
must hide the fact that they are communicating anything other than
security tricks 245
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 245


 
 

____________________________________________

SecretsAndLies.pdf page: 275

____________________________________________

example, is a perfectly reasonable stenographic data channel: alice and
bob need to tell each other whether a particular action is either “safe” or
“dangerous.” that’s one bit of information. they regularly exchange
recipes over e-mail, and agree that the key phrase “double the recipe” will
be the message indicator. if the e-mail says that the recipe can be doubled,
then the action is safe. if the e-mail says that the recipe cannot be doubled,
then the action is dangerous. any recipe without the phrase does not con-
tain a message.
this kind of system works because the secret message is much, much
smaller than the overt message, and is generally called a subliminal channel
(similar to a covert channel from chapter 8). subliminal channels are as
old as computers, and have been used by unscrupulous programmers to
leak information without the user’s consent. imagine that you’re a pro-
grammer designing a report on banking customers, and you want to get
your hands on the customers’ pins. you’re not authorized to examine
the real data, but you’ve been trusted to write the code to produce the
report from the database that contains the pins. and you can see the real
reports after they are produced. program the report generator to add
spaces after each customer’s entry, 0 through 9, corresponding to one
digit of the customer’s pin. have the report generator use the first digit
one day, the second digit the second day, and so forth until it is done, and
then cycle back to the first digit. that’s it. if the programmer can get his
hands on the electronic report for four consecutive days, he can recover
everyone’s pin. (actually, he has four possibilities for each pin, depend-
ing on which digit the report generator used when, but that’s easy to deal
with.) no one else reading the reports will see anything unusual, and
unless they examine the code that generates the reports (and how often
will that happen?) they will never know that the pins are being leaked.
there is the story of a soldier who was not allowed to say where he
was stationed. he didn’t have a middle initial, and sent a series of letters to
his girlfriend with a different middle initial in each; over time he spelled
out where he was stationed.
once you get the general idea, you can think of all sorts of ways to
embed subliminal channels in documents: the choice of fonts and font
sizes, the placement of data and graphics on a page, the use of different
synonyms in text, and so on. many cryptographic protocols allow for sub-
liminal channels in the choice of parameters, in the random bits used for
security tricks 247
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 247


 
 

____________________________________________

SecretsAndLies.pdf page: 277

____________________________________________

for example, a watermark on the little mermaid would say something
like “property of disney,” while a fingerprint on the same digital movie
would say something like: “purchased by alice, 1/1/01.”
digital watermarks (and fingerprints) go one better, though. copy
the paper, and the watermark disappears. copy the digital file, and the
watermark goes with the copy. maybe we can’t stop copying, disney rea-
sons, but we can at least point the finger at whoever copied it in the first
place. and i’ve seen watermarks proposed for a lot of things: graphics,
images, video, audio . . . even stock ticker data and computer programs.
so, depending on what data you put into the watermark, they can do
one of two things. first, they can identify the original copyright holder.
second, they can identify both the original copyright holder and the per-
son who bought the copy: if every copy of the little mermaid is water-
marked with the name and address of the person who bought it, then
when a copy appears on the internet, disney can identify the culpable
party.
great idea, but it just won’t work.
the problem is that in order for disney to be able to take a copy of
the little mermaid and find the embedded watermark, it has to be find-
able. and if disney can find it, a pirate can find it, too. companies that
market this stuff try to tell you that their watermarking schemes can’t be
removed for this or that technobabble reason.
it just isn’t true. as with a subliminal channel, it is virtually impossi-
ble to find a good watermark unless you know exactly where to look. but
unlike a subliminal channel, the detection mechanism will eventually be
made public. either it will leak into the hacking community like every-
thing else does, or it will be made public the first time a court case turns
on watermarking evidence. the mechanisms for watermarking will even-
tually become public, and when they do, they can be reverse engineered
and removed from the image.
reversal might not be easy. ingenious tricks can make it difficult, but
they can’t make it impossible. and a sagacious hacker can write an
automatic tool to strip the watermark, once he knows how it works.
another vulnerability is that watermarking doesn’t solve the underly-
ing problem. what watermarking does is allow a company to point to its
unaltered digital property and say: “that’s mine.” this is hardly enough
to be useful because digital property is so easy to alter, and watermarking
doesn’t prevent someone from altering the digital property. it also does-
n’t guarantee that the person identified by the watermark is the culprit.
security tricks 249
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 249


 
 

____________________________________________

SecretsAndLies.pdf page: 278

____________________________________________

imagine that every copy of the little mermaid is watermarked with the
identity of the buyer. how does the merchant verify the buyer’s identity?
unless we have hard-to-forge identity documents—either real or vir-
tual—this system won’t work. and there’s nothing to stop a counterfeiter
from paying $10 to a homeless drunk to walk into the video store and buy
the movie for him. he now has a movie with the embedded watermark
of someone who probably doesn’t care if disney knows his identity, and
who doesn’t have any assets if disney tries to sue.
watermarking can help convict grandma when she duplicates a single
copy of the little mermaid for all her grandchildren, but it can’t stop the
taiwanese pirates from ripping out the watermarks and selling half a mil-
lion pirate copies on the black market. or someone using a fictitious
identity to purchase the legitimate copy and then not worrying about it.
copy protection
this problem is easy to describe, and much more difficult to solve. soft-
ware companies want people to buy their products; they hate it when
someone makes a copy of a business program that costs hundreds of dol-
lars and gives it to a friend. (actually, these days they kind of like it. they
realize that the friend probably wouldn’t have bought it anyway, that he’ll
use the software and get “hooked,” and when he eventually goes legit,
either he or his boss will buy a legal copy of the same program—and not
a competitor’s. wordperfect used this scheme to increase its popularity.)
this is especially important with computer games and in countries with
little respect for intellectual property: in these cases, lots of users will pirate
rather than buy a legitimate copy. (this same problem applies to people
who want to distribute content—books, movies, videos, and so forth—
that they don’t want copied.)
there are all sorts of solutions—embedded code in the software that
disables copying, code that makes use of non-copyable aspects of the orig-
inal disk, hardware devices that the software needs to run—and i’m not
going to talk about them in detail. they all suffer from the same basic
conceptual flaw: it is impossible to copy-protect software on a  general-
purpose computer.
in the hands of joe average computer user, any copy protection sys-
tem works. he can copy ordinary files by following the directions, but has
250 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 250


 
 

____________________________________________

SecretsAndLies.pdf page: 279

____________________________________________

no idea how to defeat a reasonably sophisticated copy protection scheme.
in the hands of jane hacker, no copy protection system works.
the problem is that jane controls her computer. she can run debug-
gers, reverse engineer code, analyze the protected program. if she’s smart
enough, she can go into the software and disable the copy protection
code. the manufacturer can’t do a thing to stop her; all it can do is make
her task harder. but to jane, the challenge entices her even more.
there are a bunch of janes out there who break copy protection
schemes as a hobby. they hang out on the net, trading illegal software.
there are also those who do it for profit. they work in china, taiwan,
and elsewhere, removing copy protection code and reselling the software
on cd-rom for less than a tenth of the retail price. they can disable the
most sophisticated copy protection mechanisms. the lesson from these
people is that any copy protection scheme can be broken.
the dongle is the current state of the art in copy protection. it’s a piece
of hardware that plugs into the computer, usually into the parallel port.
(conflicts with other devices using the port, and other dongles, are only
problems occasionally.) the protected software calls the dongle at various
points during execution; for example, every thousand keypresses or
mouseclicks, when a user tries to save, or every time he selects the nail
gun as his weapon. if the dongle doesn’t respond to a call, or responds
incorrectly, the software stops running. or, more effectively, it keeps run-
ning but gives subtly wrong answers. (a 1992 version of autodesk’s 3d
studio used the dongle to create a table in memory that was required to
correctly mirror three-dimensional geometry. removing the dongle
caused the program to fail over the course of a few hours, imperceptibly
at first, but eventually dramatically. autodesk had to field a lot of calls
from unregistered users complaining about a strange bug in their version
of 3d studio.)
calls to the dongle are all encrypted, and the dongle itself is protected
from hardware reverse engineering by a variety of tricks. still, programs
that use dongles are routinely broken without attacking either the
cryptography or the tamper resistance.
how? instead of defeating the dongle, hackers go through the code
and remove all calls to it. it’s painstaking work: hackers have to go
through the code line by line, function by function, call by call. they may
have to hook a logic analyzer up to the dongle and correlate execution
addresses to dongle accesses. a sophisticated program could contain tens
security tricks 251
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 251


 
 

____________________________________________

SecretsAndLies.pdf page: 280

____________________________________________

of megabytes of code. but remember chapter 2 and my first reason why
the internet is different from the physical world: only one smart pirate
has to succeed; everyone else can just use that person’s unprotected
version of the software.
the success of software pirates doesn’t stop companies from trying to
copy-protect their programs. the 1996 quake release came on an
encrypted cd-rom: you could try it for free, but had to call the com-
pany and buy the password to unlock the entire game. it was eventually
cracked, along with every other popular copy-protected program ever
released.
hacked programs are called warez, and you can amass a collection of
the stuff yourself just by looking around the internet. you won’t find
manuals, but that’s what all the computer books are for. just about every-
thing is available, usually for trade.
copy protection gurus like to point to new technologies to save their
industry. they call for a unique serial number on the computer’s micro-
processor, so that every legitimate copy of a program could be pro-
grammed to work only on one particular computer. they talk about
encryption capabilities on the motherboard. none of this will work. all
of it will keep joe average from copying his software, but none will stop
jane hacker from dismantling the program and posting a cracked warez
version for everyone to download.
this duality of risks is no different from the watermarking problem.
look at the videotape industry: piracy is much lower than when vcrs
were new because of two reasons. one, the dinky copy protection is
respected by all vcrs, thwarting joe average. and two, the retail price
of videotapes is so cheap that the economic incentive to jane hacker has
lessened.
what’s really interesting about the problem of copy protection and
software piracy is that the solution is to pretend that there’s not a problem:
there is little to no copy protection in business software. in the compet-
itive software application industry, market share and product loyalty—
however they are achieved—are crucial. many companies reason as
follows: people who pirate my software cost my company next to
nothing, since my marginal cost of goods is zero. it’s not like they are
stealing televisions off my assembly line. almost all people who pirate my
software can’t afford to pay for it, so i’m not losing many sales. and when
these pirates eventually get into a situation where they need to buy the
252 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 252


 
 

____________________________________________

SecretsAndLies.pdf page: 282

____________________________________________

these microscopes are expensive (although amateur versions are
getting cheaper), and these attacks are probably only feasible for govern-
ments. if you are worried about a government, the only real way to erase
a magnetic disk is to shred or burn it.
data is also hard to erase in hardware. both sram and dram
retain some remnants of the data after losing power. bits in ram can be
recovered by electronically detecting changes in cell thresholds based on
previous cell content. modifying the temperature and voltage can affect a
chip’s ability to erase data. there’s a lot of physics that can be applied to
the problem of recovering data after it has been erased.
u.s. military cryptography equipment is built to erase, or zeroize, all
keys if tampered with. this is hard for two reasons: it is hard to erase data,
and it is hard to know when to erase data. there has to be some set of sen-
sors that determines when a box is being tampered with. there are obvi-
ous sensors: voltage, current, light, temperature. but if an attacker knows
what the sensors are, he can probably defeat all of them. (he can work in
a room lit by a wavelength that the sensor misses, or can vary the temper-
ature slowly enough as to fool the sensor, or whatever.) again, this is a
problem mostly for government systems and government attackers, but it
is a very difficult one.
part of the difficulty is that the device needs to reliably retain the key
under normal circumstances, and entirely obliterate it under abnormal
circumstances. the very technology used to reliably retain key bits makes
it difficult to obliterate the key bits. conflicting goals are hard to handle
well.
where this problem affects commercial systems is in things like smart
cards, pay-tv boxes, and any other device with secrets inside that the
device owner should not know. i talked about tamper resistance and ways
to defeat it. zeroization techniques are a way to defend against those sorts
of attacks. but there are ways to attack zeroization. basically, commercial
systems don’t get this right—i only know of one commercial device with
the government fips 140-3 zeroization certification—because it’s just
too expensive to do so.
254 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 254


 
 

____________________________________________

SecretsAndLies.pdf page: 287

____________________________________________

computers make all sorts of mistakes all the time, and malicious hackers
are happy to lead computers down a mistake-riddled garden path, and to
take advantage of those mistakes.
a friend installed a burglar alarm system in his home. the alarm was
wired into the burglar alarm company’s switchboard; when it went off,
the company was automatically alerted, and then they would call the
police. my friend had a secret code that he could use to call the alarm
company and register a false alarm (the police didn’t want to send a squad
car out whenever someone accidentally tripped the alarm). he also had a
second secret code, a duress code. this code meant: “there is a gun being
held to my head, and i am being forced to call you and claim that this is a
false alarm. it isn’t. help!”
one day my friend accidentally tripped the burglar alarm, and he
dutifully called the alarm company to register it as a false alarm. acciden-
tally, he gave them the duress code instead of the false alarm code. almost
immediately he realized his mistake and corrected it. the woman on the
other end gave a huge sigh of relief and said: “thank god. i had no idea
what i was supposed to do.”
when an alarm condition, or even an error condition, appears a few
times a week, people know what to do. if it only happens once every few
years, there could be an entire office that has never seen the alarm, and
hence has no idea what to do. many attacks target complacent users. dur-
ing the attack, those involved can’t imagine that the system is failing, and
attribute the problem to something else. remember chernobyl? “i’ve
never seen that red blinking light before. i wonder what it means. . . .”
this is why we all went through fire drills in primary school. we had
to practice the failure conditions, less so we would be prepared for what
happened—drills can only prepare someone so well for a panic situa-
tion—but as a constant reminder that the failure could occur. i’ve never
been in a real fire, but i’ve been drilled so often in what to do, i’ll proba-
bly be all right. it’s the same with airplanes. when oxygen masks drop
from the ceiling, you don’t want the passengers glancing up from their
novels, wondering what those silly things are, and then going back to
their reading. nor do you want bank tellers ignoring warning signs. “the
bank computer said that i should give him one million dollars in cash.
who am i to second-guess the computer?” or a nuclear power plant
operator wondering what that flashing red light means.
the human factor 259
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 259


 
 

____________________________________________

SecretsAndLies.pdf page: 289

____________________________________________

people want security, but they don’t want to see it working. it is
instructive to talk with people who remember when a front door lock
was first installed on their house. there are some of these people still alive,
usually in rural areas. (city houses have had door locks for centuries; rural
areas went without them for a long time.) these people talk about what
an imposition a front door lock was. they didn’t think it was right that
they had to fish around for a key, put it in a lock, and then turn the key .
. . just to get into their own home. and the first time they forgot or lost
their key—the shame of it all. sure, crime was a problem and front door
locks were a good thing, but people fought the change. i still know peo-
ple who leave their doors unlocked. (note the flawed “it’s never hap-
pened to me” reasoning in a lot of these cases.)
computer security is no different. find someone who used comput-
ers before there were passwords and permissions and limitations. ask
them how much they liked it when security measures were added. ask
them if they tried to get around the security, just because it was easier.
even today, when the deadline approaches and you have to get the job
done, people don’t even think twice about bypassing security. they’ll
prop the fire door open so that someone can get into the building more
easily, and they’ll give out their password or take down a firewall because
work has to get done. john deutch, the former director of the cia,
brought classified files home with him on his insecure laptop—because it
was easier.
it’s a trade-off. security is easiest when it is visible to the user, when
the user has to interact with the security and make decisions based on it:
that is, checking the name on a digital certificate. on the other hand, users
don’t want to see security. and a smart security designer doesn’t want
users to see security. a smart security designer knows that users find secu-
rity measures intrusive, that they will work around them whenever possi-
ble, that that they will screw with the system at every turn. people can’t
be trusted to implement computer security policies, just as they can’t be
trusted to lock their car doors, not lose their wallets, and not tell anyone
their mother’s maiden name.
they can’t be trusted to do things properly. in a 1999 usability study
at carnegie mellon university, researchers found that most people could
not use the pgp e-mail encryption program correctly. of the 12 people
who participated in a cmu experiment, eight never managed to figure
out how pgp 5.0 worked. four of them accidentally sent out unen-
crypted messages that revealed confidential information. and this is with
the human factor 261
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 261


 
 

____________________________________________

SecretsAndLies.pdf page: 290

____________________________________________

the program’s easy-to-use graphical interface (although, to be fair, the
pgp versions 6.0 or later have a better user interface).
and they can’t be trusted to make intelligent security decisions. after
the melissa and worm.explorezip scares of 1999, you might think peo-
ple learned not to open attachments they weren’t expecting. but the
infection rate from the iloveyou worm (and its dozens of variants)
taught us that no, people cannot be trained not to open attachments . . .
especially when so many companies are trying to make a business getting
users to send each other interesting attachments.
browsers use digital certificates in order to make secure ssl connec-
tions. when they accept the certificates, they optionally display the iden-
tification of the certificate on the other end. this is essential to the
security; it makes no sense to have a secure connection unless you are sure
who is on the other end of that connection. most people don’t bother
looking at the certificates, and don’t even know they should (or how to).
the same browsers have an option to display warnings when down-
loading java applets. the user is asked whether he trusts the particular
web site that is sending the applet. the user has no idea whether or not
he trusts the web site. nor does he care. if j. random websurfer clicks
on a button that promises dancing pigs on his computer monitor, and
instead gets a hortatory message describing the potential dangers of the
applet—he’s going to choose dancing pigs over computer security any
day. if the computer prompts him with a warning screen like: “the applet
dancing pigs could contain malicious code that might do perma-
nent damage to your computer, steal your life’s savings, and impair your
ability to have children,” he’ll click “ok” without even reading it. thirty
seconds later he won’t even remember that the warning screen even
existed.
human–computer transference
when i introduced cryptography in chapter 6, i wrote about alice and
bob encrypting, decrypting, signing, and verifying messages and docu-
ments. i wrote, for example, that alice could use public-key cryptogra-
phy to send a message to bob by finding bob’s key in a phone book, and
then encrypting a message to bob using this key. this is actually a
complete lie. alice never encrypts messages to bob. she never decrypts
262 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 262


 
 

____________________________________________

SecretsAndLies.pdf page: 292

____________________________________________

and call the washington post. meanwhile, the trojan horse erases itself and
everything is back to normal.
there’s an easy implementation in windows: a malicious macro
could simply watch for pgp’s “open file” dialog, see what file alice is
about to sign, and copy its own file to that filename, then restore the old
file afterward. word’s macro language can do this, so it could easily be a
payload for a word macro virus.
and that’s just one example. the trojan horse could sign both doc-
uments and transmit the embarrassing signature at some opportune time.
or it could just steal alice’s private key.
nothing here is difficult; the programming is easy. in any case, if we
are successful we could have possession of a damaging document, signed
by alice. we could wave it around in court or pass it to a reporter, cor-
rectly claiming that alice’s valid digital signature is on the bottom of the
document. what is more likely to happen is the reverse. as soon as some-
one writes a fake signature trojan horse, it will be assumed to be every-
where. whenever a document appears in court, one side or the other will
find an expert witness that will testify as to the existence of the trojan
horse and how easy it would be to get someone to unknowingly sign just
about anything. can the court trust this digital signature? it doesn’t
depend on the mathematics; it depends on the circumstances.
the fundamental problem is that you have no idea what the com-
puter is actually doing when you tell it to do something. when you tell
the computer to save a document, or encrypt a file, or calculate the sum
of a column of numbers, you really have no assurance that the computer
did it correctly, or even at all. you’re making a leap of faith. just as it is
hard to catch a thieving employee, it’s hard to catch a malicious computer
program. actually, it’s worse. think of it as a malicious employee who
works alone, with no one watching. all of the monitoring equipment
you might install to catch the employee—hidden cameras, hidden micro-
phones—are controlled by the malicious employee. all you can do is look
at what inputs the employee accepts and what outputs he produces. and
even then you can’t be sure.
if alice can’t trust the computer she is working on, then she can’t
trust it to do what she asks. just because she asked it to sign a particular
document doesn’t mean that it can’t sign another document. the meta-
solution is for alice to only sign documents on a trusted computer, but
264 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 264


 
 

____________________________________________

SecretsAndLies.pdf page: 294

____________________________________________

take tokens from riders, and ring them up as passholders. eventually man-
agement figured this out and arrested the clerks (1991); low estimates
were that hundreds of thousands of dollars was stolen. once honest clerks
started working at some stations, daily receipts doubled. this problem
remained unfixed for years.
companies try to reduce the risk of malicious insiders in many ways.
“hire honest people” is the best solution, although it’s easier said than
done. some companies go so far as to conduct integrity screening—pre-
employment honesty tests—for some positions. others try to diffuse trust,
to limit the amount of damage one person can do. think of public code
reviews. audit is vital: for being able to determine what damage an insider
did, and for being able to convict him in court. in the end, though, an
organization is at the mercy of its people.
social engineering
in 1994, a french hacker named anthony zboralski called the fbi office
in washington, pretending to be an fbi representative working at the
u.s. embassy in paris. he persuaded the person at the other end of the
phone to explain how to connect to the fbi’s phone-conferencing sys-
tem. then he ran up a $250,000 phone bill in seven months.
similarly, it’s a common hacker trick to telephone unsuspecting em -
ployees and pretend to be a network system administrator or security
manager. if the hacker knows enough about the company’s network to
sound convincing, he can get passwords, account names, and other sensi-
tive information from the employee. in one instance a hacker posted fly-
ers on a company bulletin board announcing a new help-desk phone
number: his own. employees would call him regularly, and he would col-
lect their passwords and account data in exchange for help.
social engineering is the hacker term for a con game: persuade the other
person to do what you want. it’s very effective. social engineering
bypasses cryptography, computer security, network security, and every-
thing else technological. it goes straight to the weakest link in any secu-
rity system: the poor human being trying to get his job done, and wanting
to help out if he can.
sadly, this is easier than you think. showing up at a computer room
with some hardware in hand and an appropriate vendor’s badge is often
266 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 266


 
 

____________________________________________

SecretsAndLies.pdf page: 295

____________________________________________

enough to give someone free rein. wandering around and asking if there
is a place to “park and work” for a while will often result in a desk and a
network connection; that person is obviously a corporate visitor.
most social engineering is done on the telephone, which makes the
perpetrator harder to catch. one attacker called people and said, “this is
the operator. i have a collect call from <insert name> in <insert city>.” if
the victim accepted the call, the operator continued: “your collect call
option is blocked. please give me your calling card number and i will con-
nect the call.” this really happened. the attacker found people on usenet
newsgroups and invented collect calls from people they corresponded
with in the newsgroup, an extra touch of verisimilitude.
when kevin mitnick testified before congress in 2000 he talked
about social engineering: “i was so successful in that line of attack that i
rarely had to resort to a technical attack,” he said. “companies can spend
millions of dollars toward technological protections and that’s wasted if
somebody can basically call someone on the telephone and either con-
vince them to do something on the computer that lowers the computer’s
defenses or reveals the information they were seeking.”
another social-engineering attack, this one against credit cards: alice
steals bob’s credit card number. she could charge purchases to bob’s
account, but she’s wilier than that. she advertises merchandise—cameras,
computers, whatever—at a very cheap price. carol sees the advertisement
and buys a product from alice. alice orders the product from a legitimate
retailer, using bob’s credit card number. the retailer ships the product to
carol—there’s so much drop-shipping going on that the packing slip
doesn’t have the price—and is stuck when bob notices the charge. even
worse: carol is inculpated, not alice.
automated social engineering can work against large groups; you can
fool some of the people all the time. in 1993, subscribers to the new
york isp phantom access received this portentous, forged, e-mail mes-
sage: “it has been brought to my attention that your account has been
‘hacked’ by an outside source. the charges added were significant, which
is how the error was caught. please temporarily change your password to
‘dph7’ so that we can judge the severity of the intrusion. i will notify
you when the problem has been taken care of. thank you for your help
in this matter. —system administrator.” and in 1999, aol users were
persistently receiving messages like: “a database error has deleted the
information for over 25,000 accounts, and yours is one. in order for us to
the human factor 267
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 267


 
 

____________________________________________

SecretsAndLies.pdf page: 296

____________________________________________

access the backup data, we do need your password. without your pass-
word, we will not be able to allow you to sign onto america online
within the next 24 hours after your opening of this letter.”
plausibility plus dread plus novelty equals compromise.
modern e-mail-borne viruses and worms use automatic social engi-
neering to entice people to open them. the iloveyou worm cloaked
itself in e-mail from people the recipient knew. it had a plausible subject
line and message body, enticing the recipient to open the attachment. it
hid the fact that it was a vbscript file, and pretended to be a harmless text
file. i talked about this in chapter 10; people don’t stand a chance against
these social-engineered viruses.
in some of these instances, technology can help. if the helpful
employees had access tokens (or biometric readers) in addition to pass-
words, they couldn’t give everything away to the nice man on the tele-
phone. if the computers had biometric fingerprint readers, there would be
no passwords to give away. if the computer system were smart enough to
recognize that someone was logging in from a remote location when the
job description states that he only works in the office, maybe someone
could have been alerted.
sometimes simple procedures can prevent social engineering. the
u.s. navy has safes with two locks (with different combinations, of
course); each combination is known by a different person. it’s much
harder to social engineer those combinations. there are probably other
tricks that the computers could have done, all designed to limit what a
duped legitimate user could give to a social engineer. technology can
certainly make the job of the social engineer harder, in some cases a lot
harder.
in the end, social engineering will probably always work. look at it
from the view of the victim, bob. bob is a good guy. he works at this
company, doing whatever low-level or mid-level job he was hired to do.
he’s not a corporate security officer. sure, he’s gotten some security
training, and might even know to be on the watch for those churlish
hackers. but bob is basically clueless. he doesn’t understand the security
of the system. he doesn’t understand the subtleties of an attack. he just
wants to get his job done. and he wants to be helpful.
the social engineer, alice, comes to bob with a problem. alice is just
like bob, a cog in the big company machine. she needs to get her job
done, too. all she wants is for bob to tell her his username and password,
268 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 268


 
 

____________________________________________

SecretsAndLies.pdf page: 297

____________________________________________

or give her information about a phone number, let her install this hard-
ware box, or do one of any number of perfectly reasonable things. sure,
it might not be technically allowed, but alice has her butt on the line and
just has to do this one thing. everyone bypasses security procedures once
in a while in order to get the job done. won’t bob help? isn’t he a team
player? doesn’t he know what it’s like to have to get something done and
for there to be a stupid corporate rule in the way? of course he does. he’s
human.
and this is why social engineering works. people are basically helpful.
and they are easily duped. by appealing to bob’s natural tendencies, alice
will always be able to cozen what she wants. she can persuade bob that
she is just like him. she can telephone bob when he least expects it. she
knows that security just gets in the way of bob doing the job he was hired
for, and she can play to that. and if she gets it wrong, and bob doesn’t fall
for it, she can call on the tens or hundreds of other bobs in the organiza-
tion that can give her what she wants.
the human factor 269
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 269


 
 

____________________________________________

SecretsAndLies.pdf page: 302

____________________________________________

274
18
vulnerabilities and the
vulnerability landscape
in part 1 we looked at attacks in theory: what kinds of attacks there
are and what kinds of attackers there are. but as i have said elsewhere,
there is a difference between theory and practice. as anyone who
reads mystery novels or newspaper crime reports knows, there is a lot
more to an attack than simply finding a vulnerability. in order to success-
fully make use of that vulnerability, the attacker has to find a target, plan
the attack, do the deed, and get away. a vulnerability in a safe’s locking
mechanism, if that safe is hidden in a secret location, is not as serious as the
same vulnerability in a bank’s night-deposit box.
it’s no different in the digital world. it’s not enough for a potential
criminal to find a flaw in the encryption algorithm for the atm network.
he has to get access to the communications line, know enough about the
protocols to create a bogus message letting him steal money, actually steal
the money, and get away with the crime. without those other steps, the
encryption flaw is just of theoretical value. 
similarly, there is a lot more to a countermeasure than simply throw-
ing a piece of technology at the problem. that vulnerability in the safe
could be fixed by installing a stronger lock, or putting alarms on the doors
and windows of the room the safe is in and keeping a phalanx of guards
nearby. the encryption vulnerability could be fixed with a better encryp-
tion algorithm, or by keeping the protocols secret, encapsulating the
messages in a private network, or simply changing the keys every five
minutes.
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 274


 
 

____________________________________________

SecretsAndLies.pdf page: 303

____________________________________________

attack methodology
generally, there are five steps to a successful attack:
1. identify the specific target that will be attacked and collect information
about that target.
2. analyze the information and identify a vulnerability in the target that will
accomplish the attack objectives.
3. gain the appropriate level of access to the target.
4. perform the attack on the target.
5. complete the attack, which may include erasing the evidence of the attack,
and avoid retaliation.
you can think of this as figuring out what to attack, figuring out how
to attack it, getting in, performing the attack, and getting out. the first
two steps are research. you can do them in the safety of your own lab;
you can often do them on simulations of the actual target. if you’re an
academic, you can stop after the second step and publish. the last three
steps carry the risk; it’s where the actual or virtual breaking and entering
happens. it’s where people either get away with the attack or get caught.
remember star wars? in order to blow up the death star, the rebels
first had to get the information that princess leia stuffed into r2-d2.
that was the whole reason luke had to get the droids off tatooine in the
first place. rescuing the princess was just a macguffin. that was step one.
step two was off-camera. we see the result when the rebel engineer
announces that he’s studied the information from the droid and found a
weakness in the station’s defense systems: the janitorial system designers
never bothered having their system designs audited by security profes-
sionals, and now the death star’s multi-billion-credit defense systems can
be breached through a ventilation shaft.
step three was the special-effects laden space dogfight between the
rebel x-wing fighters (you have to admire rebels with their own defense
contractors) and the station’s tie fighters. the job of the x-wing fight-
ers was to distract everyone so that the y-wing pilots could fly along the
trench and shoot down the ventilation shaft. access to the target was the
whole point.
it took young master luke to complete step four, after han solo got
darth vader off his tail, and alec guiness’s disembodied voice cajoled
vulnerabilities and the vulnerability landscape 275
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 275


 
 

____________________________________________

SecretsAndLies.pdf page: 304

____________________________________________

him to turn off his targeting computer (probably still in beta) and use the
force.
blowing the death star to bits (step five) effectively eliminated any
chance of retaliation, at least until the sequel. after that, getting away was
easy. our heroes get medals from a rebel alliance whose cash balance was
high enough to afford new uniforms, and the universe is saved for a new
series of themed pez dispensers. roll credits.
it’s not much different to attack a company’s computers via the inter-
net. step 1 is to identify the target and gather information. this is surpris-
ingly easy. the target’s web site will contain all sorts of information, as do
various internet databases like the one run by network solutions. war
dialers can find dial-up connections. there are many techniques an
attacker can use to figure out what is running on the target network: ping
scans, port scans, service listings, and others. network sniffers can find
more information, as can vulnerability assessment tools. a lot of this is the
internet equivalent of door rattling, although computers often tell perfect
strangers a lot about what kind of hardware they are, what kind of soft-
ware they are running, and what kind of services they allow. all this is
information an attacker can use.
step 2 is to find a vulnerability. here, the attacker goes through all the
information he collected looking for a place to attack. maybe one of the
computers is running a particular version of sendmail, or the solaris oper-
ating system, or windows nt, with a known bug. maybe he can exploit
ftp, or rlogin, or something else. maybe the target has left a maintenance
port on some piece of equipment unsecured. maybe the attacker could
exploit the target’s pbx. the more the attacker knows about different
vulnerabilities of different systems, the better he can plan his attack.
step 3 is to gain some kind of access to the computer. on the inter-
net this is trivial, since every computer is on the network and therefore
accessible. (of course, some computers are behind a firewall and inacces-
sible, but the firewall will be accessible.)
step 4 is to perform the attack. this can be either complicated or easy.
if the attacker is good, this step is surprisingly easy.
note that some attacks involve multiple iterations of this process. an
attacker might perform steps 1 through 4 many times: breaking into the
web server, gaining root access on the web server, using that access to
break into another server inside the corporate firewall, gaining root access
276 c h a p t e r  e i g h t e e n
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 276


 
 

____________________________________________

SecretsAndLies.pdf page: 305

____________________________________________

on that server, and so forth. each step involves its own information gath-
ering, target and method identification, access, and execution.
step 5 is to complete the attack. if he is looking for a particular file,
get it and leave. he can erase audit logs and otherwise obscure his trail. he
can also leave modified system files so that he can more easily gain access
next time. and if he is looking to do a particular piece of damage, do it
and leave. but get out quickly. hanging around is the sign of an amateur.
in his “faq and guide to cracking,” mixter describes the same
steps. here’s what he says are the first things to do after you get root.
(getting root privileges on the target computer constitutes a completion
of step 4.):
“1. discretely [sic] remove traces of the root compromise
2. gather some general info about the system
3. make sure you can get back in
4. disable or patch the vulnerable daemon(s)”
specifically, he suggests turning off logging and deleting log records
of the compromise, and figuring out how often the system is maintained
and administered, and how often the log files are analyzed.
hacker tools can automate a lot of the process. they’re not nearly as
good as a virtuoso hacker, but they can turn an inept teenager into a for-
midable adversary.
another example: an attack against a smart card payment system. step
1 is to gather whatever information is available on the payment system:
design specifications, public interface documents, public information on
the various algorithms and protocols used, and so forth. there is probably
a lot of information out there, if you know where to look.
step 2 is to study the documentation, looking for a weakness. part 2
of this book talks about all sorts of weaknesses that can affect a system like
this. maybe there’s a weakness in the cryptographic algorithms and pro-
tocols. maybe there’s a weakness in the smart card, and it’s not as tamper-
resistant as the designers thought it was. maybe there’s a weakness in how
the card is used that you can exploit. whatever it is, you need to find a
weakness in order to attack this system.
step 3 is to gain whatever access is needed for the attack. you might
have to become a registered user of this smart card payment system (per-
vulnerabilities and the vulnerability landscape 277
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 277


 
 

____________________________________________

SecretsAndLies.pdf page: 307

____________________________________________

basically, countermeasures can be implemented to thwart any of the
five steps of a successful attack.
most of part 2 discusses technical countermeasures applicable to com-
puters and computer networks. i tried to talk about these in context: what
they do, what they don’t do, how they work in relation to each other,
and so forth. no technology is a security panacea; the trick is using each
of them effectively.
the security of a system may be no better than its weakest link, but
that generally refers to the individual technologies. in a smart system, these
technologies can be layered in depth, and the overall security is the sum
of the links. cryptography can be defeated by brute-forcing the key,
cryptanalyzing the algorithm, or (the weak link) social-engineering the
password from an oblivious secretary. but protecting the computer
behind a locked door, or a well-configured firewall, provides defense in
depth.
remember the opening scenes of raiders of the lost ark? indiana
jones had to get past the spiders, the wall-of-spikes trap, the pit, the poi-
son darts released by stepping on the wrong floor stones, and the self-
destruct mechanism tied to moving the statue. this is defense in depth.
he bypassed the wall-of-spikes trap by avoiding the triggering mecha-
nism, but he might have dodged the wall, jammed the mechanism, or
done half a dozen other things. the security of the trap depends on the
easiest way to avoid it.
but just as attacking a system is more complicated than simply finding
a vulnerability, defending a system is more complicated than dropping in
a countermeasure. there are three parts to an effective set of counter-
measures:
• protection
• detection
• reaction
in a military office, classified documents are stored in a safe. the safe
provides protection against attack, but so does the system of alarms and
guards. assume the attacker is an outsider: someone who does not work
in the office. if he is going to steal the documents inside the safe, he is not
only going to have to break into the safe, he is also going to have to defeat
the system of alarms and guards. the safe—both the lock and the walls—
vulnerabilities and the vulnerability landscape 279
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 279


 
 

____________________________________________

SecretsAndLies.pdf page: 308

____________________________________________

are protective countermeasures, the alarms are detection countermea-
sures, and the guards are reactive countermeasures.
if guards patrol the offices every 15 minutes, then the safe only has to
withstand attack for a maximum of 15 minutes. if the safe is in an obscure
office that is only staffed during the day, then the safe has to withstand 16
hours of attack: from 5 p.m. until 9 a.m. the next day (much longer if the
office is closed during holiday weekends). if the safe has an alarm on it,
and the guards come running as soon as the safe is jostled, then the safe
only has to survive attack for as long as it takes for the guards to respond.
what this all means is that the strength of the safe is based on the
detection and reaction mechanisms in place. and safes are sold this way.
one safe might be rated as tl-15; this means that it can resist a profes-
sional safecracker, with tools, for at least 15 minutes. another might be
rated trtl-60, meaning that it can resist the same safecracker, with tools
and an oxyacetylene torch, for 60 minutes. these time ratings are for a
sustained attack, meaning that the clock was running only when the safe
was being attacked: rest and planning time is not counted. and the tests
are conducted by professionals with access to the safe’s engineering draw-
ings: no security by obscurity allowed. (sounds a lot like cryptographic
attacks, doesn’t it?)
protection, detection, and reaction countermeasures work in tandem.
strong protection mechanisms mean that you don’t need good detection
and reaction mechanisms. weak protection mechanisms—or even no
protection mechanisms—mean that you need better protection and
detection mechanisms.
the safe ratings show this clearly. what a safe buys you is time: 15
minutes, 30 minutes, 24 hours. this time is for the various alarms to
sound (detection) and for the guards to come arrest the safecrackers
(response). without detection and response, it actually doesn’t matter
whether your safe is rated tl-30 or trtl-60.
most computer-security countermeasures are prophylactic: cryptog-
raphy, firewalls, passwords. some are detection mechanisms: intrusion
detection systems. even rarer are reaction mechanisms—a login system
that locks users out after three failed login attempts is an example—even
though detection mechanisms are useless without them. think about an
intrusion detection system that has just detected an attack. it alerts a sys-
tem administrator, maybe by e-mailing his pager. if that administrator
won’t respond for hours—maybe he’s at lunch—then it really doesn’t
280 c h a p t e r  e i g h t e e n
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 280


 
 

____________________________________________

SecretsAndLies.pdf page: 310

____________________________________________

fect security barriers are detection and reaction: detection to notice when
security has been breached, and reaction to do something about it.
the vulnerability landscape
real systems have many different vulnerabilities, and there are many dif-
ferent ways to launch an attack. a terrorist wanting to blow up an aircraft
could smuggle a bomb onboard, shoot it down with a missile, or sneak
onboard, hijack the controls, and fly it into a mountain. a computer
hacker intent on penetrating a corporate network could attack the fire-
wall, attack the web server, exploit a dial-up modem, and so forth.
real systems can also have many different countermeasures. airlines
have metal detectors, chemical analyzers, and x-ray machines to detect
bombs, and bag-matching systems to ensure that someone doesn’t stay on
the ground while his bag flies alone. (this system of countermeasures
assumes that fewer terrorists are willing to blow themselves up on an air-
craft than are willing to stay on the ground while an aircraft blows up.)
military aircraft also have assorted antimissile defenses. corporate net-
works can have firewalls, intrusion detection systems, procedures for rou-
tinely updating passwords, and encrypted file servers.
this can get tortuous pretty fast.
i use the term vulnerability landscape to limn this imaginary, compli-
cated world of attacks and countermeasures. the metaphor is supposed to
evoke a vast expanse of possible attacks—pulling a gun on a bank teller,
blackmailing a programmer to put a trojan horse in a piece of software,
drilling through the bank wall, calling up an unsuspecting clerk and ask-
ing for his password—and countermeasures: bulletproof glass protecting
the tellers, running background checks on all employees, cameras watch-
ing the outside of the building, biometric verification. different parts of
the landscape represent different types of attacks. computer attacks are, of
course, only a small area of the landscape.
each system has its own vulnerability landscape, although different
systems will have common landscape features. (every computerized sys-
tem has to deal with the threat of a power shutdown, for example. and
almost every system uses threat of arrest as a countermeasure.) a filled
vulnerability landscape is rough terrain, made up of peaks and valleys of
varying heights and depths. the higher the peak, the better the counter-
282 c h a p t e r  e i g h t e e n
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 282


 
 

____________________________________________

SecretsAndLies.pdf page: 314

____________________________________________

the life cycle of a system
an industrial spy might choose to bug the telephones in his competitor’s
offices. he then must choose when and where to conduct this attack. the
office equipment is vulnerable during its entire life cycle: on the drawing
board, in the manufacturing plant, on the loading dock, in the competi-
tor’s offices, or even after disposal. depending on access afforded to him,
the adversary may choose to alter or swap the equipment during produc-
tion, shipment, installation, normal operations, or maintenance. at some
point during the equipment’s life cycle, soviet spies bugged typewriters in
the u.s. embassy in moscow. did they install the bugs at the factory in
the u.s., while the typewriters were being shipped to the embassy, or
after they were sitting on desks inside the embassy? we don’t know, but
each option represents a possible point of attack. and depending on how
good the audit systems were, they may have been able to figure it out.
similarly, a criminal who wants to steal money from a slot machine
has the same array of choices: he can introduce a flaw into the design,
modify it during installation, or break into it when it is on the casino floor.
each of these attacks has different characteristics—difficulty, success prob-
ability, profitability—but they are all possible.
the work environment of the virtual world is software running on
network computers. attackers can attack this software anywhere during
its life cycle. a malicious software developer could intentionally leave a
back door in the latest release of the operating system. an adversary could
put a trojan horse in an already popular net browser and distribute it
for free over the internet. he could write a virus that attacks accounting
software and delivers it in an executable attachment to an 
e-mail message. he could analyze the software and exploit an accidental
vulnerability. the possibilities are staggering.
rationally applying countermeasures
the vulnerability landscape is a vast expanse of potential attacks, and it
makes the most sense to apply countermeasures evenly across the land-
scape. the idea is to protect against those threats that pose the greatest
risk, instead of protecting against the most manifest threats while ignoring
all the others.
286 c h a p t e r  e i g h t e e n
453803_ch18.qxd:453803_ch18.qxd  4/12/13  2:13 pm  page 286


 
 

____________________________________________

SecretsAndLies.pdf page: 316

____________________________________________

288
19
threat modeling and
risk assessment
threat modeling is the first step in any security solution. it’s a way
to start making sense of the vulnerability landscape. what are the
real threats against the system? if you don’t know that, how do
you know what kind of countermeasures to employ?
threat modeling is hard to do, and a skill that only comes with expe-
rience. it involves thinking about a system and imagining the vast vulner-
ability landscape. just how can you attack this system? i find that true
hackers are masterful at this kind of thing, which is probably why they’re
drawn to computers in the first place. hackers enjoy thinking about sys-
tems and their limitations: how they fail, when they fail, what happens
when they fail. they delight in making systems do things they weren’t
intended to. it’s the same whether the hacker is modifying the engine in
his car to work how he wants it to and not how the manufacturer wants
it to, or whether he is poking at an internet firewall to see if he can “own”
the computer it is running on. 
i find that the best security analysts are people who go through life
finding the limitations of systems; they can’t help it. they can’t walk into
a polling place without thinking about the security measures and figuring
out ways that they can vote twice. they can’t use a telephone calling card
without thinking about the possible antifraud mechanisms and how to get
around them. these people don’t necessarily act on these thoughts—just
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 288


 
 

____________________________________________

SecretsAndLies.pdf page: 317

____________________________________________

because they found the blind spot in the store’s video surveillance system
doesn’t mean they start shoplifting—but they can’t help looking.
threat modeling is a lot like this, and the only way to learn it is to do
it. so let’s start by stealing some pancakes.
our goal is to eat, without paying, at the local restaurant. and we’ve
got a lot of options. we can eat and run. we can pay with a fake credit
card, a fake check, or counterfeit cash. we can persuade another patron
to leave the restaurant without eating and eat his food. we can imperson-
ate (or actually become) a cook, a waiter, a manager, or the restaurant
owner (who might be someone that few workers have ever met). we
could snatch a plate off someone’s table before he eats it, or from under
the heat lamps before the waiters can get to it. we can wait at the dump-
ster for the busboy to throw away the leftovers. we can pull the fire alarm
and sneak in after everyone evacuates. we can even try to persuade the
manager that we’re some kind of celebrity who deserves a free breakfast,
or maybe we can find a gullible patron and talk her into paying for our
food. we could mug someone, nowhere near the restaurant, and buy the
pancakes. we can forge a coupon for free pancakes. and there’s always
the time-honored tradition of pulling a gun and shouting, “give me all
your pancakes.”
there are probably even more possibilities, but you get the idea.
looking at this list, most of the attacks have nothing to do with the point
where money changes hands. this is interesting, because it means that
securing the payment system does not prevent illicit pancake stealing.
it’s similar in the digital world. if this were a web-based digital pan-
cake store, most of the attacks would have nothing to do with the
electronic payment scheme. there are many other areas of vulnerability.
(remember the beautiful web page hack against shopping cart software
from chapter 10, where an attacker could change the price of an item to
an arbitrary amount. this brings up another possible attack: change the
menu so the pancakes cost $0.00.) the most fruitful attacks are rarely the
physical ones.
fair elections
let’s move on to bigger and better things. let’s rig an election. it’s a local
election—mayor of a town. cheating in elections is almost as old as elec-
tions themselves. how hard could it be?
threat modeling and risk assessment 289
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 289


 
 

____________________________________________

SecretsAndLies.pdf page: 318

____________________________________________

assume a dozen different voting precincts, each with its own polling
place. each polling place has three election judges who monitor the
process. voters get paper ballots from these judges, blacken a circle corre-
sponding to the candidate of their choice, and then drop the ballot into a
large box. at the end of the day, all the ballots are fed into an automatic
vote-counting machine. the judges at each of the 12 polling places phone
their results in to a central office. then the results are summed together,
and the winner gets to declare victory over the sound of a noisy band
while dodging confetti.
the system has many attack points. we can attack the voters, the
election judges, the ballot boxes, the vote-counting machines, the phone
calls, or the central office. let’s examine each in turn.
bribing voters is a time-honored way of rigging an election. this isn’t
just something that happened in the dim history of the developed world,
or in third world countries. in dodge county, georgia (population
17,000), 21 people were indicted for a variety of illegal voting practices,
vote buying included; the election was in 1996. in most jurisdictions
(including georgia) it’s illegal to pay cash for votes, so politicians are usu-
ally forced to resort to other bribes: tax breaks, public works projects,
friendly legislation, and white house sleepovers. we can do this, but it’s
expensive.
and we can’t rely on it. the whole point of having private voting
booths is so that people can’t reliably buy and sell votes. we can pay vot-
ers $100 each to vote for a particular candidate, but when they go into the
polling place, they can mark their ballots however they please. (tax
breaks work better in this regard, especially for incumbents; the voters
think that by voting a certain way they can get more of them.) there’s an
old chicago story about a politician who bought votes. he had his
henchmen smear black gunk on the mechanical voting pulls associated
with a vote for him, and was then able to confirm if the bribed voter
delivered the goods.
this avenue of fraud is returning due to the prevalence of mail-in bal-
lots. somewhere between a third and a half of all ballots cast in silicon
valley elections are mail-in. in oregon today every election (except pres-
idential) is mail-in only. arizona experimented with an internet voting
scheme for the 2000 democratic primary. the risk is there—someone
could walk into a poor section of town and buy a pile of blank ballots for
$10 each (arizona used pins, equally fungible)—but the locals feel that it
is worth it.
290 c h a p t e r  n i n e t e e n
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 290


 
 

____________________________________________

SecretsAndLies.pdf page: 322

____________________________________________

we could modify the phones so that they don’t work properly. there
are lots of options: we can force the encryption algorithm to be weak, we
can mess with the key generation system, we can make the phones radi-
ate the unencrypted phone call, or we can add a subliminal channel to
make the phones leak the keys onto the voice circuit (this is known as
“clipper” when it is done openly). all of these attacks could be put into
place during product design and development, while the phones are being
shipped to the organization, or during maintenance. they could be done
by sneaking into the manufacturing facility at night, bribing someone
who works there, or simply designing the surreptitious feature in from the
start.
this might seem far-fetched, but if we have the resources of a
national intelligence organization, they’re perfectly reasonable methods of
attack. crypto ag, a swiss company, sells encryption hardware to a lot of
third world governments. in 1994, one of their senior executives was
arrested by the iranian government for selling bad cryptographic hard-
ware. when he was released from jail a few years later, he went public
with the news that his company had been modifying their equipment for
years at the request of the u.s. intelligence community. in the 1950s,
xerox modified photocopiers sold to the russians so that they also had a
little camera inside; copier repairmen would periodically remove and
replace the film.
the soviets weren’t any less wily; they modified all sorts of office
equipment, including ibm selectric typewriters, in the american
embassy in moscow to broadcast data. british encryption companies are
rumored to add exploitable features into products they sell to foreign gov-
ernments. even if they didn’t hear the rumor, you’d think that the argen-
tine government would think twice before using british-supplied
encryption devices during the falklands war.
there are a lot of things we can do that don’t directly involve the
secure telephone: installing bugs inside the secure phones (or the rooms
where the phones are), bribing the people making and receiving the calls,
and so forth. but the organization can’t reasonably expect the phones to
be able to deal with that.
one of the best attacks is to simply force the phones not to work.
this is easier if the attacker owns the phone system: for example, the
phones are being used by a human rights organization in a questionable
third world country, or by a multinational corporation calling a field
294 c h a p t e r  n i n e t e e n
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 294


 
 

____________________________________________

SecretsAndLies.pdf page: 325

____________________________________________

vacy, anonymity, and so forth. we’re not going to get detailed. let’s just
look at possible attacks against the most general formulation of the system.
there are three different parties involved in the plasticash system: the
customer, the merchant, and the bank. and there are three protocols:
bank/customer. the customer loads plasticash onto his card.
customer/merchant. the customer transfers plasticash from his card to
the merchant’s card.
bank/merchant. the merchant deposits plasticash from his card into his
bank account.
part 1 of this book talks about the possible attacks: monetary theft,
framing, privacy violations, vandalism and terrorism, or publicity. plasti-
cash might also have to worry about ancillary crimes: an attacker may
wish to use the system to carry out some other crime, such as money
laundering. ancillary crimes are hard to define clearly, since they can
change at each border crossing, and even at each election. it’s also not
clear to what extent different countries’ laws and customs may be in con-
flict. for example, in an international arena, u.s. financial reporting
requirements may run afoul of swiss banking secrecy laws.
when we talk about attacks by the bank, we are not necessarily
postulating a malefic banking empire. these attacks can be mounted by
rogue employees of venerable banks. in general, we are more concerned
with attacks by customers and merchants (rogue employees of merchants),
on the theory that banks can afford better security mechanisms and mea-
sures, and have greater potential losses to reputation if they attack their
own systems. still, it’s prudent to be careful.
the first type of attack is theft. there are several ways to mount an
attack to steal money from plasticash. these attacks can be mounted by
customers or merchants:
modify the card so that is has more value than it should. this can be done in
several ways; the most obvious is to find the data register inside the card that
records the value of the card and change it.
alter records to reflect either a larger or smaller payment amount.
learn to create or emulate new cards. this attack is creating a fake plasticash
card that can act like a real card. the fake card doesn’t have to look like a
real card; an attacker can use it only for purchases over the internet, or he
threat modeling and risk assessment 297
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 297


 
 

____________________________________________

SecretsAndLies.pdf page: 326

____________________________________________

can transfer money from his fake card to a real card and then spend with the
real card.
learn how to clone cards. the attacker would need to steal a legitimate card,
make a clone, and then return it. (this attack succeeded against canadian
bank cards; several arrests were made in 1999. a rogue merchant could
clone a card in seconds, while a customer used the card to make a legitimate
purchase.)
these are customer attacks:
repudiate a set of valid card transactions. this is the old “buy an expensive
something with your card, and then report the card stolen and deny the
transaction.” there’s a new variant—using stupidity as an excuse—for
example: “visa forced me to lose all my money gambling online; it’s not my
fault.” this attack should be dealt with at an administrative and legal level.
whether you’re dealing with checks, credit cards, traveler’s checks, or
whatever, some people will decide they wish they hadn’t spent all that
money and try to avoid paying the charges.
report another user’s card stolen and arrange to intercept his replacement
card. again, this attack is bigger than the plasticash system.
and attacks that can only be mounted by merchants:
accept a transaction and refuse to deliver the goods. this is outside the scope
of what the card can resist, but administrative and legal procedures need to
exist to handle it.
get access to some stolen customer cards, and alter data. or generate a bunch
of apparently valid checks to deposit. this is an obvious attack. the attacker
will probably try to deposit the money and then quickly withdraw it all.
replay valid transactions. a merchant could somehow charge a customer
twice for the same transaction.
finally, these attacks can be mounted by banks:
refuse to load value into a plasticash card that a customer has paid for. this
can be resisted only by administrative procedures and logging. the cus-
tomer will have evidence enough to prove to a neutral third party what has
happened if the protocol is designed competently.
pocket the cash and never credit the customer’s account, when someone tries
to deposit money from his plasticash card into his account. again, this can
be defended against only through administrative procedures.
298 c h a p t e r  n i n e t e e n
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 298


 
 

____________________________________________

SecretsAndLies.pdf page: 327

____________________________________________

all of these attacks can be mounted by pairs (or the trio) as well. i
can’t think of any different sort of attack that a merchant and customer
can mount, but depending on the security characteristics of the system,
the pair could be successful where either one acting alone might not be.
additionally, think about attacks by people pretending to service the ter-
minal. or repair the phone lines.
the second type of attack is framing. first, customer or merchant
attacks:
a customer can claim that a merchant has an invalid plasticash card (or termi-
nal). a merchant can claim the same thing about a customer card. this has
to be resolved by administrative means.
then, bank attacks:
forge customer cards (or merchant cards, for that matter) and frame the cus-
tomer. presumably, if the bank can issue plasticash cards, it could also forge
them. wouldn’t a customer hate to see his itemized plasticash record
include a prostitute visit?
the third type of attack is a privacy violation. privacy violations hap-
pen whenever some user’s personal information is given without that per-
son’s consent to some third party. depending on the jurisdiction, a
privacy violation may be legal. since the developers of plasticash want to
their product to propagate worldwide, it makes sense to list the attacks
and then later ignore them if they are legal (and harmless).
unless the system is specifically designed to prevent this, the bank is
in a position to collect unlimited information about customer spending.
(“i know what you bought last summer.”) it is possible to avoid some of
this (but only some) by having users buy precharged stored-value cards in
fixed denominations, like some prepaid telephone cards.
the merchant can’t directly get the customer’s name and such data,
but it can collect and share information about this card’s id with other
merchants, and try to link this back to the user’s identity.
and we also have to worry about eavesdroppers: people not involved
in the protocol at all listening in on transactions and collecting informa-
tion.
threat modeling and risk assessment 299
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 299


 
 

____________________________________________

SecretsAndLies.pdf page: 333

____________________________________________

were real, so they had no incentive to police for forgeries. (the designers
also thought that a $100 limit per card would cap their losses.) the attack
was subtle—it involved reconditioning real cards, a bunch of pachinko
machines that “disappeared” after the kobe earthquake, and multiple
pachinko parlors—but the total amount of fraud was about $600 million.
rumor is that the money was funneled into north korea.
manufacturers of slot machines have long anticipated attacks by players manip-
ulating physical devices. cheating attempts have included holes drilled into
the machine so as to manipulate the reel mechanisms, and devices used to
interfere with the sensors that track the number of coins that have been paid
out. years ago, one video poker machine manufacturer was surprised by a
completely unanticipated attack: static electricity. some payers discovered,
probably by accident, that after building up a large static charge from the
plush casino carpets they could shock the machine, causing it to empty its
hopper of all stored coins.
in late 1999, the encryption used to encrypt dvds was broken. even though
the discs were encrypted, the decryption key had to be in the players.
there’s no way around it. this worked fine as long as the players were tam-
per-resistant hardware, but as soon as someone built a software player, the
decryption keys were in software. someone simply reverse engineered the
software and recovered the key, allowing them to freely copy and distribute
dvd data over the internet. 
in 1980, the host of the pennsylvania lottery drawing, an official with the
pennsylvania lottery, and some assorted stagehands rigged the ping-pong
balls used in the drawing and won a $1.2 million jackpot. no one expected
that complex a collusion. these days, kpmg audits the drawings much
more carefully. (a similar flaw—this time a random error—occurred in the
arizona lottery. in 1998, someone noticed that no winning number in its
pick 3 game had ever included a single numeral 9. it turned out that the
pseudorandom number generator algorithm had an elementary program-
ming error. ping-pong balls are safer than computers, it seems.) 
most european countries enforce trucking regulations with something called a
tachograph: a device attached to the truck’s speedometer that logs the vehi-
cle’s speed, distance, and other information. these devices would record
this data on a waxed paper tape that the driver had to sign and date, and
keep with him for a period of time. these were hard to forge, and attacks
tended to exploit procedural weaknesses instead of technological weak-
nesses. recently the eu funded the tachosmart project, designed to build
an all-digital replacement. any such system will open itself to all the attacks
described in this book (even worse, it is likely to be based on smart cards),
and will be much less secure.
threat modeling and risk assessment 305
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 305


 
 

____________________________________________

SecretsAndLies.pdf page: 338

____________________________________________

as an example, look at the online gaming community. many games
allow for multiplayer interaction over the internet, and some even have
tournaments for cash prizes. hackers have written computer bots that assist
play for some of these games, particularly quake and nettrek. the idea
is that the bots can react much quicker than a human, so that the player
becomes much more effective when using these bots. an arms race has
ensued, as game designers try to disable these bots and force fairer play,
and the hackers make the bots cleverer and harder to disable.
these games are trying to rely on trusted client software, and the
hacker community has managed to break every trick the game designers
have thrown at them. i am continuously amazed by the efforts hackers
will go through to break the security. the lessons are twofold: not only is
there no reasonable way to trust a client-side program in real usage, but
there’s no possible way to ever achieve that level of protection. 
against all of these systems—disappearing e-mail, rights management
for music and videos, fair game playing—there are two types of attackers:
the average user and the skilled attacker. against the average user, any-
thing works. this is uncle steve, who just wants a single copy of norton
utilities, the lion king, or robin hitchcock’s latest cd, and doesn’t
want to pay for it. there’s no analogue for him in the physical world;
uncle steve couldn’t make a single copy of a chanel handbag, even if he
wanted one. on the one hand, he’s more elusive; on the other hand, he’s
much less of a financial threat. uncle steve isn’t an organized criminal;
he’s not going to have a criminal network and he’s not going to leave
much in the way of a trail. he might not even have bought the software,
video, or cd if he couldn’t get a free pirated copy. against uncle steve,
almost any countermeasure works; there’s no need for complex security
software.
against the skilled user, no countermeasure works. in chapter 16, 
i talked about the heroic lengths some hackers go to to disable copy
protection schemes. earlier in this section i talked about the specially
designed bots to subvert the user interface in computer games. because
breaking the countermeasure can have so much value, building a system
that is secure against these attackers is futile. and even worse, most sys-
tems need to be secure against the smartest attacker. if one person hacks
quake (or intertrust or disappearing inc.), he can write a point-and-click
software tool that anyone can use. suddenly a security system that is
secure against almost everyone can now be compromised by everyone.
310 c h a p t e r  t w e n t y
453803_ch20.qxd:453803_ch20.qxd  4/12/13  2:33 pm  page 310


 
 

____________________________________________

SecretsAndLies.pdf page: 339

____________________________________________

the only possible solution is to put the decryption mechanism in
secure hardware, and then hope that this slows the professionals down by
a few years. but as soon as someone wants a software player, it will be bro-
ken within weeks. this is what the dvd industry learned in 1999. this
is what glassbook learned in 2000, when unprotected copies of stephen
king’s “riding the bullet” materialized two days after the ebook version
(supposedly secured against this kind of thing) was released.
any rational security policy will recognize that the professional pirates
cannot be defended against with technology. professional digital pirates
are no different than people who counterfeit chanel handbags, and soci-
ety has ways of catching these people (noncomputer detection and reac-
tion mechanisms). they may or may not be effective ways, but that has
nothing to do with the digital nature of the forgery. the same security
policy would recognize that uncle steve is an amateur, and that almost any
countermeasure—as long as it could not be broken completely or triv-
ially—will work in this case.
note that this analysis implies that content providers would be smart
to find alternate ways to make money. selling physical copies of a book
doesn’t work as well in the digital world. better is to sell real-time
updates, subscriptions, and additional reasons to buy a paper copy. i like
buying cds instead of copying them because i get the liner notes. i like
buying a physical book instead of printing a digital copy because i want
the portability and the binding. i’m willing to pay for stock information
because i want its timeliness.
you can see alternate models in the public financing of good works:
public television, public art, and street performers. the performance is
free, but individual contributions make it happen. instead of charging
each of you $29.99 for this book, maybe i should have put up a web page
asking for contributions. i would write the book and put it in the public
domain, but only after i received $30,000 in contributions. (this idea was
used to fund some anti-bush campaign ads in 2000. people would pledge
contributions on their credit card, but would only be charged if the target
total was reached. notice that the credit card company acted as the trusted
third party in this transaction.)
other industries have different solutions. the smarter game compa-
nies dealt with this by specifically allowing bots in some tournaments, and
having final rounds of other tournaments live at trade shows, where the
computer is trusted by the game company. the smarter self-distrusting e-
security policies and countermeasures 311
453803_ch20.qxd:453803_ch20.qxd  4/12/13  2:33 pm  page 311


 
 

____________________________________________

SecretsAndLies.pdf page: 341

____________________________________________

to avoid the problem of maintenance personnel injecting trojan horses
into the system. and so on.
the physical security is also straightforward. the money should be
kept in a safe. there should be audit records of anyone opening the safe
(perhaps each person can have his own combination, or a unique token).
any long-term cryptographic keys should be erased at the first sign of
tampering. 
it’s interesting to note that atm owners only recently got the phys-
ical countermeasures correct. until a few years ago, atms were built into
bank walls and other secure locations. elsewhere in this book i mentioned
attackers who stole entire machines; that was the concern. then, some-
one reached the conclusion that these attacks were rare, and that there was
a lot more money to be made by putting atms in every bus terminal,
bar, shopping mall, and gas station. these are small, freestanding atms:
much less secure, but that doesn’t matter. these atms are in public
places, so there’s some basic detection and reaction. there’s less cash in
them, so the risk is less. and the fees are high, so they’re profitable. if the
occasional atm disappears, it’s still worth it.
even more recently there was another change in the security policy.
someone finally realized that an atm has two parts: a physical vault with
money in it, and a networked computer that tells the vault how much to
dispense and when. there’s no reason for these two parts to be in the
same physical housing. a retail store already has a secure money vault: the
cash register. now some atms have no money in them; they’re just a
computer. the computer goes through the authentication process and
prints a slip of paper. the user takes the slip of paper to the cash register
and gets his money. these are only good for small amounts, but they
work. this is a beautiful example of thinking about security correctly . . .
until someone successfully forges the paper slips.
computerized lottery terminals
computerized lottery terminals are used in most keno-style lotteries.
basically, lottery vendors get a secure computer/printer that prints out
and validates lottery picks. this “validation” consists of a printed ticket
with the chosen numbers plus some authenticating information. once or
twice a week there is a public drawing. there are small winners and large
jackpot winners.
security policies and countermeasures 313
453803_ch20.qxd:453803_ch20.qxd  4/12/13  2:33 pm  page 313


 
 

____________________________________________

SecretsAndLies.pdf page: 342

____________________________________________

the threats are obvious. attackers are most likely the lottery vendors
themselves, possibly in cahoots with people working inside the lottery
system. they can attack the system in one of two ways: “buying” tickets
after the results are known, or altering already-purchased tickets after the
results are known. more subtle, but also damaging, is operating a phony
terminal that collects money but doesn’t pay out any prizes (actually, they
would be smarter to pay out small prizes and disappear if any of their
phony tickets won a large prize).
these threats imply a straightforward security policy. the lottery ter-
minals should be online, and register all picks with a central server. this
server keeps good audit logs, with timestamps, and sends the terminals
audit information that is printed on the ticket. this server needs to be
secured prior to the drawing. and there needs to be some way of identi-
fying bogus vendors: the obvious one is to allow low-value tickets to be
redeemed at any vendor, not just the one the ticket was purchased at.
regular audits also help.
there are still a lot of details to work out, but you get the idea.
smart cards vs. memory cards
as a final example, let’s look at two different protection mechanisms:
smart cards and magnetic stripe memory cards. in chapter 14, i talked
about tamper resistance, secure perimeters, and attacks against smart cards.
in chapter 19, i did a basic threat model of a hypothetical digital cash sys-
tem based on smart cards. now let’s apply all that knowledge and ask the
following question: is it more secure to have a smart card (a card with a
microprocessor on it) than a memory card (either a card with just a mem-
ory chip on it, or a magnetic stripe card) for a given application.
to someone who can reverse engineer the smart card, there’s no dif-
ference. he can recover the data from both types of cards, and both types
of cards can encrypt their secrets to protect against this. to someone who
cannot reverse engineer the smart card, there is a big difference. that
someone can read the magnetic stripe card, but he cannot read the mem-
ory on the smart card. on the other hand, if the information is encrypted
anyway, what does it matter if he can read the magnetic stripe? maybe
there’s less of a difference than we thought.
314 c h a p t e r  t w e n t y
453803_ch20.qxd:453803_ch20.qxd  4/12/13  2:33 pm  page 314


 
 

____________________________________________

SecretsAndLies.pdf page: 343

____________________________________________

let’s look at the process of using the two different cards.
magnetic stripe cards. the user puts his card into a reader, and then types
a pin or password or code into the reader. the reader reads the data off the
magnetic stripe and uses the pin to decrypt the data. this data is then used
by the reader to do whatever the system is supposed to do: log in to the sys-
tem, sign an electronic check, pay for parking, or whatever.
smart cards. the user puts his card into a different reader, and types the
same pin into this reader. the reader sends the pin into the smart card,
which decrypts the data. the data is then used by the card, not the reader,
to do whatever the system is supposed to do. the reader just acts as the
input/output device for the system.
what’s the difference? in both cases, a malicious reader can subvert
the system. the reader is the only contact the card has with the outside
world. once the magnetic stripe card gives up its secrets, the reader can
do whatever it wants. once the smart card has been fed the correct pin
by the reader, the reader can make the smart card believe any reality that
it wants.
the primary difference between the two cards is that the smart card
can exert some control, because it is secure within itself. if someone steals
a magnetic stripe card, for example, he can do a brute-force search against
the secrets stored on the card. he can do this brute-force search offline,
on a computer, without the user even knowing. (a canny attacker can
steal the card, read the data off the magnetic stripe, and then slip it back
into the victim’s wallet.) smart cards can’t be attacked this way. smart
cards can be programmed to shut down after ten (or so) invalid password
attempts in a row. so if someone steals a smart card, he won’t be able to
brute-force the password. he’s only got ten guesses. (again, this assumes
that he can’t just reverse engineer the card. if he can, he can do an offline
attack just like a magnetic stripe card.)
another major difference is that the smart card doesn’t have to give
up its secrets. if, for example, the cards are used to sign documents, the
smart card may be more secure than a magnetic stripe card. the magnetic
stripe card has to rely on the reader to do the actual signing; it gives up its
secret to the reader and hopes for the best. a malicious reader can steal the
signing key. the smart card does the signing itself. a malicious reader can
feed the card bogus things to sign, but the reader cannot learn the signing
key.
security policies and countermeasures 315
453803_ch20.qxd:453803_ch20.qxd  4/12/13  2:33 pm  page 315


 
 

____________________________________________

SecretsAndLies.pdf page: 346

____________________________________________

318
21
attack trees
danaë was the daughter of acrisius. an oracle warned acrisius
that danaë’s son would someday kill him, so acrisius shut
danaë in a bronze room, away from anything even remotely
masculine. zeus had the hots for danaë, so he penetrated the bronze
room through the roof, in the form of a shower of gold that poured down
into her lap. danaë gave birth to perseus, and you can probably guess the
end of the story. 
threat modeling is, for the most part, ad hoc. you think about the
threats until you can’t think of any more, then you stop. and then you’re
annoyed and surprised when some attacker thinks of an attack you did-
n’t. my favorite example is a band of california art thieves that would
break into people’s houses by cutting a hole in their walls with a chain-
saw. the attacker completely bypassed the threat model of the defender.
the countermeasures that the homeowner put in place were door and
window alarms; they didn’t make a difference to this attack.
to help the process, i invented something called an attack tree. attack
trees provide a methodical way of describing threats against, and counter-
measures protecting, a system. by extension, attack trees provide a
methodical way of representing the security of systems. they allow you
to make calculations about security, compare the security of different sys-
tems, and do a whole bunch of other cool things.
basically, you represent attacks against a system in a tree structure,
with the goal as the root node and different ways of achieving that goal as
leaf nodes. by assigning values to the nodes, you can do some basic cal-
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 318


 
 

____________________________________________

SecretsAndLies.pdf page: 347

____________________________________________

culations with the tree (it’s called an and/or tree, if you’re interested) to
make statements about different attacks against the goal.
i’ll start with a simple attack tree for a noncomputer security system,
and build the concepts up slowly.
basic attack trees
figure 21.1 is a simple attack tree against a physical safe. each attack tree
has a goal, represented by the root node in the tree. the goal in this
example is opening the safe. that’s the root node; trees in computer sci-
ence grow upside down. to open the safe, an attacker can pick the lock,
learn the combination, cut open the safe, or install the safe improperly so
that he can easily open it later. to learn the combination, the attacker
either has to find the combination written down or get the combination
from the safe owner. and so on. each node becomes a subgoal, and chil-
dren of that node are ways to achieve that subgoal. (of course, this is just
a sample attack tree, and an incomplete one at that.)
note the and nodes and or nodes (in the figures, everything that
isn’t explicitly an and node is an or node). or nodes are alternatives:
attack trees 319
open safe
pick lock
i
learn combo cut open safe
p
install
improperly
i
get combo
from target
find written
combo
i
get target to
state combo
i
listen to
conversation
p
p = possible
i  = impossible
threaten
i
eavesdrop bribe
p
blackmail
i
and
figure 21.1 attack nodes. 
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 319


 
 

____________________________________________

SecretsAndLies.pdf page: 352

____________________________________________

to make this work, you must marry attack trees with knowledge
about attackers. different attackers have different levels of skill, access, risk
aversion, money, and so on. if you’re worried about organized crime, you
have to worry about expensive attacks and attackers who are willing to go
to jail. if you are worried about terrorists, you also have to worry about
attackers who are willing to die to achieve their goal. if you’re worried
about bored graduate students studying the security of your system, you
usually don’t have to worry about illegal attacks such as bribery and black-
mail. the characteristics of your attacker determine which parts of the
attack tree you have to worry about.
attack trees also let you play “what if” games with potential counter-
measures. in figure 21.6, for example, the goal has a cost of $20,000. this
is because the cheapest attack requiring no special equipment is bribing
the person who knows the combination. what if you implemented a
countermeasure—paying that person more so that he or she is less sus-
ceptible to bribes? if you assume that the cost to bribe that person is now
$80,000 (again, this is an example; in the real world you’d be expected to
research exactly how a countermeasure affects the node value), then the
cost increases to $60,000 (presumably to hire the thugs to do the threat-
ening).
pgp attack tree
figure 21.7 is an attack tree for the pgp e-mail security program. since
pgp is a complex program, this is a complex tree, and it’s easier to write
it in outline form than graphically. pgp has several security features, so
this is only one of several attack trees for pgp. this particular attack tree
has “read a message encrypted with pgp” as its goal. other goals might
be: “forge someone else’s signature on a message,” “change the signature
on a message,” “undetectably modify a pgp-signed or pgp-encrypted
message,” and so on.
if software can be modified (trojan horse) or corrupted (virus), it can
be used to have pgp generate an insecure public/private key pair (e.g.,
with a modulus whose factorization is known to the attacker). 
what immediately becomes apparent from the attack tree is that
breaking the rsa or idea encryption algorithms is not the most prof-
itable attack against pgp. there are many ways to read someone’s pgp-
324 c h a p t e r  t w e n t y - o n e
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 324


 
 

____________________________________________

SecretsAndLies.pdf page: 354

____________________________________________

326 c h a p t e r  t w e n t y - o n e
1.1.1.3.3. timing attacks on rsa/elgamal 
timing attacks have been reported on rsa; they
should also be feasible on elgamal. such an attack,
however, requires low-level monitoring of the recipi-
ent’s computer while he is decrypting the message.
1.1.2. break symmetric-key encryption 
1.1.2.1. brute-force break symmetric-key encryption (or) 
all symmetric-key algorithms supported for use by pgp
have key sizes of at least 128 bits. this is currently infeasi-
ble for brute-force searching.
brute-force searching is made somewhat easier by the
redundancy included at the beginning of all encrypted mes-
sages. see the openpgp rfc.
1.1.2.2. cryptanalysis of symmetric-key encryption 
the symmetric-key algorithms supported by pgp 5.x are
idea, 3-des, cast-5, blowfish, and safer-sk128.
no efficient methods are currently known for general
cryptanalysis of these algorithms.
1.2. determine symmetric key used to encrypt the message via other
means 
1.2.1. fool sender into encrypting message using public key
whose private key is known (or) 
1.2.1.1. convince sender that a fake key (with known pri-
vate key) is the key of the intended recipient 
1.2.1.2. convince sender to encrypt using more than one
key—the real key of the recipient, one a key whose pri-
vate key is known 
1.2.1.3. have the message encrypted with a different public
key in the background, unknown to the sender 
this could be done by running a program that fools the
user into believing that the correct key is being used, while
actually encrypting with a different key.
figure 21.7 (continued)
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 326


 
 

____________________________________________

SecretsAndLies.pdf page: 355

____________________________________________

attack trees 327
1.2.2. have the recipient sign the encrypted symmetric key (or) 
if the recipient blindly signs the encrypted key, he unwittingly
reveals the unencrypted key. the key is short enough so that
hashing should not be necessary before signing. or, if a mes-
sage can be found that hashes to the value of the encrypted key,
the recipient can be asked to sign the (hash of the) message.
1.2.3. monitor sender’s computer memory (or)
1.2.4. monitor receiver’s computer memory (or) 
the (unencrypted) symmetric key must be stored somewhere
in memory at some point during the encryption and decryp-
tion. if memory can be accessed, this gives a way to capture the
key and get at the message.
1.2.5. determine key from random number generator (or) 
1.2.5.1. determine state of the random number generator
when message was encrypted (or) 
1.2.5.2. implant software (virus) that deterministically alters
the state of random number generator (or) 
1.2.5.3. implant software that directly affects the choice of
symmetric key 
1.2.6. implant virus that exposes the symmetric key 
1.3. get recipient to (help) decrypt message (or) 
1.3.1. chosen ciphertext attack on symmetric key (or) 
the cipher feedback mode used by pgp is completely insecure
under a chosen ciphertext attack. by sending the (encryption
of the) same key to the recipient, along with a modified body
of the message, the entire contents of the message can be
obtained.
1.3.2. chosen ciphertext attack on public key (or) 
since rsa and elgamal are malleable, known changes can be
made to the symmetric key which is encrypted. this modified
continues
figure 21.7 (continued)
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 327


 
 

____________________________________________

SecretsAndLies.pdf page: 358

____________________________________________

encrypted messages without breaking the cryptography. you can capture
the person’s screen when he decrypts and reads the messages (using a tro-
jan horse like back orifice, a tempest receiver, or a secret camera),
grab the person’s private key after he enters a passphrase (back orifice, or
a dedicated computer virus), recover the person’s passphrase (a keyboard
sniffer that simply captures user keystrokes, tempest receiver, or back
orifice again), or simply try to brute-force the person’s passphrase (it will
have much less entropy than the 128-bit idea keys that it generates). in
the scheme of things, the choice of algorithm and the key length is prob-
ably the least important thing that affects pgp’s overall security. pgp not
only has to be secure, but it has to be used in an environment that lever-
ages that security without creating any new insecurities.
figure 21.8 is a more general attack tree: reading a specific message,
either in transit or on one of two computers.
330 c h a p t e r  t w e n t y - o n e
goal: read a specific message that has been sent from one win-
dows 98 computer to another.
1. convince sender to reveal message (or)
1.1. bribe user
1.2. blackmail user
1.3. threaten user
1.4. fool user
2. read message when it is being entered into the computer (or)
2.1. monitor electromagnetic emanations from computer screen
(countermeasure: use a tempest computer)
figure 21.8 attack tree for reading a specific e-mail message. 
1.4.4. implant virus to expose private key 
really a more sophisticated version of 1.4.2.1.4. in which the
virus waits for the private key to be decrypted before exposing it.
1.4.5. generate insecure public/private key pair for recipient.
figure 21.7 (continued)
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 330


 
 

____________________________________________

SecretsAndLies.pdf page: 360

____________________________________________

332 c h a p t e r  t w e n t y - o n e
creating and using attack trees
how do you create an attack tree? first, you identify the possible attack
goals. each goal forms a separate tree, although they might share subtrees
and nodes. then, think of all attacks against each goal. add them to the
tree. repeat this process down the tree until you are done. give the tree
to someone else, and have him think about the process and add any nodes
he thinks of. repeat as necessary, possibly over the course of several
months.
the process still requires creativity, but the structure takes an ad hoc
brainstorming process and replaces it with a repeatable methodology.
remember to look for attacks throughout the vulnerability landscape,
and at every step of the attack process. of course there’s always the chance
that you forgot about an attack, but you’ll get better with time. like any
security analysis, creating attack trees requires a certain mindset and takes
practice.
once you have the attack tree, and have researched all the node val-
ues (these values will change over time, both as attacks become easier and
as you get more exact information on the values), you can use the attack
tree to make security decisions. you can look at the values of the root
node to see if the system’s goal is vulnerable to attack. you can determine
if the system is vulnerable to a particular kind of attack; distributed denial-
of-service, for instance. you can use the attack tree to delineate the secu-
rity assumptions of a system; for example, the security of pgp might
assume that no one could successfully bribe the programmers. you can
determine the impact of a system modification or a new vulnerability dis-
covery; recalculate the nodes based on the new information and see how
the goal node is affected. and you can compare and rank attacks: which
are cheaper, which are more likely to succeed, and so on.
one of the surprising things that comes out of this kind of analysis is
that the areas people think of as vulnerable usually aren’t. with pgp, for
example, people generally worry about key length. should they use 1024-
bit rsa or 2048-bit rsa? the attack tree shows that the rsa key
length doesn’t really matter. there are all sorts of other attacks—installing
a keyboard sniffer, modifying the program on the victim’s hard drive—
that are much easier than breaking the public key. increasing the key
length from 1024 bits to 2048 bits doesn’t affect any of the overall diffi-
culty of the attack tree; it’s the computer-security attacks that are much
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 332


 
 

____________________________________________

SecretsAndLies.pdf page: 365

____________________________________________

unfortunately, there is no such thing as a comprehensive security
checklist. those of us who do this kind of thing frequently have devel-
oped our own security checklists: lists of attacks and potential vulnerabil-
ities that we’ve either seen in commercial products, read about in
academic papers, or thought of on our own. these lists are huge—a cou-
ple of years ago i had 759 separate attacks on my list—but they are not
comprehensive.
it is easy to test for any given weakness. some are easier to test for
than others. testing for every weakness on my list is time-consuming, but
straightforward. testing for every known weakness is harder still; it means
that i have to keep my list up to date. it takes time, but i can do it. but
here’s the rub: testing for all possible weaknesses is impossible.
note that i didn’t say “very hard” or “incredibly difficult.” i said
“impossible.”
testing for all possible weaknesses means testing for weaknesses that
you haven’t thought of yet. it means testing for weaknesses that no one
has thought of yet: weaknesses that haven’t even been invented yet. it’s
like building a bridge. you might be able to say that the bridge cannot
collapse as a result of natural causes. more likely, you will be able list the
conditions that cannot be the proximal cause of a collapse. you might
even be able to delineate the sorts of terrorist attacks that the bridge can
withstand. but you can never say that the bridge will stand in the face of
technology that hasn’t been invented yet.
nothing here is meant to imply that this holds true for mass-market
software only. this discussion applies equally to security hardware, large
proprietary systems, military hardware and software, and everything else.
it even applies to security technologies having nothing to do with com-
puters. the problems are there regardless.
so what is a system developer to do? ideally, he has to stop relying on
his in-house developers and beta testers. he has to hire security experts to
do his security testing. he has to spend a lot of money on this; assume it
takes the same level of effort to test the security of a system as it did to
design and implement it in the first place.
no one is going to do that, with the exception of the military. and
even the military is probably not even going to do that, with the excep-
tion of things like nuclear command and control systems.
what companies are going to do is what they’ve done all along.
they’re going to release insecure products and fix security problems that
are discovered, and published, after the fact. they’re going to make out-
product testing and verification 337
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 337


 
 

____________________________________________

SecretsAndLies.pdf page: 366

____________________________________________

landish claims and hope nobody calls them on it. they’re going to hold
cracking contests and other publicity stunts. they’ll issue new versions so
fast that by the time someone bothers to complete a security analysis,
they’ll say “but that was three versions ago.” but the products will be
insecure nonetheless.
discovering security flaws after the fact
every day, new security flaws are discovered in shipping software prod-
ucts. they’re discovered by customers, researchers (academics and hack-
ers), and criminals. how frequently depends on the prominence of the
product, the doggedness of the researchers, the complexity of the prod-
uct, and the quality of the company’s own internal security testing. in the
case of a popular operating system, it might happen several times per
week. in the case of an obscure encryption program that no one’s heard
of, it might happen once a lifetime.
anyway, someone finds a security vulnerability. now what?
there are several things he can do. he can keep quiet and tell no one.
he can tell his confidants. he can alert the product vendor. he can just
tell his customers, trying to keep the bug obscure so that only his products
protect the user. (i’ve seen companies do this.) or he can tell the world.
(of course he can always try to commit a crime using the vulnerability,
but let’s assume that he is an honest bloke.) the practice of telling the
world is known as full disclosure, and it has become popular over the past
several years. and it is the subject of a violent debate.
but first a soupçon of history.
in 1988, after the morris worm illustrated how susceptible the inter-
net is to attack, the defense advanced research projects agency
(darpa) funded a group that was supposed to coordinate security
response, increase security awareness, and generally do good things. the
group is known as cert—more formally, the computer emergency
response team—and its response center is in pittsburgh at carnegie
mellon university.
over the years cert has acted as kind of a clearinghouse for secu-
rity vulnerabilities. people are supposed to send vulnerabilities they find to
cert. cert then verifies that the vulnerability is real, quietly alerts the
vendor, and publishes the details (and the fix) once the vendor fixes the
vulnerability.
338 c h a p t e r  t w e n t y - t w o
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 338


 
 

____________________________________________

SecretsAndLies.pdf page: 369

____________________________________________

when microsoft published the patch, and (2) afterward, because many sys-
tem administrators didn’t implement microsoft’s patch.
was publishing better, or would it have been better to keep quiet?
sometimes it depends on the vendor. most companies react well to
attacks against their systems. they acknowledge and fix the problem, post
the fix on their web sites, and everything goes back to normal. some
vendors react less well; the various digital cellular companies responded
with all sorts of lies, insults, and misdirection in response to the published
breaks of their encryption algorithms. the entertainment industry
responded by initiating legal action against the people who exposed the
dvd player’s lousy security (and the people who subsequently talked
about it). generally, exposed vulnerabilities that can’t be fixed easily—it’s
a lot harder to modify 10 million fielded cellular telephones than it is to
post a software fix on the internet—aggravate companies more.
sometimes the researcher has no choice. one nsa employee, speak-
ing off the record, claimed that his colleagues have discovered several new
internet attacks but have been prohibited from publishing them. some
have been later discovered by other researchers; others remain secret.
sometimes he has a choice, but chooses to remain silent. steve bellovin
suppressed a paper he wrote on attacks against the dns system for several
years. bellovin and cheswick purposely didn’t talk about the syn flood
attack in their firewalls book.
netscape used to offer $1,000 (and a free t-shirt) rewards to anyone
who found a security bug in their software. they wrote quite a few
checks, except for a 1997 incident when danish hacker found a security
hole and demanded more money. as it turned out, he didn’t get his
money: his description of the effects of the bug enabled netscape engi-
neers to reproduce and fix it without his help. in 2000, a french
researcher figured out how to break the security in the cb (groupement
des cartes bancaires) smart card system. then, depending on whom you
believe, he offered his services to groupement or tried to blackmail them.
he was arrested, and eventually received a suspended sentence.
security is by nature adversarial, even in the ivory towers of academia.
someone proposes a new scheme: an algorithm, a protocol, a technique.
someone else breaks it. a third person repairs it. and so on. it’s all part of
the fun. but when it comes to fielded systems, it can get a lot trickier. is
the benefit of publicizing an attack worth the increased threat of the
enemy learning about it? (in nsa’s language, this is known as the equities
product testing and verification 341
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 341


 
 

____________________________________________

SecretsAndLies.pdf page: 370

____________________________________________

issue.) why should the company profit from the work of the researcher?
will the company ignore the problem unless the researcher calls the press?
does the researcher even care about the public’s reaction? what’s the
researcher’s agenda anyway?
this last question isn’t discussed as much as it should be. publishing a
security vulnerability is a publicity attack; the researcher is looking to get
his own name in the newspaper by successfully bagging his prey. some-
times the publicizer is a security consultant, or an employee of a company
that offers vulnerability assessments or defensive network security prod-
ucts. this is especially true if the vulnerability is publicized in a press
release; sending something out on pr newswire or business wire is
expensive, and no one would do it unless he thought he was getting
something in return.
in general, i am in favor of the full-disclosure movement, and think
it has done a lot more to increase security than it has to decrease it. the
act of writing this book, which can be read by both the good guys and the
bad guys, does not cause the insecurities i talk about. similarly, publiciz-
ing a vulnerability doesn’t cause it to come into existence. given that
vendors don’t bother fixing vulnerabilities that are not published—this is
not just a jeremiad against microsoft, we’ve seen this from almost every
major software company—publicizing is the first step toward closing that
vulnerability. punishing the publicizer feels a lot like shooting the mes-
senger; the real blame belongs to the vendor that released software with
the vulnerability in the first place.
there are exceptions to this rule.
first, i am opposed to publicity that primarily sows fear. publishing
vulnerabilities for which there’s no real evidence is bad. (an example of
this is when someone found a variable containing the three letters “nsa”
in microsoft’s cryptography api and announced that the national secu-
rity agency had installed a trap door in microsoft products, solely on the
basis of the variable name.) publishing security vulnerabilities in critical
systems that cannot be easily fixed and whose exploitation will cause seri-
ous harm (the air traffic control software, for example) is bad. i believe it
is the researchers’ responsibility to balance disclosing the vulnerability ver-
sus endangering the public.
second, i believe in giving the vendor advance notice. cert took
this to an extreme, sometimes giving the vendor years to fix the problem.
the result is that many vendors didn’t take the notifications seriously. but
342 c h a p t e r  t w e n t y - t w o
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 342


 
 

____________________________________________

SecretsAndLies.pdf page: 375

____________________________________________

one, the contests are generally unfair. cryptanalysis assumes that the
attacker knows everything except the secret. he has access to the algo-
rithms and protocols, the source code, everything. he knows the cipher-
text and the plaintext. he may even know something about the key. and
a cryptanalytic result can be anything. it can be a complete break: a result
that breaks the security in a reasonable amount of time. it can be a theo-
retical break: a result that doesn’t work “operationally,” but still shows
that the security isn’t as good as advertised. it can be anything in between.
most cracking contests have arbitrary rules. they define what the attacker
has to work with, and how a successful break looks. some don’t disclose
the algorithms.
computer-security hacking contests are generally no better. they
don’t disclose how the products are being used, so that you can’t tell
whether a particular attack is a result of a product failure or an implemen-
tation failure. they don’t clearly delineate between the various pieces of
the system: if the contest is to test a firewall, what about vulnerabilities of
the operating system that compromise the firewall?
these tests have arbitrary rules of winning. in 1999, microsoft set up
a windows 2000 web server and dared hackers to try and break in. the
server soon disappeared from the internet, only to reappear later with
microsoft claiming a power failure as the reason for the disappearance.
(oddly enough, this power failure only affected the test system, and they
seemed to have forgotten to install an uninterruptible power supply.)
unfair contests aren’t new. back in the mid-1980s, the authors of an
encryption algorithm called feal issued a contest. they provided a
ciphertext file, and offered a prize to the first person to recover the plain-
text. since then, the algorithm has been repeatedly broken by cryptogra-
phers. everyone agrees that the algorithm is fundamentally flawed. still,
no one won the contest.
two, the analysis is not controlled. contests are random tests. do ten
people, each working 100 hours to win the contest, count as 1,000 hours
of analysis? or did they all try the same dozen attacks? are they even
competent analysts, or are they just random people who heard about the
contest and wanted to try their luck? just because no one wins a contest
doesn’t mean the target is secure . . . it just means that no one won.
in 1999, pc magazine set up both a windows nt and a linux box,
and announced a hacking contest. the linux box was the first one
hacked. does that mean that linux is less secure? of course not; it just
product testing and verification 347
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 347


 
 

____________________________________________

SecretsAndLies.pdf page: 376

____________________________________________

means that the people who bothered playing the game broke into the
linux box first.
three, contest prizes are rarely good incentives. security analysis is a
lot of work. people who are good at it are going to do the work for a vari-
ety of reasons—money, prestige, boredom—but trying to win a contest is
rarely one of them. security professionals are much better off analyzing
systems where they are being paid for their analysis work, or systems for
which they can publish a paper explaining their results.
just look at the economics. taken at a conservative $200 an hour for
a competent cryptanalyst or computer-security guru, a $10k prize pays
for just over a week of work—not enough time to even dig through the
code. a $100k prize sounds impressive, but reverse engineering the prod-
uct is boring and that still might not be enough time to do a thorough job.
a prize of $1 million starts to become interesting, but most companies
can’t afford to offer that. and the analyst has no guarantee of getting paid:
he may not find anything, he may get beaten to the attack and lose out
to someone else, or the company might change the rules and not pay.
why should someone donate his time (and good name) to the company’s
publicity campaign?
and four, contents can never end with a positive security result. if
something is broken in a contest, you know that it is insecure. but if
something isn’t broken in a contest, it doesn’t mean that it is secure.
the preceding four reasons are generalizations. there are exceptions,
but they are few and far between. the rsa challenges, both their factor-
ing challenges and their symmetric brute-force challenges, are fair and
good contests. these contests are successful not because the prize money
is an incentive to factor numbers or build brute-force cracking machines,
but because researchers are already interested in factoring and brute-force
cracking. the contests simply provide a spotlight for what was already an
interesting endeavor. the aes contest, although more a competition
than a cryptanalysis contest, was also fair.
contests, if implemented correctly, can provide useful information
and reward particular areas of research. they can help find flaws and cor-
rect them. but they are not useful metrics to judge security. a home-
owner can offer $10,000 to the first person who successfully breaks in and
steals a book on a certain shelf. if no one does so before the contest ends,
that doesn’t mean the home is secure. maybe no one with any burgling
348 c h a p t e r  t w e n t y - t w o
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 348


 
 

____________________________________________

SecretsAndLies.pdf page: 377

____________________________________________

ability heard about the contest. maybe they were too busy doing other
things. maybe they weren’t able to break into the home, but they figured
out how to forge the real estate title to put the property in their name.
maybe they did break into the home, but took a look around and decided
to come back when there was something more valuable than a $10,000
prize at stake. the contest proved nothing. 
cryptanalysis contests are generally nothing more than a publicity
tool. sponsoring a contest, even a fair one, is no guarantee that people will
analyze the target. surviving a contest is no guarantee of no flaws in the
target.
evaluating and choosing security products
it’s generally not possible for average people—or the average company, or
the average government, for that matter—to create their own security
products. most often they’re forced to choose between an array of off-
the-shelf solutions and hope for the best. the lessons of this book, that it’s
practically impossible to design secure products and that most commercial
products are insecure, aren’t heartening. what can the harried system
administrator, charged with securing his embassy’s e-mail system or his
company’s network, do? what about the average citizen, concerned
about the security of different electronic commerce systems or the privacy
of her personal medical information?
the first question to ask is whether or not it really matters. or, more
exactly, whose security problem is this anyway? i care about my personal
privacy. i don’t really care about visa’s credit card fraud problems. they
limit my liability to $50, and will even waive that if i complain. i do care
about the pin on my atm card; if someone cleans out my account, it’s
my problem and not the bank’s.
similarly, some systems matter but are not within my control. i can’t
control what kind of firewalls and database security measures the irs uses
to protect my tax information, or my medical insurer uses to protect my
health records. maybe i can change insurers, but generally i don’t have
that kind of freedom. (i suppose that if i were wealthy enough, i could
choose banks in better regulatory environments—switzer land, for exam-
ple—but that option is out of reach of most people.) even if laws demand
product testing and verification 349
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 349


 
 

____________________________________________

SecretsAndLies.pdf page: 383

____________________________________________

the second reason is the modularity of complex systems. in chapter
10, i talked about modular code and the security problems associated with
it. complex systems are necessarily modular; there’s no other way to han-
dle the complexity than breaking it up into manageable pieces. we could
never have made the internet as complex and interesting as it is today
without modularity. but increased modularity means increased security
flaws, because security often fails where two modules interact.
the third reason is the interconnectedness of complex systems. dis-
tributed and networked systems are inherently risky. complexity leads to
the coupling of systems, which can lead to butterfly effects (minor prob-
lems getting out of hand). we’ve already seen examples of this as every-
thing becomes internet-aware. for years we knew that internet
applications like sendmail and rlogin had to be secure, but the recent epi-
demic of macro viruses shows that microsoft word and excel need to be
secure. java applets not only need to be secure for the uses they are
intended for, but they also need to be secure for any other use an attacker
might think of. cross-site scripting exploits subtle interactions among
cgi scripts, html, frames, web server software, and cookies. in 2000,
a bug in internet explorer 5.0 locked up windows 2000 when it was
installed with 128-bit cryptography. photocopiers, maintenance ports on
routers, mass storage units: these can all be made internet-aware, with
the associated security risks. rogue printer drivers can compromise win-
dows nt; postscript files can have viruses. malicious e-mail attachments
can tunnel through firewalls. remember the version of windows nt
that had a c2 security rating, but only if it was unconnected to a network
and had no floppy drive? remember the webtv virus? how long
before someone writes a virus that infects cell phones?
the fourth reason is that the more complex a system is, the more
recondite it is. in chapter 17, i talked about social engineering and the
poor interactions between people and security. these problems are exac-
erbated by complex systems. the people running the actual system typi-
cally do not have a thorough understanding of the system and the security
issues involved. and if someone doesn’t understand a system, he is more
likely to be taken advantage of by someone who does. complexity not
only makes it virtually impossible to create a secure system, it also makes
the system extremely hard to manage. 
the fifth reason is the difficulty of analysis. in chapters 18 through
21, i outlined a procedure for designing and analyzing secure systems:
the future of products 355
453803_ch23.qxd:453803_ch23.qxd  4/17/13  11:31 am  page 355


 
 

____________________________________________

SecretsAndLies.pdf page: 386

____________________________________________

serves to illustrate how inadequate 500 people-years are. 
you can also see this complexity increase in the number of system
calls an operating system has. the 1971 version of unix had 33. by the
early 1990s, operating systems had about 150. windows nt 4.0 sp3 has
3,433. see table 23.2.
table 23.2 trend to complexity in operating systems
operating system year system calls
unix 1ed 1971 33
unix 2ed 1979 47
sunos 4.1 1989 171
4.3 bsd net 2 1991 136
sun os 4.5 1992 219
hp ux 9.05 1994 163
line 1.2 1996 211
sun os 5.6 1997 190
linux 2.0 1998 229
windows nt 4.0 sp3 1999 3,433
early firewalls had to deal with ftp, telnet, smtp, nntp, and
dns. that’s all. modern firewalls have to handle hundreds of protocols,
and a labyrinthine set of network-access rules. some neoteric protocols
are designed to look like http, in order to “work with” (i.e., avoid)
firewalls. and dial-in users didn’t used to have to be concerned with fire-
walls; now home broadband users, on dsl and cable modems, do. even
worse, there’s software available that lets home users set themselves up as
web servers. more features, more complexity, more insecurity.
public-key certificates in x.509 version 1 were specified in 20 lines of
asn.1 notation. x.509 version 3 certificates took about 600 lines. set
certificates: about 3,000 lines. 
the entire set standard is 254 pages long. and that’s just the formal
protocol specification; there’s also a 619-page programmer’s guide and a
72-page business description. for various reasons it seems that set will
never see widespread use, but in any case i believe that we are not capa-
ble of implementing something this labyrinthine without bugs. the per-
formance bugs will be (for the most part) fixed during beta testing; the
security bugs will lie dormant. but they will be there. if the right person
finds one, he will announce his findings to the press. if the wrong person
358 c h a p t e r  t w e n t y - t h r e e
453803_ch23.qxd:453803_ch23.qxd  4/17/13  11:31 am  page 358


 
 

____________________________________________

SecretsAndLies.pdf page: 387

____________________________________________

finds one, he will use it to attack the online credit card system: maybe to
mint money, maybe to create valid-looking but phantom credit card
accounts, maybe just to screw with credit card processing and bring the
system to its knees.
complexity is creeping into everything. the 2000 mercedes 500 has
more computing power than a 747-200. my old thermostat had one dial;
it was easy to set the temperature. my new thermostat has a digital inter-
face and a programming manual. i guarantee that most people will have
no idea how to set it. thermostats based on sun microsystems’s “home
gateway” system come with an internet connection, so you can conve-
niently contract with some environmental control company to operate
your too-complicated thermostat. sun is envisioning internet connec-
tions for all your appliances and your door locks. do you think anyone
will have checked the refrigerator software for security bugs? i’ve talked
about modern malicious code, and the interactions among java, html,
cgi scripts, and web browsers. isn’t anyone else worried that the new
cell phones, equipped with the wireless access protocol, will be able to
download java applets? it’s only a matter of time before we have a cell
phone virus.
computer games used to be simple. now they’re networked. any-
one can go to a web site and set himself up for multiplayer play. now
anyone else can log in to his machine across the internet. presto, he’s a
server. mom and dad might keep some proprietary information on their
computer—work secrets, financial information—and suddenly junior has
invited the world to log on. has anyone checked these games for security
bugs? a vulnerability in the automatic update feature in the game
quake3arena allows an attacker to update any file on the user’s com-
puter. napster also opens your computer up as a server, and overflow
bugs have already been found in the software.
it gets worse. the current generation of video game machines—the
sega dreamcast, sony playstation 2—comes with features like 56k
modems, ip stacks, and web browsers. millions of these have been sold.
maybe the browsers and operating systems will be secure; if they are,
they’ll be the first ever. it’ll be big fun; you’re playing sonic over modem
with some other kid, and he’ll get root on your game machine and win.
if it’s just a game console: woo hoo! it doesn’t matter. but remember that
the game companies are going to want you to do all your e-shopping with
your game console. there’ll be credit card numbers, electronic wallets,
the future of products 359
453803_ch23.qxd:453803_ch23.qxd  4/17/13  11:31 am  page 359


 
 

____________________________________________

SecretsAndLies.pdf page: 392

____________________________________________

2000. microsoft issued a patch that fixed the vulnerability in july 1998,
and reissued a warning in july 1999 when it became clear that many users
never bothered installing the patch.
isn’t anyone paying attention?
not really. or, at least, far fewer people are paying attention than
should be. and the enormous need for digital security products necessi-
tates experts to design, develop, and implement them. this resultant
dearth of experts means that the percentage of people paying attention
will get even smaller.
here is a paradigmatic scenario for the design of most products with
security in them. the manager finds some guy who thinks security is cool
and designates him as the person in charge of that part of the system. this
person might know something about security, or he might not. he might
read a book or two on the subject, or he might not. designing security is
fun—cat and mouse, spy vs. spy, just like in the movies—so he does.
implementing it is just like implementing anything else in the product:
make it work and meet your deadline. everything works great—after all,
security has nothing to do with functionality—so the manager is happy.
however, due to the general lack of security expertise, the security
features are completely ineffective. no one has any reason to believe that
this is so, so no one knows.
it’s a little better if the product being designed is a security product.
it’s more likely that the designers will understand security. but they can’t
do everything. someone who designed a firewall product once told me
about buffer overflows in his code. he said that he did all he could to
ensure that there were none—and i believe that he was thorough—but
he said that he couldn’t control the rest of the programmers on the team.
he tried, but he couldn’t. several serious vulnerabilities due to buffer
overflows in the code have been discovered, and fixed, over the years. no
one believes there aren’t more, waiting to be discovered.
i’ve been constantly amazed by the kinds of things that break security
products. i’ve seen a file encryption product with a user interface that
accidentally saves the key in the clear. i’ve seen vpns where the tele-
phone configuration file accidentally allows untrusted persons to authen-
ticate themselves to the server, or where one vpn client can see the files
of all other vpn clients. there are a zillion ways to make a product inse-
cure, and manufacturers manage to stumble on a lot of those ways again
and again.
364 c h a p t e r  t w e n t y - t h r e e
453803_ch23.qxd:453803_ch23.qxd  4/17/13  11:31 am  page 364


 
 

____________________________________________

SecretsAndLies.pdf page: 396

____________________________________________

money in a pouch hidden under their clothing. that way, if they’re pick-
pocketed, the thief doesn’t get everything. smart espionage or terrorist
organizations divide themselves up into small cells; people know others in
their own cell, but not those in other cells. that way, if someone is cap-
tured or turned in, he can only damage those in his own cell. compart-
mentalization is smart security, because it limits the damage from a
successful attack. it’s common sense, and there are lots of examples: users
get individual accounts, office doors are locked with different keys, access
is based on clearance plus need to know, individual files are encrypted
with unique keys. security is not all-or-nothing; security breaches should
not be, either.
a similar precept is the one of least privilege. basically, this means that
you should only give someone (or, by extension, some computer
processes) the privileges needed to accomplish the task. you see this all
the time in everyday life: you have the key to your office, but not every
office in the building. only authorized armored-car delivery people can
unlock atms and put money inside. even if you have a particular secu-
rity clearance, you are only told things that you “need to know.” 
computers offer many more examples. users only have access to the
servers they need to do their job. only the system administrator has the
root password to the entire computer; users have individual passwords to
their own files. sometimes group passwords protect shared files; only
those who need access to those files know the group password. certainly
it’s easier to give everyone the root password, but it’s more secure to only
give people the privileges they need. the whole unix and nt permis-
sions system is based on this idea.
many internet attacks can be traced to breaking this principle 
of least privilege. once an attacker gets access to a user account—by
breaking a password or something—he tries a bunch of attacks in order to
get root privileges. many of the attacks against java try to break out of the
java sandbox—a way of enforcing minimal privileges—and into a mode
where the attacker can get privileged status. attacks against the dvd
security system, security systems in some transit farecard systems, and
many pay-tv security systems can all be traced to the system having a
global secret in each of the consumer devices: a violation of least privilege.
compartmentalization is also important because a system’s security
degrades in proportion to its use. the larger, more popular, more integral
a computer is, the less secure it is. this is one reason why the internet—
368 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 368


 
 

____________________________________________

SecretsAndLies.pdf page: 399

____________________________________________

using a live database terminal. when the clerk takes your card and swipes
it through the verifone terminal, that terminal calls back to a database
and confirms that the card is not stolen, that you have available credit, and
so forth. think back to a time when, for whatever reason, the terminal
didn’t work: it was broken, the phone line was down, whatever. did the
merchant tell you that he wouldn’t accept your credit card? of course
not. he pulled out the old system of paper slips and did the transaction
manually.
this cavalier approach to security is pervasive, and it’s the reason
denial-of-service attacks can become invasive attacks. i already talked
about attackers tripping burglar alarms until they are turned off. other
attacks are subtler. few people have the discipline not to communicate if
they cannot communicate securely. even the military, which you think
would take this seriously, has screwed this up again and again.
what you want is for systems to fail securely; that is, fail in such a way
as to be more secure, not less. if an atm’s pin verification system does
not work, it should fail in such a way as to not spit money out the slot. if
a firewall crashes, it should crash in such a way as to not let any packets in.
if a slot machine fails, it should not send coins pouring into the payout
tray.
this same principle is used in safety engineering, and is called fail-safe.
if a microprocessor in an automobile fails, you don’t want it failing by
forcing maximum throttle. if a nuclear missile fails, you don’t want it fail-
ing by launching. fail-safe is a good design principle.
leverage unpredictability
again and again in this book i rail against security by obscurity: proprietary
cryptography, closed source code, secret operating systems. obscurity has
its uses: not in products, but in how products are used. i call this unpre-
dictability.
one of the strengths a defender has against an attacker is knowledge
of the terrain. just as an army doesn’t broadcast the location of its tanks,
antiaircraft batteries, and battalions to the enemy, there’s no reason to
broadcast your network topology to everyone that asks. too many com-
puters respond to any query with their operating system and version
number; there’s no reason to give out this information. much better
would be a login screen that reads: “warning: proprietary computer. use
security processes 371
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 371


 
 

____________________________________________

SecretsAndLies.pdf page: 403

____________________________________________

have the manpower. he’s on his own. he can hire a bodyguard if he can
afford it, but that doesn’t guarantee anything either.
what society does is detect crime after the fact. “hmm, officer, we
just found bob’s bullet-riddled body buried in the end zone at giants sta-
dium. i think i detect a crime here.” we investigate crimes that we have
detected, collect evidence that can be used (here’s the critical piece) to
convince a group of neutral parties that the defendant is guilty, and then
punish that person. this punishment process is supposed to act as some
kind of back channel into society at large and have a preventive effect on
copycat criminals. (yes, the point of sentencing is to punish the guilty, but
the real benefit to society is in preventing more crime.) even better, the
mere threat of the whole process is supposed to have a preventive effect.
and it’s a good thing that the whole complicated system works, more
or less, because preventing crime is a whole lot harder than detecting
crime. in the digital world, the same truth holds. credit card companies
do what they can to prevent criminals from committing fraud, but mostly
they rely on detection and, in extreme cases, prosecution. cell phones can
be cloned, but detection mechanisms limit financial losses.
think of antishoplifting technologies. you can make things hard to
steal by bolting them down, attaching cables to them, locking them in
glass cases, or putting them behind the counter. this works, but reduces
sales because the consumer likes to touch the merchandise. in response,
industry has developed many theft detection technologies: tags attached to
the merchandise that cause an alarm to sound if they are removed from
the building. (there’s another interesting antishoplifting technology used
for garments: tags attached to the garment that spread colored dye if
removed improperly. this is known as benefit denial.)
on the internet, detection can be a lot of work. it’s not enough to
put up a firewall and be done with it; you need to detect attacks against
the network. this means reading, understanding, and interpreting the
reams of audit logs that the firewall produces. this means reading, under-
standing, and interpreting the reams of audit logs that the routers, servers,
and other devices on the networks produce—we have to assume that
some attacks will bypass the firewall. these bypass attacks always leave
footprints somewhere; detection means finding them.
good detection means finding intruders in something approaching
real time, while they are still engaged in the attack. (responding after the
attack appears in the morning newspapers is often too late.) this neces-
security processes 375
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 375


 
 

____________________________________________

SecretsAndLies.pdf page: 404

____________________________________________

sarily means a real-time monitoring system, whether it is a security-
conscious network operations center monitoring your computer net-
work, an ai program looking for anomalous visa spending patterns or
phone calling card usage patterns, or the norad ballistic missile track-
ing systems. the sooner you detect something, the sooner you can
respond.
analyze attacks
simple detection isn’t enough; you need to understand the attack and
what it means. traditionally, the military breaks the process down into
four generic steps:
detection. perceiving that you’re under attack. imagine that three key
servers on your network crash at the same time. is that an attack, or just a
problem with your networking software? or maybe a freak coincidence? if
you don’t even know you’re under attack, it’s impossible to respond. 
localization. determining where the attack is. just because you know that
your network is under attack, it doesn’t necessarily mean that you know
which computers or ports are under attack. you might know that the server
crashes are the result of an attack, but have no idea what the attacker has
done to cause the crashes, and what other things he is doing. 
identification. determining who the attacker is and where he is working
from. each attacker has different strengths and weaknesses, depending on
who he is and where he is working from. an attacker in the united states,
for example, can be dealt with differently than an attacker in moldavia.
(this step is more important in a traditional military process than in net-
work security.)
assessment. understanding the attacker, his strategy and tactics, his capabili-
ties, and maybe even his vulnerabilities. this information is critical to deter-
mining a suitable response. a script kiddie deserves a very different reaction
than an industrial spy. the kiddie is likely to just go away if you respond at
all; a more tenacious attacker won’t be dissuaded so easily. 
each of these steps is more difficult than the previous one, and each
requires more detailed information and expertise of analysis. and often
this analysis requires human expertise; a computer alone is going to fail
sooner or later (although an automatic program may do a pretty good job
most of the time).
376 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 376


 
 

____________________________________________

SecretsAndLies.pdf page: 405

____________________________________________

each step also gives you more information about the situation, and
the more information you have (and the sooner you have it), the better
armed you are. unfortunately, most network administrators never know
they’re under attack, or if they do, they don’t understand where the attack
is coming from. identification and assessment is particularly hard on the
internet, where it is easy for an attacker to disguise his location.
speed is of the essence. the faster you can analyze an attack, the faster
you can respond.
respond to attacks
it’s all about response. a burglar alarm that rings and rings, with no one
to respond to it, is no better than no burglar alarm at all. it’s like a car
alarm sounding in a bad neighborhood; no one pays attention. response
is what makes detection valuable.
sometimes the response is easy: an attacker has stolen someone’s
phone calling card number, so don’t allow that number to be used any-
more. sometimes it’s more complicated: “someone has broken into our
electronic commerce server. we can shut the server down, but we’ll lose
$10 million for every hour we’re down. now what?”
response is complicated, and often involves intelligent people mak-
ing split-second decisions without a lot of time to fully think things
through. “he’s over the wall and approaching the skylight. what do we
do now?” it depends a lot on the situation. you can do nothing. you can
shoo him away. you can shoo him away and try to make sure he can
never get back. you can shoo him away, try to figure out how he got in,
and close the vulnerability.
that’s only one half of response: making the problem go away.
equally important is the other half: tracking down and finding the attack-
ers. this can be very difficult on some systems; on the internet an attacker
can engage in what is called connection laundering: hopping from one com-
puter to another to disguise the origin of a connection. the police don’t
have a lot of investigative time for this, unless lives or a lot of money is
involved, and i expect private companies to offer this kind of forensic ser-
vice. a company that has a broad view of the entire internet can even start
collecting dossiers on particular attackers.
prosecution opens a can of worms that is completely foreign to most
computer people: the legal system. identifying an attacker isn’t enough;
security processes 377
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 377


 
 

____________________________________________

SecretsAndLies.pdf page: 406

____________________________________________

you also need to be able to prove it in court. there have been cases in
england where people have been accused of this or that atm fraud. the
defense attorney requests details about the bank’s security mechanisms:
the technologies, the audit logs, the procedures . . . everything he can
think of. the bank turns to the judge and says: “we can’t show them that,
it would compromise security.” the judge throws out the case. the secu-
rity system might be the paragon of detection—it might correctly finger
the criminals—but if it can’t survive the discovery process, it’s not suffi-
ciently useful.
when john walker was put on trial for spying, the nsa carefully
weighed the risks of making information about the cryptography devices
he compromised public versus keeping the full extent of the damage he
caused secret. good detective security measures need to be able to go
through the legal process—including inquisitional cross-examinations
with the help of expert witnesses—without losing their effectiveness in
the process. and good detection and audit mechanisms should produce
audit logs that are admissible in court, and that prove guilt. and it should
be possible to make these logs public without revealing any organizational
secrets: something called knowledge partitioning. a legal discovery process
should not result in any security violations.
be vigilant
vigilance means continuous. for detection and response to be effective, it
needs to work all the time: 24 hours a day, 365 days a year. guard services
offer 24-hour protection. security-alarm monitoring companies don’t go
home for the weekends. it can’t be any different in the digital world. you
can’t put a splash screen on your network connections saying: “please
restrict all hacking attempts to within the hours of nine in the morning
and five in the evening, monday through friday, excepting holidays.”
attackers follow their own schedules.
attacks often happen at inconvenient times. criminal hacking follows
the academic year. all sorts of commerce fraud—atms, credit cards—
goes up during the christmas season as people find themselves in need of
money. smart criminals attack banking systems friday afternoon, after
they’ve closed for the weekend. in 1973, the arab countries attacked
israel during yom kippur: the holiest jewish holiday of the year. if some-
one were going to launch a serious attack against a system, he would pick
an equally inconvenient time.
378 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 378


 
 

____________________________________________

SecretsAndLies.pdf page: 407

____________________________________________

vigilance means immediateness. in any aspect of security, timeliness is
next to godliness. it’s much more useful to detect an attack in progress
than a week, or even an hour, after it has happened. it’s far better to
upgrade your systems in response to a vulnerability now, and not next
month. sometimes reacting late is no better than not reacting at all.
vigilance also means preparedness. any detection and response team
needs to know what to do when an attack occurs. when yahoo! got
whacked with a denial-of-service attack in 2000, it took them three hours
to get back up and running. partly this was because yahoo! had never
seen this kind of attack before. whenever processes are automated, and
exceptions become rarer, people forget how to react. a monitoring and
response service is only useful if it regularly sees attacks, and continuously
practices how to respond.
watch the watchers
the banking industry has long known that layers of audit provide good
security. managers audit the tellers. internal auditors audit the managers.
outside auditors reaudit things, but with different methods—effectively
auditing the internal auditors. the outside auditors act as a trusted third
party does in a protocol; they are paid to audit the system and don’t care
whether they find problems or not; they get paid regardless. the casino
industry likes to call this process “people watching people watching peo-
ple.” dealers watch the players, floormen watch the dealers, pit bosses
watch the floormen, and surveillance watches the pit bosses.
in the banking industry, this process is enhanced by mandatory vaca-
tions. the idea is that if someone else is doing your job, then maybe he’ll
notice evidence of your crimes. one example is lloyd benjamin lewis,
an assistant operations office at a large bank. he engaged in large-scale
fraud over two years, and during that time never took a single vacation
day, sick day, or was late to work. he had to be there, otherwise the fraud
might be discovered. 
it’s not enough to have a good system administration staff who knows
all about computer and network security, monitors the systems, and
responds to attacks 24 hours a day. someone has to watch them. it’s not
just because they might be malicious, although this has happened. (there
is a long history of crimes committed by senior bank officials, since they’re
security processes 379
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 379


 
 

____________________________________________

SecretsAndLies.pdf page: 408

____________________________________________

the ones most likely to get away with it. someone who was charged with
auditing slot machines, making sure they were not rigged by the house,
was caught modifying the roms so he could force them to produce a
jackpot at will.) the real reason is that people are people; they make mis-
takes. even processes have security flaws, and there has to be another
process in place to catch and fix them. it’s a lesson that has long needed to
be applied to cyberspace.
recover from attacks
when a french banking smart card was broken in 2000, they had a prob-
lem. there was nothing they could do about it except turn the system off
or live with the problem. we saw similar problems in the new york city
transit farecards, a canadian cash card, and the dvd encryption scheme.
if you spend all your time thinking about preventive countermeasures,
you can forget to plan what to do if those countermeasures fail.
preventive countermeasures fail all the time. fixing the problem and
tracking down the bad guys are part of a good response, but so is recov-
ering from a compromise. this can mean designing systems so that they
can be upgraded in the field, and building processes to facilitate that
upgrade. this can mean building systems with emergency cryptography,
emergency protocols, or emergency procedures. this can mean cutting
your losses and returning your system to a secure state.
i’ve seen too many security systems with the implicit assumption: “if
someone breaks the security, we all go home and get new jobs.” that just
doesn’t cut it. compromise recovery should be a core element of any
security system. 
counterattack
in the war for security, it sometimes looks pretty bleak. attackers have it
easier. they can cheat. they can invent new science and new technology
to attack systems already in place. they can use techniques the defenders
never considered. they don’t have to follow the defender’s threat model.
and the odds are in their favor. the defender occupies what karl von
clausewitz calls “the position of the interior.” an attacker needs to find
one successful attack: one minor vulnerability that the defender forgot to
380 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 380


 
 

____________________________________________

SecretsAndLies.pdf page: 409

____________________________________________

close. a defender, on the other hand, needs to protect against every pos-
sible attack. he needs to think of everything; he can’t afford to miss one.
and the defenders are in disarray. they make stupid mistakes. they
write buggy code. they don’t install security upgrades and patches. they
have near theological beliefs about the security of products. they don’t
understand the real threats against themselves, and they don’t protect
themselves accordingly.
time is on the attackers’ side. systems have to go on working, day in
and day out. attackers can sit and wait, looking for a vulnerability, wait-
ing for the defenders to drop their guard, changing strategies and tactics to
suit the situation.
one solution is to go on the offensive.
we don’t fight crime by making our banks 100 percent immune to
attack; we fight crime by catching criminals. luckily, criminals are pretty
stupid. and given the kind of salary a good computer security expert can
command, computer crime doesn’t pay nearly as well.
if the united states was ever the target of a nuclear attack by the
ussr, the planned response was to counterattack. mutual assured
destruction is about as surreal as a security defense gets, but it worked.
the pinkerton detective agency was established in 1852. one of
their early services was to protect trains from robbers in the american
west. early on, they realized that it was expensive to put a pinkerton
guard on every train. they also realized that robbing a train was a com-
plicated operation—you needed an insider who knew the schedule, a
dozen or so people, horses, pack animals, and so forth—and that only a
few criminals were capable of pulling it off. so they decided to go after
the train robbers directly. it didn’t matter if the railroad paid for the pur-
suit; the pinkertons did it because catching train robbers made all of their
customers more secure.
the pinkertons were known for not giving up. if you robbed a
pinkerton-protected train, they would hunt you down. and they were
serious; there were gun battles against the hole in the wall gang that
involved hundreds of pinkerton men. there’s a scene in butch cassidy and
the sundance kid where they’re being chased after a train robbery by a
group who just will not give up. “who are these guys?” butch says to
sundance. they were the pinkertons.
cyberspace needs a few good counterattacks like this. today’s situa-
tion is a kind of prisoner’s dilemma for hacking: if you don’t face conse-
security processes 381
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 381


 
 

____________________________________________

SecretsAndLies.pdf page: 410

____________________________________________

quences for your actions, it’s in your best interest to beat the system.
breaking into networks is not a game; it’s a crime. stealing money by
hacking a digital payment system is a crime. distributing copyrighted
material on the internet is a crime. and criminals should be prosecuted.
this prosecution does two things. one, the convicted criminal is less
likely to do it again. and two, everyone else is less likely to do it in the
first place.
this is not meant to be a call for the vigilante-like “justice” we’ve
seen out of the fbi and others over the past decade. in the 1980s, they
knew little about computers and networks and computer crimes. every-
thing was potentially dangerous, and everything was investigated haphaz-
ardly. in 1989, when the macintosh rom source code was stolen and
broadcast on the internet by the nuprometheus league, the fbi investi-
gated dozens of completely random computer people. in 1990, the secret
service raided the headquarters of a role-playing game company, steve
jackson games, because the company was working on a role-playing
game (not even a computer program) that had something to do with
“cyberpunks” and hackers, and because they believed an employee, loyd
blankenship, was a member of the “legion of doom” hacker group. in
1999, the dvd copy control association tried to gag 500 web sites
whose only crime was writing about the dvd encryption break. and in
2000, microsoft tried to force slashdot to delete postings about its propri-
etary extensions to the kerberos protocol.
this is also not meant to be a call for overreaction, which we saw a
lot of in the 1990s. david smith, the author of the melissa virus, faces five
to ten years in prison. kevin mitnick got (and served) almost five years,
and was prohibited from using a computer for another three. (all his skills
are related to computers, and he has been prohibited from lecturing on
the subject. supposedly, his parole officer suggested he get a job at
arby’s.) kevin poulson received almost the same sentence. the chinese
government sentenced a hacker to death for hacking a bank computer
and stealing $87,000. (to be fair, all bank robbers get the death penalty in
china.) i am reminded that in the american west in the 1800s, horse
thieving was often punished by hanging. this is because the society
wanted to send a clear message that stealing horses was not to be tolerated.
various european governments sent a similar message in the 1970s when
they started gunning terrorists down in the streets. the message was a
very clear: “we’re not playing games anymore.” some of the overreac-
382 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 382


 
 

____________________________________________

SecretsAndLies.pdf page: 414

____________________________________________

damage, and you want the insurance company to compensate you for
your loss. but the second type is even more important: someone breaks
into your network and wreaks havoc with your customers, their propri-
etary information, and their reputations. the third-party liability can be
huge. not only is it a breach of fiduciary responsibility, but the resulting
lawsuits could easily exceed the net worth of the attacked company. a
warranty-type of insurance to deal with this kind of threat is critical.
risk management is the future of digital security. whoever learns
how to best manage risk is the one who will win. insurance is one critical
component of this. technical solutions to mitigate risk to the point where
it is insurable is another.
outsourcing security processes
security processes are a way of mitigating the risks. network security
products will have flaws; a process is necessary both to catch attackers
exploiting those flaws, and fixing the flaws once they become known.
insider attacks will occur; a process is necessary to detect the attack, repair
the damage, and prosecute the attacker. large systemwide flaws will com-
promise entire products and services (think cell phones, think dvd); a
process is necessary to recover from the compromise and stay in business.
counterintelligence is the only way to stay abreast of what’s really going
on. insurance will handle the residual risk.
none of this is easy, and it all requires experts. and as more and more
aspects of our lives move into cyberspace, the demand for cyberspace
security (and hence the demand for these experts) increases. the only
workable solution is to leverage these experts as much as possible. out-
sourcing is the only way to do this efficiently.
think about a security-monitoring center for a large network. it takes
five trained security analysts to man a single 24x7 seat; a concerted attack
can require the attention of half a dozen analysts. a single organization
can’t afford to hire all those people for the few events they’re needed; an
outsourced service can deploy those people when needed. an outsourced
service can train those analysts, both in the classroom and through expe-
rience. an outsourced service can actively test new security countermea-
sures, analyze new intrusion tools, and stay abreast of hacking techniques
and product vulnerabilities. and an outsourced service can see large
386 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 386


 
 

____________________________________________

SecretsAndLies.pdf page: 417

____________________________________________

25
conclusion
mark loizeaux is president of controlled demolitions; he
blows up buildings for a living. complaining about the inep-
titude of modern terrorists, he’s quoted in the july 1997
harper’s magazine as saying: “we could drop every bridge in the united
states in a couple of days. . . . i could drive a truck on the verrazano nar-
rows bridge and have a dirt bike on the back, drop that bridge, and i
would get away. they would never stop me.”
as technology becomes more complicated, society’s experts become
more specialized. and in almost every area, those with the expertise to
build society’s infrastructure also have the expertise to destroy it.
ask any doctor how to poison someone untraceably, and he can tell
you. ask someone who works in aircraft maintenance how to drop a 747
out of the sky without getting caught, and he’ll know. now ask any
internet security professional how to take down the internet, perma-
nently. i’ve heard about half a dozen different ways, and i know i haven’t
exhausted the possibilities. 
the knowledge is there; the systems are vulnerable. all it takes is
someone with just the right combination of skill and morals. sometimes
it doesn’t even take that much skill. timothy mcveigh did quite a num-
ber on the oklahoma city federal building, even though his banausic use
of explosives probably disgusted a professional like loizeaux. dr. harold
shipman murdered possibly as many as 150 of his patients, using artless
techniques like injecting them with morphine.
at first glance cyberspace is no different from any other piece of our
389
453803_ch25.qxd:453803_ch25.qxd  4/15/13  9:32 am  page 389


 
 

____________________________________________

SecretsAndLies.pdf page: 418

____________________________________________

society’s infrastructure: fragile and vulnerable. but as i argued in chapter
2, the nature of the attacks is very different. mcveigh had to acquire the
knowledge, go to a private farm and practice, rent the truck, fill it with
explosives, drive to the federal building, set the fuse, and get away. dr.
shipman had to build a medical practice and meet his patients; our hypo-
thetical aircraft maintainer had to work on the planes. they all had to get
close to their target, put themselves at risk, get in, get away, make mis-
takes. and they had to know what they were doing.
or think of nuclear proliferation. when the knowledge for manufac-
turing nuclear bombs became accessible by the public, there was still no
large-scale proliferation of nuclear munitions. why? because the knowl-
edge about how wasn’t the critical barrier, it was the vast resources and
unwieldy engineering programs that only a handful of countries could
assemble.
cyberspace is different. you can be elsewhere, far away from the site
you are attacking. you can have no skill, nothing more than a software
package you downloaded from some web site somewhere. and you
don’t even have to put yourself at risk. an ethical hacker could describe a
vulnerability on the internet, a criminal hacker with fewer ethics could
write an exploit that demonstrates the vulnerability, and then someone
with no skill or ethics could use it to break into computers. a philippine
student could write a worm that infects ten million computers, and costs
$10 billion in damage, time, and lost productivity. or maybe there’s a
web site in some badly policed third world country that includes a java
application: “click here to bring down the internet.” it’s not a pretty
thought.
in the late nineteenth century, french sociologist emile durkheim
postulated that anomie led people to become criminals. you can extend
his arguments to the hacker psychology we’re seeing now: no one is con-
nected to anyone else, people feel anonymous behind their handles, and
there are no repercussions to actions; this leads some people to do antiso-
cial things. the miasma of the internet virtually guarantees it.
technology alone cannot prevent this, just as it could not prevent
mcveigh or shipman. both of them were captured (and others were dis-
suaded) by security processes: detection and response. (in the case of ship-
man, the detection and response processes were egregious, and he got
away with his massacre for decades.) forensics techniques figured out
390 c h a p t e r  t w e n t y - f i v e
453803_ch25.qxd:453803_ch25.qxd  4/15/13  9:32 am  page 390


 
 

____________________________________________

SecretsAndLies.pdf page: 422

____________________________________________

also possible is an underwriters laboratory model for cyberspace
security. underwriters laboratory is a private lab that tests and certifies
electrical equipment. (they also provide ratings for safes.) consumer
reports does a similar service for other products. a private company can
do the same for computer and network security, but the costs quickly
become exorbitant. and new laws in the united states are moving in the
opposite direction, making it illegal for companies and individuals to eval-
uate the security of products.
still another model is licensing, like medical doctors and registered
nurses. engineers who are certified and have liability insurance can put
“pe” after their name. but certifications are local, and the internet is
global. and still there is no guarantee.
all this seems to leave us in a quandary. we need technological solu-
tions, but they’re not perfect. we need experts to build, configure, and
manage these technological solutions, but there aren’t enough experts to
go around. we need strong laws to prosecute criminals and a willingness
to do so, but most companies who are attacked don’t want to go public.
in chapter 24, i argued that the only way to maintain security in the
face of the technological limitations is to build security processes. and that
these security processes are not reasonable to build inside an organization,
and will most likely be outsourced to cyberspace security professionals.
this seems to be the only way out of the previous paragraph’s bind as
well.
assuming you can trust the outsourcing organization.
in my first book, applied cryptography, i wrote: “encryption is too
important to be left solely to the government.” i still believe that, but in
a more general sense. security is too important to be left solely to any
organization. and it is too personal to be left to an arbitrary organization.
trust is personal. one person might trust the government com-
pletely, while another might not trust the government at all. different
people might trust different governments. some people might trust dif-
ferent corporations, but no governments. it is impossible to design a secu-
rity system (product or process) that is devoid of trust; even the person
who writes his own security software has to trust his compiler and com-
puter.
unfortunately, most organizations don’t realize whom they trust.
some might blindly trust companies for no good reason. (witness the
394 c h a p t e r  t w e n t y - f i v e
453803_ch25.qxd:453803_ch25.qxd  4/15/13  9:32 am  page 394


 
 

____________________________________________

SecretsAndLies.pdf page: 425

____________________________________________

i came to security from cryptography, and thought of the problem in
a military-like fashion. most writings about security come from this
perspective, and it can be summed up pretty easily: security threats are to
be avoided using preventive countermeasures.
this is how encryption works. the threat is eavesdropping, and
encryption provides the prophylactic. this could all be explained with
block diagrams. alice is communicating with bob; both are identified by
boxes, and there is a line between them signifying the communication.
eve is the eavesdropper; she also is a box and has a dotted line attached to
the communications line. she is able to intercept the communication.
the only way to prevent eve from learning what alice and bob are talk-
ing about is through a preventive countermeasure: encryption. there’s no
detection. there’s no response. there’s no risk management; you have to
avoid the threat.
for decades we have used this approach to computer security. we
draw boxes around the different players and lines between them. we
define different attackers—eavesdroppers, impersonators, thieves—and
their capabilities. we use preventive countermeasures like encryption and
access control to avoid different threats. if we can avoid the threats, we’ve
won. if we can’t, we’ve lost.
imagine my surprise when i learned that the world doesn’t work this
way. i had my epiphany in april 1999: that security was about risk man-
agement, that the process of security was paramount, that detection and
response was the real way to improve security, and that outsourcing was
the only way to make this happen effectively. it suddenly all made sense.
so i rewrote this book and reformed my company. counterpane systems
is now counterpane internet security, inc. we provide managed secu-
rity monitoring services—detection and response—for networks.
in the world of alice and bob and eve, that answer made no sense.
when the model was invented, communication was over radio or long
wires. detection isn’t possible. response isn’t possible. but in today’s
electronic world, it’s a lot more complicated. an attacker doesn’t passively
monitor a communication. he breaks into a firewall. he tries to steal
money using a forged smart card. he manipulates a digital network.
today’s world is much more like the physical world, with all its potential
for rich interaction.
and it’s not all or nothing. if eve could eavesdrop, she could eaves-
drop on everything. if she could not eavesdrop, she could not eavesdrop
afterword 397
453803_afterword.qxd:453803_afterword.qxd  4/12/13  11:08 am  page 397


 
 

____________________________________________

SecretsAndLies.pdf page: 427

____________________________________________

resources
the ideas in this book have been heavily influenced by the ideas
and writings of others. i deliberately did not disrupt the flow of
text with footnotes or citations. what follows is a list of some of
my more useful sources.
all urls are guaranteed accurate as of 1 july 2000. some internet
pundits have decried the web as useless for scholarly archives, claiming
that urls move or disappear regularly. consider this list to be an ongo-
ing experiment to prove or disprove that thesis.
ross anderson’s writing are always interesting and worth reading.
his web site is www.cl.cam.ac.uk/users/rja14/. look for his new book,
coming out next year: security engineering: a comprehensive guide to
building dependable distributed systems (john wiley & sons, 2001).
dorothy denning has written about cryptography, computer and
database security, and (more recently) information warfare. i used her
most recent book, information warfare and security (addison-wesley,
1999), as well has her classic cryptography and data security (addison-wes-
ley, 1982).
whit diffie’s writings and speeches have affected my thinking. i
recommend the book he co-wrote with susan landau: privacy on the line
(mit press, 1998).
carl ellison has continued to write common-sense essays and papers
on public-key infrastructure. much of his writing can be found on his
web site, world.std.com/~cme/.
ed felton has spoken on the insecurities inherent in software modu-
larity, and on java security. i always learn something when i hear him. i
first saw the figures on page 160 in one of his talks.
dan geer’s speeches have been similarly educational.
399
453803_resources.qxd:453803_resources.qxd  4/12/13  11:15 am  page 399


 
 

____________________________________________

SecretsAndLies.pdf page: 428

____________________________________________

dieter gollmann’s excellent text, computer security (john wiley &
sons, 1999), was a very useful resource.
david kahn’s classic book the codebreakers provided invaluable
historic background on the subject of cryptography.
stuart mcclure, joel scrambray, and george kurtz wrote hacking
exposed (osborne/mcgraw-hill, 1999), which i strongly recommend. i
wrote the foreword to the second edition, which should be available by
the time this book is published.
gary mcgraw has written extensively about secure software engi-
neering, as well as the pros and cons of open source software. i used his
book, securing java (john wiley & sons, 1999), written with ed felton.
peter neumann’s observations on computer security are so profound
and obvious that i often forget that i didn’t always believe him. his back-
page column, “inside risks,” running for the past ten years in communi-
cations of the acm, is always interesting. i strongly recommend his book
computer-related risks (addison-wesley, 1995) and the internet risks
forum mailing list he moderates.
marcus ranum’s essays, speeches, and dinnertime banter have
long been a source of inspiration and common sense. i strongly recom-
mend reading everything he’s written. his web site is at
http://pubweb.nfr.net/~mir/.
avi ruvin, dan geer, and marcus ranum co-wrote the web secu-
rity sourcebook (john wiley & sons, 1997), which i recommend highly.
winn schwartau’s time based security (interpact press, 1999), con-
tains ideas very similar to my own on the importance of detection and
response in computer security.
diomidis spinellis provided the data on complexity of operating sys-
tems and programming languages on pages 357 and 358 in his article
“software reliability: modern challenges” (in g. i. schuëller and p.
kafka, editors, proceedings esrel ’99—the tenth european conference on
safety and reliability, pages 589–592, munich-garching, germany, sep-
tember 1999).
richard thieme’s musings on hacking and the epistemology of the
internet have long been a source of inspiration. the comment about the
dead marine and mogadishu was from one of his stories. you can find his
writings at www.thiemeworks.com.
hundreds of essays, articles, and papers are published each year on
computer security. i feel as if i’ve read them all, and undoubtedly
thoughts, ideas, ruminations, nuances, and clever one-liners from my
readings have crept into this book. i apologize for not giving everyone the
credit they deserve.
400 resources
453803_resources.qxd:453803_resources.qxd  4/12/13  11:15 am  page 400


 
 

____________________________________________

Absolute.pdf page: 10

____________________________________________

viii     preface
 acknowledgments 
 numerous individuals have contributed invaluable help and support to making this 
book happen. frank ruggirello and susan hartman at addison-wesley first conceived 
the idea and supported the first edition, for which i owe them a debt of gratitude. 
a special thanks to matt goldstein who was the editor for the second, third, and 
fourth editions. his help and support were critical to making this project succeed. 
chelsea kharakozova, marilyn lloyd, yez alayan, and the other fine people at pearson 
education also provided valuable support and encouragement. 
 the following reviewers provided suggestions for the book. i thank them all for 
their hard work and helpful comments. 
 richard albright university of delaware 
 j. boyd trolinger butte college 
 jerry k. bilbrey, jr francis marion university 
 albert m. k. cheng university of houston 
 david cherba michigan state university 
 fredrick h. colclough colorado technical university 
 drue coles boston university 
 stephen corbesero moravian college 
 christopher e. cramer 
 ron dinapoli cornell university 
 qin ding pennsylvania state university, harrisburg 
 martin dulberg north carolina state university 
 h. e. dunsmore purdue university 
 evan golub university of maryland 
 terry harvey university of delaware 
 joanna klukowska hunter college, cuny 
 lawrence s. kroll san francisco state university 
 stephen p. leach florida state university 
 alvin s. lim auburn university 
 tim h. lin cal poly pomona 
 r. m. lowe clemson university 
 jeffrey l. popyack drexel university 
 amar raheja cal poly pomona 
 victoria rayskin university of central los angeles 
 loren rhodes juniata college 
 jeff ringenberg university of michigan 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 11

____________________________________________

preface      ix
 victor shtern boston university 
 aaron striegel university of notre dame 
 j. boyd trolinger butte college 
 chrysafis vogiatzis university of florida 
 joel weinstein northeastern university 
 dick whalen college of southern maryland 
 a special thanks goes to kenrick mock (university of alaska anchorage) who 
executed the updating of this edition. he once again had the difficult job of satisfying 
me, the editor, and himself. i thank him for a truly excellent job. 
 walter savitch 


 
 

____________________________________________

Absolute.pdf page: 33

____________________________________________

introduction to c++      3
 the c language is peculiar because it is a high-level language with many of the 
features of a low-level language. c is somewhere in between the two extremes of a very 
high-level language and a low-level language, and therein lies both its strengths and 
its weaknesses. like (low-level) assembly language, c language programs can directly 
manipulate the computer’s memory. on the other hand, c has the features of a high-
level language, which makes it easier to read and write than assembly language. this 
makes c an excellent choice for writing systems programs, but for other programs 
(and in some sense even for systems programs) c is not as easy to understand as other 
languages; also, it does not have as many automatic checks as some other high-level 
languages.
 to overcome these and other shortcomings of c, bjarne stroustrup of at&t bell 
laboratories developed c++ in the early 1980s. stroustrup designed c++ to be a better 
c. most of c is a subset of c++ and so most c programs are also c++ programs. (the 
reverse is not true; many c++ programs are definitely not c programs.) unlike c, c++ 
has facilities for classes and so can be used for object-oriented programming. 
 c++ and object-oriented programming 
 object-oriented programming (oop) is a currently popular and powerful programming 
technique. the main characteristics of oop are encapsulation, inheritance, and polymorphism. 
encapsulation is a form of information hiding or abstraction. inheritance has to do 
with writing reusable code. polymorphism refers to a way that a single name can have 
multiple meanings in the context of inheritance. having made those statements, we must 
admit that they will hold little meaning for readers who have not heard of oop before. 
however, we will describe all these terms in detail later in this book. c++ accommodates 
oop by providing classes, a kind of data type combining both data and algorithms. 
c++ is not what some authorities would call a “pure oop language.” c++ tempers its 
oop features with concerns for efficiency and what some might call “practicality.” this 
combination has made c++ currently the most widely used oop language, although not 
all of its usage strictly follows the oop philosophy. 
 the character of c++ 
 c++ has classes that allow it to be used as an object-oriented language. it allows for 
overloading of functions and operators. (all these terms will be explained eventually, so 
do not be concerned if you do not fully understand some terms.) c++’s connection to 
the c language gives it a more traditional look than newer object-oriented languages, 
yet it has more powerful abstraction mechanisms than many other currently popular 
languages. c++ has a template facility that allows for full and direct implementation 
of algorithm abstraction. c++ templates allow you to code using parameters for types. 
the newest c++ standard, and most c++ compilers, allow multiple namespaces to 
accommodate more reuse of class and function names. the exception handling facilities 
in c++ are similar to what you would find in other programming languages. memory 
management in c++ is similar to that in c. the programmer must allocate his or her
own memory and handle his or her own garbage collection. most compilers will allow 


 
 

____________________________________________

Absolute.pdf page: 36

____________________________________________

6     chapter 1    c++ basics
 if you have not programmed in c++ before, then the use of  cin and  cout for 
console i/o is likely to be new to you. that topic is covered a little later in this chapter, 
but the general idea can be observed in this sample program. for example, consider the 
following two lines from display  1.1 : 
cout << "how many programming languages have you used? "; 
cin >> numberoflanguages; 
 the first line outputs the text within the quotation marks to the screen. the text inside 
the quotation marks is called a string , or to be more precise, a  c-string . the second 
line reads in a number that the user enters at the keyboard and sets the value of the 
variable numberoflanguages to this number. 
 the lines 
cout << "read the preface. you may prefer\n" 
<< "a more elementary book by the same author.\n"; 
 output two strings instead of just one string. the details are explained in  section  1.3 later 
in this chapter, but this brief introduction will be enough to allow you to understand 
the simple use of cin and  cout in the examples that precede  section  1.3 . the 
symbolism \n is the newline character, which instructs the computer to start a new line 
of output. 
 although you may not yet be certain of the exact details of how to write such 
statements, you can probably guess the meaning of the if-else statement.  the details 
will be explained in the next chapter. 
 (by the way,  if you have not had at least some experience with some programming 
languages, you should read the preface to see if you might prefer a more elementary 
book. you need not have had any experience with c++ to read this book, but some 
minimal programming experience is strongly suggested.) 
 1.2  variables, expressions, and 
assignment statements 
 once a person has understood the way variables are used in 
programming, he has understood the quintessence of programming. 
 e. w. dijkstra ,  notes on structured programming 
 variables, expressions, and assignments in c++ are similar to those in most other 
general-purpose languages. 
string
c-string
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 76

____________________________________________

 “would you tell me, please, which way i ought to go from here?” 
“that depends a good deal on where you want to get to,” said the cat. 
 lewis carroll ,  alice in wonderland 
 introduction 
 as in most programming languages, c++ handles flow of control with branching and 
looping statements. c++ branching and looping statements are similar to branching 
and looping statements in other languages. they are the same as in the c language and 
very similar to what they are in the java programming language. exception handling 
is also a way to handle flow of control. exception handling is covered in chapter  18 . 
 2.1 boolean expressions 
 he who would distinguish the true from the false must have an adequate idea 
of what is true and false. 
 benedict spinoza ,  ethics 
 most branching statements are controlled by boolean expressions. a  boolean
expression is any expression that is either true or false. the simplest form for a 
boolean expression consists of two expressions, such as numbers or variables, which 
are compared with one of the comparison operators shown in  display  2.1 . notice that 
some of the operators are spelled with two symbols, for example, == ,  != ,  <= , or >= . 
be sure to notice that you use a double equal == for the equal sign and that you use the 
two symbols != for not equal. such two-symbol operators should not have any space 
between the two symbols. 
 building boolean expressions  
 you can combine two comparisons using the “and” operator, which is spelled  && in 
c++. for example, the following boolean expression is true provided x is greater than 
2  and  x is less than  7 :  
(2 < x) && (x < 7) 
 when two comparisons are connected using an  && , the entire expression is true, 
provided both of the comparisons are true; otherwise, the entire expression is false. 
2 flow of control
boolean
expression
&& means 
“and”
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 158

____________________________________________

128     chapter 3    function basics
if a function is well designed, the programmer can use the function as if it were a 
black box. all the programmer needs to know is that if he or she puts appropriate 
arguments into the black box, then it will take some appropriate action. designing a 
function so that it can be used as a black box is sometimes called information hiding
to emphasize the fact that the programmer acts as if the body of the function were 
hidden from view. 
 writing and using functions as if they were black boxes is also called  procedural
abstraction . when programming in c++ it might make more sense to call it  functional
abstraction . however,  procedure is a more general term than  function and computer 
scientists use it for all “function-like” sets of instructions, and so they prefer the 
 term  procedural abstraction . the term  abstraction is intended to convey the idea that 
when you use a function as a black box, you are abstracting away the details of the code 
contained in the function body. you can call this technique the black box principle or 
the principle of procedural abstraction or  information hiding . the three terms mean the 
same thing. whatever you call this principle, the important point is that you should 
use it when designing and writing your function definitions. 
 information 
hiding 
 procedural 
abstraction 
 procedural abstraction 
when applied to a function definition, the principle of procedural abstraction means that 
your function should be written so that it can be used like a black box . this means that the 
programmer who uses the function should not need to look at the body of the function 
definition to see how the function works. the function declaration and the accompanying 
comment should be all the programmer needs to know in order to use the function. to 
ensure that your function definitions have this important property, you should strictly adhere 
to the following rules: 
 how to write a black-box function definition 
 ■  the function declaration comment should tell the programmer any and all conditions 
that are required of the arguments to the function and should describe the result of a 
function invocation. 
 ■  all variables used in the function body should be declared in the function body. 
(the formal parameters do not need to be declared, because they are listed in the 
 function heading.) 
 global constants and global variables 
 as we noted in  chapter  1 , you can and should name constant values using the  const
modifier. for example, in  display  3.5 we used the  const modifier to give a name to 
the rate of sales tax with the following declaration: 
 const double taxrate = 0.05; //5%  sales tax 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 169

____________________________________________

programming projects      139
since you will use the previous formula, the gravitational force will be in dynes. 
one dyne equals a 
 g • cm/sec 2 
   you should use a globally defined constant for the universal gravitational constant. 
embed your function definition in a complete program that computes the gravita-
tional force between two objects given suitable inputs. your program should allow 
the user to repeat this calculation as often as the user wishes. 
 5.  write a program that asks for the user’s height, weight, and age, and then computes 
clothing sizes according to the following formulas. 
  •  hat size = weight in pounds divided by height in inches and all that multiplied 
by 2.9. 
   •  jacket size (chest in inches) = height times weight divided by 288 and then 
adjusted by adding one-eighth of an inch for each 10 years over age 30. (note 
that the adjustment only takes place after a full 10 years. so, there is no adjust-
ment for ages 30 through 39, but one-eighth of an inch is added for age 40.) 
   •  waist in inches = weight divided by 5.7 and then adjusted by adding one-tenth 
of an inch for each 2 years over age 28. (note that the adjustment only takes 
place after a full 2 years. so, there is no adjustment for age 29, but one-tenth 
of an inch is added for age 30.) 
   use functions for each calculation. your program should allow the user to repeat 
this calculation as often as he or she wishes. 
  6. write a function that computes the average and standard deviation of four scores. 
the standard deviation is defined to be the square root of the average of the four 
values: (s i  -  a ) 2 , where  a is the average of the four scores s 1 , s 2 , s 3 , and  s4 . the func-
tion will have six parameters and will call two other functions. embed the function 
in a program that allows you to test the function again and again until you tell the 
program you are finished. 
 7.  in cold weather, meteorologists report an index called the  wind chill factor , which 
takes into account the wind speed and the temperature. the index provides a 
measure of the chilling effect of wind at a given air temperature. wind chill may 
be approximated by the following formula, 
   w = 33 -
1102v - v + 10 .52 133 - t2
23 .1
   where 
    v = wind speed in m/sec 
    t = temperature in degrees celsius: t 6= 10 
    w = wind chill index (in degrees celsius) 
   write a function that returns the wind chill index. your code should ensure that 
the restriction on the temperature is not violated. look up some weather reports 
in back issues of a newspaper in your library and compare the wind chill index you 
calculate with the result reported in the newspaper. 


 
 

____________________________________________

Absolute.pdf page: 170

____________________________________________

  8 . write a program that outputs all 99 stanzas of the “ninety-nine bottles of beer 
on the wall” song. your program should print the number of bottles in english, 
not as a number: 
   ninety-nine bottles of beer on the wall, 
   ninety-nine bottles of beer, 
   take one down, pass it around, 
   ninety-eight bottles of beer on the wall. 
   … 
   one bottle of beer on the wall, 
   one bottle of beer, 
   take one down, pass it around, 
   zero bottles of beer on the wall. 
   your program should not use ninety-nine different output statements! 
  9.  in the game of craps, a “pass line” bet proceeds as follows. the first roll of the two, 
six-sided dice in a craps round is called the “come out roll.” the bet immediately 
wins when the come out roll is 7 or 11, and loses when the come out roll is 2, 3, 
or 12. if 4, 5, 6, 8, 9, or 10 is rolled on the come out roll, that number becomes 
“the point.” the player keeps rolling the dice until either 7 or the point is rolled. 
if the point is rolled first, then the player wins the bet. if the player rolls a 7 first, 
then the player loses. 
   write a program that plays craps using those rules so that it simulates a game without 
human input. instead of asking for a wager, the program should  calculate whether 
the player would win or lose. create a function that simulates rolling the two dice 
and returns the sum. add a loop so that the program plays 10,000 games. add 
counters that count how many times the player wins, and how many times the 
player loses. at the end of the 10,000 games, compute the probability of winning, 
as wins / (wins + losses), and output this value. over the long run, who is going 
to win more games of craps, you or the house? 
 10.  one way to estimate the height of a child is to use the following formula, which 
uses the height of the parents: 
   hmale_child = (( hmother 13>12) + hfather )>2
   hfemale_child = (( hfather 12>13) + hmother )>2
   all heights are in inches. write a function that takes as input parameters the gen-
der of the child, height of the mother in inches, and height of the father in inches, 
and outputs the estimated height of the child in inches. embed your function in a 
program that allows you to test the function over and over again until telling the 
program to exit. the user should be able to input the heights in feet and inches, 
and the program should output the estimated height of the child in feet and inches. 
use the integer data type to store the heights. 
140     chapter 3    function basics
solution to 
programming
project 3.9 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 171

____________________________________________

programming projects      141
 11.  the game of pig is a simple two player dice game in which the first player to 
reach 100 or more points wins. players take turns. on each turn a player rolls a 
six-sided die: 
   •  if the player rolls a 2–6 then he or she can either 
    — roll again or 
    —  hold. at this point the sum of all rolls made this turn is added to the 
player’s total score and it becomes the other player’s turn. 
   •  if the player rolls a 1 then the player loses his or her turn. the player gets no 
new points and it becomes the opponent’s turn. 
   if a player reaches 100 or more points after holding then the player wins. 
   write a program that plays the game of pig, where one player is a human and the 
other is the computer. allow the human to input “r” to roll again or “h” to hold. 
   the computer program should play according to the following rule: keep rolling 
on the computer’s turn until it has accumulated 20 or more points, then hold. of 
course, if the computer wins or rolls a 1 then the turn ends immediately. allow the 
human to roll first. 
   write your program using at least two functions: 
   int humanturn( int humantotalscore); 
   int computerturn( int computertotalscore); 
   these functions should perform the necessary logic to handle a single turn for 
either the computer or the human. the input parameter is the total score for the 
human or computer. the functions should return the turn total to be added to the 
total score upon completion of the turn. for example, if the human rolls a 3 and 
6 and then holds, then humanturn should return 9. however, if the human rolls a 
3 and 6 and then a 1, then the function should return 0. 
 12.  write a program that inputs a date (e.g., july 4, 2008) and outputs the day of 
the week that corresponds to that date. the following algorithm is from http://
en.wikipedia.org/wiki/calculating_the_day_of_the_week. the implementation 
will require several functions: 
   bool isleapyear(int year);
   this function should return  true if  year is a leap year and  false if it is not. here 
is pseudocode to determine a leap year: 
    leap_year = ((year divisible by 400) or (year divisible by 4 and year not divisible 
by 100)) 
   int getcenturyvalue(int year); 
   this function should take the first two digits of the year (i.e., the century),  divide 
by 4, and save the remainder. subtract the remainder from 3 and return this 
value multiplied by 2. for example, the year 2008 becomes (20/4) = 5 remainder 
0. 3 - 0 = 3. return 3 * 2 = 6. 
   int getyearvalue(int year); 


 
 

____________________________________________

Absolute.pdf page: 189

____________________________________________

parameters      159
 6 void swapvalues(int variable1, int variable2); 
 7 //interchanges the values of variable1 and variable2 . 
 8 void showresults(int output1, int output2); 
 9 //shows the values of variable1 and variable2, in that order . 
 10 int main( ) 
 11  { 
 12 int firstnum, secondnum; 
 13    getnumbers(firstnum, secondnum); 
 14    swapvalues(firstnum, secondnum); 
 15    showresults(firstnum, secondnum); 
 16 return 0; 
 17  } 
 18 void swapvalues(int variable1, int variable2) 
 19  { 
 20 int temp; 
 21      temp = variable1; 
 22      variable1 = variable2; 
 23      variable2 = temp; 
 24  } 
 25 the definitions of  getnumbers and 
 26 showresults are the same as in  display  4.2 . 
 sample dialogue 
enter two integers: 5 6 
in reverse order the numbers are: 5 6 
display 4.4 inadvertent local variable (part 2 of 2)
forgot the & here
inadvertent 
local variables
error due to 
inadvertent local 
variables
forgot the & here
 tip: choosing formal parameter names 
 functions should be self-contained modules that are designed separately from the 
rest of the program. on large programming projects, different programmers may 
be assigned to write different functions. the programmer should choose the most 
meaningful names he or she can find for formal parameters. the arguments that will 
be substituted for the formal parameters may well be variables in another function or 
in the main function. these variables should also be given meaningful names, often 
chosen by someone other than the programmer who writes the function definition. 
this makes it likely that some or all arguments will have the same names as some of 
the formal parameters. this is perfectly acceptable. no matter what names are chosen 
for the variables that will be used as arguments, these names will not produce any 
confusion with the names used for formal parameters. ■


 
 

____________________________________________

Absolute.pdf page: 209

____________________________________________

programming projects      179
 14.  //this is just a stub. 
   double rainprob( double pressure, 
           double humidity, double temp) 
   {
       return 0.25; //not correct,
                    //but good enough for some testing.
   }
 programming projects 
visit www.myprogramminglab.com to complete select exercises online and get instant 
feedback.
 1.  write a program that converts from 24-hour notation to 12-hour notation. for 
example, it should convert 14:25 to 2:25 p.m. the input is given as two integers. 
there should be at least three functions: one for input, one to do the conversion, 
and one for output. record the a.m./p.m. information as a value of type char , 
'a' for a.m. and  'p' for p.m. thus, the function for doing the conversions will 
have a call-by-reference formal parameter of type  char to record whether it is a.m. 
or p.m. (the function will have other parameters as well.) include a loop that lets 
the user repeat this computation for new input values again and again until the user 
says he or she wants to end the program. 
 2.  the area of an arbitrary triangle can be computed using the formula 
area = 2s1s - a2 1s - b2 1s - c2
   where a, b, and c are the lengths of the sides, and s is the semiperimeter. 
s = 1a + b + c2 >2 
   write a  void function that uses five parameters: three value parameters that pro-
vide the lengths of the edges, and two reference parameters that compute the area 
and perimeter ( not the semiperimeter ). make your function robust. note that not 
all combinations of a, b, and c produce a triangle. your function should produce 
correct results for legal data and reasonable results for illegal combinations. 
 3.  write a program that tells what coins to give out for any amount of change from 
1 cent to 99 cents. for example, if the amount is 86 cents, the output would be 
something like the following: 
   86 cents can be given as 
   3 quarter(s) 1 dime(s) and 1 penny(pennies) 
   use coin denominations of 25 cents (quarters), 10 cents (dimes), and 1 cent (pen-
nies). do not use nickel and half-dollar coins. your program will use the following 
function (among others): 
void computecoin( int coinvalue, int& number, int& amountleft); 
   //precondition: 0 < coinvalue < 100; 0 <= amountleft < 100. 
   //postcondition: number has been set equal to the maximum number 
   //of coins of denomination coinvalue cents that can be obtained 


 
 

____________________________________________

Absolute.pdf page: 210

____________________________________________

   //from amountleft cents. amountleft has been decreased by the 
   //value of the coins, that is, decreased by number*coinvalue .
   for example, suppose the value of the variable  amountleft is  86 . then, after the 
following call, the value of number will be  3 and the value of  amountleft will be  11
(because if you take three quarters from 86 cents, that leaves 11 cents): 
   computecoins(25, number, amountleft); 
   include a loop that lets the user repeat this computation for new input values until 
the user says he or she wants to end the program. ( hint: use integer division and 
the % operator to implement this function.) 
 4.  write a program that will read in a length in feet and inches and output the equiva-
lent length in meters and centimeters. use at least three functions: one for input, 
one or more for calculating, and one for output. include a loop that lets the user 
repeat this computation for new input values until the user says he or she wants to 
end the program. there are 0.3048 meters in a foot, 100 centimeters in a meter, 
and 12 inches in a foot. 
 5.  write a program like that of the previous exercise that converts from meters and 
centimeters into feet and inches. use functions for the subtasks. 
 6.  (you should do the previous two programming projects before doing this one.) 
write a program that combines the functions in the previous two programming 
projects. the program asks the user if he or she wants to convert from feet and 
inches to meters and centimeters or from meters and centimeters to feet and inches. 
the program then performs the desired conversion. have the user respond by typ-
ing the integer 1 for one type of conversion and  2 for the other conversion. the 
program reads the user’s answer and then executes an if-else statement. each 
branch of the if-else statement will be a function call. the two functions called 
in the if-else statement will have function definitions that are very similar to 
the programs for the previous two programming projects. thus, they will be fairly 
complicated function definitions that call other functions. include a loop that lets 
the user repeat this computation for new input values until the user says he or she 
wants to end the program. 
 7.  write a program that will read in a weight in pounds and ounces and will output 
the equivalent weight in kilograms and grams. use at least three functions: one for 
input, one or more for calculating, and one for output. include a loop that lets 
the user repeat this computation for new input values until the user says he or she 
wants to end the program. there are 2.2046 pounds in a kilogram, 1000 grams in 
a kilogram, and 16 ounces in a pound. 
 8.  write a program like that of the previous exercise that converts from kilograms and 
grams into pounds and ounces. use functions for the subtasks. 
 9.  (you should do the previous two programming projects before doing this one.) 
write a program that combines the functions of the previous two programming 
projects. the program asks the user if he or she wants to convert from pounds 
and ounces to kilograms and grams or from kilograms and grams to pounds and 
ounces. the program then performs the desired conversion. have the user respond 
by typing the integer 1 for one type of conversion and  2 for the other. the program 
180     chapter 4    parameters and overloading
solution to 
programming
project 4.4 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 211

____________________________________________

programming projects      181
reads the user’s answer and then executes an if-else statement. each branch of 
the if-else statement will be a function call. the two functions called in the 
if-else statement will have function definitions that are very similar to the programs 
for the previous two programming projects. thus, they will be fairly complicated 
function definitions that call other functions in their function bodies. include a 
loop that lets the user repeat this computation for new input values until the user 
says he or she wants to end the program. 
 10.  (you should do programming projects 4.6 and 4.9 before doing this program-
ming project.) write a program that combines the functions of programming 
projects 4.6 and 4.9. the program asks the user if he or she wants to convert 
lengths or weights. if the user chooses lengths, then the program asks the user if 
he or she wants to convert from feet and inches to meters and centimeters or from 
meters and centimeters to feet and inches. if the user chooses weights, a similar 
question about pounds, ounces, kilograms, and grams is asked. the program then 
performs the desired conversion. have the user respond by typing the integer 1 for 
one type of conversion and 2 for the other. the program reads the user’s answer 
and then executes an if-else statement. each branch of the  if-else statement 
will be a function call. the two functions called in the if-else statement will 
have function definitions that are very similar to the programs for programming 
projects 4.6 and 4.9. thus, these functions will be fairly complicated func-
tion definitions that call other functions; however, they will be very easy to 
write by adapting the programs you wrote for programming projects 4.6 and 
4.9. notice that your program will have if-else statements embedded inside of 
if-else statements, but only in an indirect way. the outer  if-else statement 
will include two function calls, as its two branches. these two function calls will 
each in turn include an if-else statement, but you need not think about that. 
they are just function calls and the details are in a black box that you create when 
you define these functions. if you try to create a four-way branch, you are probably 
on the wrong track. you should only need to think about two-way branches (even 
though the entire program does ultimately branch into four cases). include a loop 
that lets the user repeat this computation for new input values until the user says 
he or she wants to end the program. 
 11.  you are a contestant on a game show and have won a shot at the grand prize. 
before you are three doors. $1,000,000 in cash has randomly been placed behind 
one door. behind the other two doors are the consolation prizes of dishwasher 
 detergent. the game show host asks you to select a door, and you randomly pick 
one. however, before revealing the prize behind your door, the game show host 
reveals one of the other doors that contains a consolation prize. at this point, the 
game show host asks if you would like to stick with your original choice or to 
switch to the remaining door. 
   write a function to simulate the game show problem. your function should randomly 
select locations for the prizes, select a door at random chosen by the contestant, 
and then determine whether the contestant would win or lose by sticking with 
solution to 
programming
project 4.11 
videonote


 
 

____________________________________________

Absolute.pdf page: 212

____________________________________________

the original choice or switching to the remaining door. you may wish to create 
 additional functions invoked by this function. 
   next, modify your program so that it simulates playing 10,000 games. count the 
number of times the contestant wins when switching versus staying. if you are 
the contestant, what choice should you make to optimize your chances of winning 
the cash, or does it not matter? 
 12.  in the land of puzzlevania, aaron, bob, and charlie had an argument over which 
one of them was the greatest puzzle-solver of all time. to end the argument once 
and for all, they agreed on a duel to the death. aaron was a poor shot and only hit 
his target with a probability of 1/3. bob was a bit better and hit his target with a 
probability of 1/2. charlie was an expert marksman and never missed. a hit means 
a kill and the person hit drops out of the duel. 
   to compensate for the inequities in their marksmanship skills, the three decided 
that they would fire in turns, starting with aaron, followed by bob, and then by 
charlie. the cycle would repeat until there was one man standing. that man 
would be remembered for all time as the greatest puzzle-solver of all time. 
   an obvious and reasonable strategy is for each man to shoot at the most accurate 
shooter still alive, on the grounds that this shooter is the deadliest and has the best 
chance of hitting back. 
   write a program to simulate the duel using this strategy. your program should use 
random numbers and the probabilities given in the problem to determine whether 
a shooter hits his target. you will likely want to create multiple subroutines and 
functions to complete the problem. once you can simulate a duel, add a loop to 
your program that simulates 10,000 duels. count the number of times that each 
contestant wins and print the probability of winning for each contestant (e.g., for 
aaron your program might output “aaron won 3595/10,000 duels or 35.95%”). 
   an alternate strategy is for aaron to intentionally miss on his first shot. modify the 
program to accommodate this new strategy and output the probability of winning 
for each contestant. what strategy is better for aaron, to intentionally miss on the 
first shot or to try and hit the best shooter? 
 13.  you would like to know how fast you can run in miles per hour. your treadmill 
will tell you your speed in terms of a pace (minutes and seconds per mile, such as 
“5:30 mile”) or in terms of kilometers per hour (kph). 
   write an overloaded function called  converttomph . the first definition should 
take as input two integers that represent the pace in minutes and seconds per mile 
and return the speed in mph as a double. the second definition should take as 
input one double that represents the speed in kph and return the speed in mph as 
a double. one mile is approximately 1.61 kilometers. write a driver program to 
test your function. 
 14.  your time machine is capable of going forward in time up to 24 hours. the machine 
is configured to jump ahead in minutes. to enter the proper number of minutes 
into your machine, you would like a program that can take a start time and an 
end time and calculate the difference in minutes between them. the end time will 
182     chapter 4    parameters and overloading
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 261

____________________________________________

programming projects      231
   after you have completed the previous program, produce an enhanced version 
that also outputs a graph showing the average rainfall and the actual rainfall for 
each of the previous 12 months. the graph should be similar to the one shown in 
 display  5.4 , except that there should be two bar graphs for each month and they 
should be labeled as the average rainfall and the rainfall for the most recent month. 
your program should ask the user whether he or she wants to see the table or the 
bar graph, and then should display whichever format is requested. include a loop 
that allows the user to see either format as often as the user wishes until the user 
requests that the program end. 
 2.  write a function called  deleterepeats that has a partially filled array of characters 
as a formal parameter and that deletes all repeated letters from the array. since a 
partially filled array requires two arguments, the function will actually have two 
formal parameters: an array parameter and a formal parameter of type int that 
gives the number of array positions used. when a letter is deleted, the remaining 
letters are moved forward to fill in the gap. this will create empty positions at 
the end of the array so that less of the array is used. since the formal parameter is 
a partially filled array, a second formal parameter of type int will tell how many 
array positions are filled. this second formal parameter will be a call-by-reference 
parameter and will be changed to show how much of the array is used after the 
repeated letters are deleted. for example, consider the following code: 
   char a[10]; 
   a[0] = 'a'; 
   a[1] = 'b'; 
   a[2] = 'a'; 
   a[3] = 'c'; 
   int size = 4; 
   deleterepeats(a, size); 
 after this code is executed, the value of  a[0] is  'a' , the value of  a[1] is  'b' , the 
value of a[2] is  'c' , and the value of  size is  3 . (the value of  a[3] is no longer of 
any concern, since the partially filled array no longer uses this indexed variable.) 
you may assume that the partially filled array contains only lowercase letters. 
embed your function in a suitable test program. 
 3.  the standard deviation of a list of numbers is a measure of how much the num-
bers deviate from the average. if the standard deviation is small, the numbers are 
clustered close to the average. if the standard deviation is large, the numbers are 
scattered far from the average. the standard deviation, s , of a list of  n numbers x
i
is defined as follows, 
  s = ha
n
i = 1
1xi - x22
n
   where  x is the average of the  n numbers  x
1
 ,  x
2
 , …. define a function that takes a 
partially filled array of numbers as its argument and returns the standard deviation 
of the numbers in the partially filled array. since a partially filled array requires two 
arguments, the function will actually have two formal parameters: an array parameter 


 
 

____________________________________________

Absolute.pdf page: 411

____________________________________________

character manipulation tools      381
 9.2 character manipulation tools 
 they spell it vinci and pronounce it vinchy; foreigners always spell better than 
they pronounce. 
mark twain, the innocents abroad
 any form of string is ultimately composed of individual characters. thus, when doing 
string processing it is often helpful to have tools at your disposal to test and manipulate 
individual values of type char . this section is about such tools. 
 character i/o 
 all data is input and output as character data. when your program outputs the 
number 10 , it is really the two characters  '1' and  '0' that are output. similarly, 
when the user wants to type in the number 10, he or she types in the character 
'1'  followed by the character  '0' . whether the computer interprets this  "10" as 
two characters or as the number 10 depends on how your program is written. but, 
however your program is written, the computer hardware is always reading the 
 self-test exercises 
 13. consider the following code (and assume it is embedded in a complete and 
correct program and then run): 
char a[80], b[80]; 
cout << "enter some input:\n"; 
cin >> a >> b; 
cout << a << '-' << b << "end of output\n"; 
 if the dialogue begins as follows, what will be the next line of output? 
enter some input: 
the
time is now. 
 14. consider the following code (and assume it is embedded in a complete and 
correct program and then run): 
char mystring[80]; 
cout << "enter a line of input:\n"; 
cin.getline(mystring, 6); 
cout << mystring << "<end of output"; 
 if the dialogue begins as follows, what will be the next line of output? 
enter a line of input: 
may the hair on your toes grow long and curly. 


 
 

____________________________________________

Absolute.pdf page: 444

____________________________________________

414     chapter 9    strings
   the input 
   mary a. user 
   should also produce the output 
   user, mary a. 
   your program should place a period after the middle initial even if the input did 
not contain a period. your program should allow for users who give no middle 
name or middle initial. in that case, the output, of course, contains no middle 
name or initial. for example, the input 
   mary user 
   should produce the output 
   user, mary 
   if you are using c-strings, assume that each name is at most 20 characters long. 
 alternatively, use the class  string . ( hint: you may want to use three string vari-
ables rather than one large string variable for the input. you may find it easier to 
not use  getline .) 
 4.  write a program that reads in a line of text and replaces all four-letter words with 
the word "love" . for example, the input string 
   i hate you, you dodo! 
   should produce the following output: 
   i love you, you love! 
   of course, the output will not always make sense. for example, the input string 
   john will run home. 
   should produce the following output: 
   love love run love. 
   if the four-letter word starts wit h a capital letter, it should be replaced by  "love" , 
not by "love" . you need not check capitalization, except for the first letter of a 
word. a word is any string consisting of the letters of the alphabet and delimited at 
each end by a blank, the end of the line, or any other character that is not a letter. 
your program should repeat this action until the user says to quit. 
 5.  write a program that can be used to train the user to use less sexist language by 
suggesting alternative versions of sentences given by the user. the program will ask 
for a sentence, read the sentence into a string variable, and replace all occurrences 
of masculine pronouns with gender-neutral pronouns. for example, it will replace 
"he" with  "she or he" . thus, the input sentence 
   see an adviser, talk to him, and listen to him. 
   should produce the following suggested changed version of the sentence: 
   see an adviser, talk to her or him, and listen to her or him. 
   be sure to preserve uppercase letters for the first word of the sentence. the pronoun 
"his" can be replaced by  "her(s)" ; your program need not decide between  "her"
and "hers" . allow the user to repeat this for more sentences until the user says she 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 445

____________________________________________

or he is done. this will be a long program that requires a good deal of patience. 
your program should not replace the string "he" when it occurs inside another 
word such as "here" . a word is any string consisting of the letters of the alphabet 
and delimited at each end by a blank, the end of the line, or any other character 
that is not a letter. allow your sentences to be up to 100 characters long. 
  6.  there is a cd available for purchase that contains  .jpeg and  .gif images of music 
that is in the public domain. the cd includes a file consisting of lines contain-
ing the names, then composers of that title, one per line. the name of the piece is 
first, then zero or more spaces then a dash (-) character, then one or more spaces, 
then the composer’s name. the composer name may be only the last name, an 
initial and one name, two names (first and last), or three names (first, middle, and 
last). there are a few tunes with “no author listed” as author. in the subsequent 
processing, “no author listed” should not be rearranged. here is a very abbreviated 
list of the titles and authors. 
   1. adagio “moonlight” sonata - ludwig van beethoven 
   2. an alexis - f.h. hummel and j.n. hummel 
   3. a la bien aimee - ben schutt 
   4. at sunset - e. macdowell 
   5. angelus - j. massenet 
   6. anitra’s dance - edward grieg 
   7. ase’s death - edward grieg 
   8. au matin- benj. - godard 
   … 
   37. the dying poet - l. gottschalk 
   38. dead march - g.f. handel 
   39. do they think of me at home - chas. w. glover 
   40. the dearest spot - w.t. wrighton 
   1. evening - l. van beethoven 
   2. embarrassment - franz abt 
   3. erin is my home - no author listed 
   4. ellen bayne - stephen c. foster 
   … 
   9. alla mazurka - a. nemerowsky 
   … 
   1. the dying volunteer - a.e. muse 
   2. dolly day - stephen c. foster 
   3. dolcy jones - stephen c. foster 
   4. dickory, dickory, dock - no author listed 
programming projects      415


 
 

____________________________________________

Absolute.pdf page: 592

____________________________________________

562     chapter 12    streams and file i/o
  b.  for a sorted file, a quartile is one of three numbers: the first has one-fourth the 
data values less than or equal to it, one-fourth the data values between the first and 
second numbers (up to and including the second number), one-fourth the data 
points between the second and the third (up to and including the third number), 
and one-fourth above the third quartile. find the three quartiles for the data file you 
used for part a. note that “one-fourth” means as close to one-fourth as possible. 
hint : you should recognize that having done part a you have one-third of your 
job done. (you have the second quartile already.) you also should recognize that 
you have done almost all the work toward finding the other two quartiles as well. 
 4.  write a program that takes its input from a file of numbers of type  double . the 
program outputs to the screen the average and standard deviation of the numbers 
in the file. the file contains nothing but numbers of type double separated by 
blanks and/or line breaks. the standard deviation of a list of numbers n
1
, n
2
, n
3
,
and so forth, is defined as the square root of the average of the following numbers: 
  (n
1
- a)2 , (n
2
- a)2 , (n
3
- a)2 , and so forth 
 the number  a is the average of the numbers n
1
 , n
2
 , n
3
 , and so forth. 
hint : write your program so that it first reads the entire file and computes the 
 average of all the numbers, then closes the file, then reopens the file and com-
putes the standard deviation. you will find it helpful to first do  programming 
project  12.2 and then modify that program to obtain the program for this project. 
 5.  write a program that gives and takes advice on program writing. the program 
starts by writing a piece of advice to the screen and asking the user to type in a 
 different piece of advice. the program then ends. the next person to run the pro-
gram receives the advice given by the person who last ran the program. the advice 
is kept in a file, and the contents of the file change after each run of the program. 
you can use your editor to enter the initial piece of advice in the file so that the 
first person who runs the program receives some advice. allow the user to type in 
advice of any length (any number of lines long). the user is told to end his or her 
advice by pressing the return key two times. your program can then test to see that 
it has reached the end of the input by checking to see when it reads two consecutive 
occurrences of the character '\n' . 
 6.  write a program that merges the numbers in two files and writes all the numbers 
into a third file. your program takes input from two different files and writes its 
output to a third file. each input file contains a list of numbers of type int in 
sorted order from the smallest to the largest. after the program is run, the output 
file will contain all the numbers in the two input files in one longer list in sorted 
order from smallest to largest. your program should define a function that is called 
with the two input-file streams and the output-file stream as three arguments. 
 7.  write a program to generate personalized junk mail. the program takes input 
both from an input file and from the keyboard. the input file contains the text of 
a letter, except that the name of the recipient is indicated by the three characters 
#n# . the program asks the user for a name and then writes the letter to a second 
file but with the three letters #n# replaced by the name. the three-letter string  #n#
will occur exactly once in the letter. 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 597

____________________________________________

programming projects      567
 depending on the speed of your computer and your implementation, execution of 
this program may take from minutes to hours. 
 18.  the text files  boynames.txt and  girlnames.txt , which are included in the 
source code on this  book’s website , contain a list of the 1,000 most popular boy 
and girl names in the united states for the year 2003 as compiled by the social 
security administration. 
 these are blank-delimited files where the most popular name is listed first, the 
second most popular name is listed second, and so on to the 1,000th most popular 
name, which is listed last. each line consists of the first name, followed by a blank 
space, followed by the number of registered births in the year using that name. for 
example, the girlnames.txt file begins with 
 emily 25494 
 emma 22532 
 madison 19986 
 this indicates that emily is the most popular name with 25,494 registered nam-
ings, emma is the second most popular with 22,532, and madison is the third 
most popular with 19,986. 
 write a program that reads both the girl’s and boy’s files into memory using arrays. 
then, allow the user to input a name. the program should search through both 
arrays and, when there is a match, output the popularity and number of namings. 
the program should also indicate if there is no match. 
 for example, if the user enters the name “justice,” the program should output 
 justice is ranked 456 in popularity among girls with 655 namings. 
 justice is ranked 401 in popularity among boys with 653 namings. 
 if the user enters the name “walter,” the program should output 
 walter is not ranked among the top 1000 girl names. 
 walter is ranked 356 in popularity among boys with 775 namings. 
 19.  html files use tags enclosed in angle brackets to denote formatting instructions. 
for example, <b> indicates bold and  <i> indicates italics. if a web browser is dis-
playing an html document that contains < or >, it may mistake these symbols 
for tags. this is a common problem with c++ files, which contain many <’s and 
>’s. for example, the line "#include <iostream>" may result in the browser 
interpreting <iostream> as a tag. 
 to avoid this problem, html uses special symbols to denote < and >. the <
symbol is created with the string &lt; while the > symbol is created with the 
string &gt; . 
 write a program that reads in a c++ source file and converts all < symbols to 
& it; and all > symbols to &gt;. also add the tag <pre> to the beginning of the 
file and </pre> to the end of the file. this tag preserves whitespace and formatting 
in the html document. your program should output the html file to disk. 


 
 

____________________________________________

Absolute.pdf page: 602

____________________________________________

 after a lecture on cosmology and the structure of the solar system, 
william james was accosted by a little old lady. 
 “your theory that the sun is the center of the solar system, and the 
earth is a ball which rotates around it has a very convincing ring to it, 
mr. james, but it’s wrong. i’ve got a better theory,” said the little old lady. 
 “and what is that, madam?” inquired james politely. 
 “that we live on a crust of earth which is on the back of a giant turtle.” 
 not wishing to demolish this absurd little theory by bringing to bear the 
masses of scientific evidence he had at his command, james decided 
to gently dissuade his opponent by making her see some of the 
inadequacies of her position. 
 “if your theory is correct, madam,” he asked, “what does this turtle 
stand on?” 
 “you’re a very clever man, mr. james, and that’s a very good question” 
replied the little old lady, “but i have an answer to it. and it is this: the 
first turtle stands on the back of a second, far larger, turtle, who stands 
directly under him.” 
 “but what does this second turtle stand on?” persisted james patiently. 
to this the little old lady crowed triumphantly. “it’s no use, mr. james— 
it’s turtles all the way down.” 
 j. r. ross, constraints on variables in syntax
 introduction 
 a function definition that includes a call to itself is said to be  recursive . like most 
modern programming languages, c++ allows functions to be recursive. if used with a 
little care, recursion can be a useful programming technique. this chapter introduces 
the basic techniques needed for defining successful recursive functions. there is 
nothing in this chapter that is truly unique to c++. if you are already familiar with 
recursion, you can safely skip this chapter. 
 this chapter uses material from  chapters  1 to  5 only.  sections  13.1 and  13.2 do not 
use any material from  chapter  5 , so you can cover recursion any time after  chapter  4 . 
if you have not read  chapter  11 , you may find it helpful to review the section of 
 chapter  1 on namespaces. 
13 recursion
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 639

____________________________________________

programming projects      609
do  programming project  13.1 iteratively, rather than recursively; that is, do 
the problem with a loop. you should compute each fibonacci number once 
on the way to the number requested and discard the numbers when they 
are no longer needed. 
  b.  time the solution for  programming project  13.1 and part a of this project in
      finding the 1 st , 3 rd , 5 th , 7 th , 9 th , 11 th , 13 th , and 15 th fibonacci numbers. determine
      how long each function takes. compare and comment on your results. 
   hints: if you are running linux, you can use the bash  time utility. it gives real time 
(as in wall clock time), user time (time measured by cpu cycles devoted to your 
program), and sys time (cpu cycles devoted to tasks other than your program). if 
you are running in some other environment, you will have to read your manual, 
or ask your instructor, in order to find out how to measure the time a program 
takes to run. 
 7.  (you need to have first completed  programming project  13.6 to work on this 
project.) when computing a fibonacci number using the most straightforward 
recursive function definition, the recursive solution recomputes each fibonacci 
number too many times. to compute f
i+2
= f
i
+ f
i+1
 , it computes all the numbers 
computed in  f i a second time in computing  f
i+1
 . you can avoid this by saving the 
numbers in an array while computing f
i
 . write another version of your recursive 
fibonacci function based on this idea. in the recursive solution for calculating the 
n th fibonacci number, declare an array of size n. array entry with index i stores 
the ith (i … n) fibonacci number as it is computed the first time. then use the array 
to avoid the second (redundant) recalculation of the fibonacci numbers. time this 
solution as you did in  programming project  13.6 , and compare it to your results 
for the iterative solution. 
 8.  a savings account typically accrues savings using compound interest. if you deposit 
$1000 with a 10% interest rate per year, after one year you will have $1100. if you 
leave this money in the account for another year at 10% interest, you will have 
$1210. after three years you will have $1331, and so on. 
   write a program that inputs the initial amount, an interest rate per year, and the 
number of years the money will accrue compound interest. write a recursive func-
tion that calculates the amount of money that will be in the savings account using 
the input information. 
  to verify your function, the amount should be equal to  p(1+i)n , where  p is the 
amount initially saved, i is the interest rate per year, and  n is the number of years. 
 9 . we have  n people in a room, where  n is an integer greater than or equal to 1. each 
person shakes hands once with every other person. what is the total number, h(n) , 
of handshakes? write a recursive function to solve this problem. to get you started, 
if there are only one or two people in the room, then 
 handshake(1) = 0 
 handshake(2) = 1 
  if a third person enters the room, he or she must shake hands with each of the 
two people already there. this is two handshakes in addition to the number 
solution to 
programming
project 13.9
videonote


 
 

____________________________________________

Absolute.pdf page: 640

____________________________________________

610     chapter 13    recursion
of handshakes that would be made in a room of two people, or a total of three 
handshakes. 
  if a fourth person enters the room, he or she must shake hands with each of the 
three people already there. this is three handshakes in addition to the number of 
handshakes that would be made in a room of three people, or six handshakes. 
   if you can generalize this to  n handshakes, you should be able to write the 
recursive solution. 
 10.  consider a frame of bowling pins, where each * represents a pin: 
solution to 
programming 
project  13.11
videonote
   there are five rows and a total of fifteen pins. if we had only the top four rows, 
there would be a total of ten pins. if we had only the top three rows, there would 
be a total of six pins. if we had only the top two rows, there would be a total of 
three pins. if we had only the top row, there would be a total of one pin. 
  write a recursive function that takes as input the number of rows,  n , and outputs 
the total number of pins that would exist in a pyramid with n rows. your program 
should allow for values of n that are larger than 5. 
 11.  write a recursive function named  contains with the following header: 
 bool contains ( char *haystack,  char *needle) 
  the function should return  true if the c-string  needle is contained within the 
c-string haystack and false if needle is not in haystack . for example,
 contains("c++ programming", "ogra") should return  true 
 contains("c++ programming", "grammy") should return  false 
  you are not allowed to use the string class substr or find functions to 
determine a match.
 12.  the following diagram is an example of a  deterministic finite state automaton , or 
dfa. this particular dfa describes an algorithm to determine if a sequence of 
characters is a properly formatted monetary amount with commas. for example, 
“$1,000” and “$25” and “$551,323,991,391” are properly formatted but “1,000” 
(no initial $) and “$1000” (missing comma) and “$5424,132” (missing comma) 
are not. 
*
* *
* * *
* * * *
* * * * *
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 705

____________________________________________

pointers and virtual functions      675
 anything that is a  dog is also a  pet . it would seem to make sense to allow programs 
to consider values of type dog to also be values of type  pet , and hence the following 
should be allowed: 
vdog.name = "tiny"; 
vdog.breed = "great dane"; 
vpet = vdog; 
 c ++ does allow this sort of assignment. you may assign a value, such as the value of 
vdog , to a variable of a parent type, such as  vpet , but you are not allowed to perform 
the reverse assignment. although the preceding assignment is allowed, the value that is 
assigned to the variable vpet loses its  breed field. this is called the  slicing problem . 
the following attempted access will produce an error message: 
cout << vpet.breed; 
// illegal: class pet has no member named breed 
 you can argue that this makes sense, since once a  dog is moved to a variable of type 
pet it should be treated like any other  pet and not have properties peculiar to  dog s. 
this makes for a lively philosophical debate, but it is usually just a nuisance when 
programming. the dog named tiny is still a great dane, and we would like to refer to 
its breed, even if we treated it as a pet someplace along the way. 
 fortunately, c ++ does offer us a way to treat a  dog as a  pet without throwing away 
the name of the breed. to do this, we use pointers to dynamic variables. 
 suppose we add the following declarations: 
pet *ppet; 
dog *pdog; 
 if we use pointers and dynamic variables, we can treat tiny as a  pet without losing his 
breed. the following is allowed. 1
pdog = new dog; 
pdog->name = "tiny"; 
pdog->breed = "great dane"; 
ppet = pdog; 
 moreover, we can still access the  breed field of the node pointed to by  ppet . 
suppose that 
dog::print( ) const;
 has been defined as follows: 
void dog::print( ) const
{
    cout << "name:" << name << endl; 
    cout << "breed:" << breed << endl; 
}
slicing
problem
1 if you are not familiar with the -> operator, see the subsection of  chapter  10 entitled “the ->
operator.” 


 
 

____________________________________________

Absolute.pdf page: 762

____________________________________________

 if somebody there chanced to be 
 who loved me in a manner true 
 my heart would point him out to me 
 and i would point him out to you. 
 gilbert and sullivan,  ruddigore 
 introduction 
 a  linked list is a list constructed using pointers. a linked list is not fixed in size but can 
grow and shrink while your program is running. a tree is another kind of data structure 
constructed using pointers. this chapter introduces the use of pointers for building 
such data structures. the standard template library (stl) has predefined versions 
of these and other similar data structures.  the stl is covered in  chapter  19 . it often 
makes more sense to use the predefined data structures in the stl rather than defining 
your own. however, there are cases where you need to define your own data structures 
using pointers. (somebody had to define the stl.) also, this material will give you 
some insight into how the stl might have been defined and will introduce you to 
some basic widely used material. 
 linked data structures produce their structures using dynamic variables, which 
are created with the new operator. the linked data structures use pointers to connect 
these variables. this gives you complete control over how you build and manage your 
data structures, including how you manage memory. this allows you to sometimes do 
things more efficiently. for example, it is easier and faster to insert a value into a sorted 
linked list than into a sorted array. 
 there are basically three ways to handle data structures of the kind discussed in this 
chapter:
1.  the c-style approach of using global functions and struct s with everything public 
2.  using classes with all member variables private and using accessor and mutator 
functions
3.  using friend classes (or something similar, such as private or protected inheritance 
or locally defi ned classes) 
 we give examples of all three methods. we introduce linked lists using method 1. we 
then present more details about basic linked lists and introduce both the stack and 
queue data structures using method 2. we give an alternate definition of our queue 
template class using friend classes (method 3), and also use friend classes (method 3) 
to present a tree template class. this way you can see the virtues and shortcomings of 
each approach. our personal preference is to use friend classes, but each method has its 
own advocates. 
 17  linked data structures 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 768

____________________________________________

738     chapter 17    linked data structures
 linked lists 
 lists such as those shown in  display  17.1 are called  linked lists. a  linked list is a list 
of nodes in which each node has a member variable that is a pointer that points to the 
next node in the list. the first node in a linked list is called the  head , which is why 
the pointer variable that points to the first node is named head . note that the pointer 
named head is not itself the head of the list but only points to it. the last node has no 
special name, but it does have a special property: it has null as the value of its member 
pointer variable. to test whether a node is the last node, you need only test whether the 
pointer variable in the node is equal to null . 
 our goal in this section is to write some basic functions for manipulating linked 
lists. for variety, and to simplify the notation, we will use a simpler type of data for 
the nodes than that used in  display  17.2 . these nodes will contain only an integer and 
a pointer. however, we will make our nodes more complicated in one sense. we will 
make them objects of a class, rather than just a simple struct . the node and pointer 
type definitions that we will use are as follows: 
class intnode 
{
public:
    intnode( ) {} 
    intnode( int thedata, intnode* thelink) 
            : data(thedata), link(thelink) {} 
    intnode* getlink( ) const { return link; } 
int getdata( ) const { return data; } 
 2.  suppose that your program contains the type defi nitions and code given in 
 self-test exercise  1 . that code creates a node that contains the string "sally"
and the number 18. what code would you add to set the value of the member 
variable next of this node equal to null? 
 3.  consider the following structure defi nition: 
struct listnode 
{
    string item; 
int count; 
    listnode *link; 
};
listnode *head = new listnode; 
 give code to assign the string "wilbur's  brother  orville" to the member 
variable item of the variable to which  head points. 
self-test exercises (continued)
linked list 
head
node type 
definition
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 821

____________________________________________

 iterators      791
in a setintersection function that examines at most n + m nodes. nevertheless, 
our linked list implementation would probably be fine for an application that uses 
small sets or for an application that does not frequently invoke the setintersection
function, and we have the benefit of relatively simple code that is easy to understand. 
 if we really needed the efficiency, we could maintain the same interface to the 
set<t> class but replace our linked list implementation with something else. if we 
used the hash table implementation from  display  17.25 , the  contains function 
would run much more quickly. however, switching to a hash table makes it more 
difficult to iterate through the set of items. instead of traversing a single linked list to 
retrieve every item in the set, the hash table version must now iterate through the hash 
table array and then, for each index in the array, iterate through the linked list at that 
index. examination of each entry in the hash table array takes extra time that was not 
necessary in the singly linked list implementation of a set. so while we have decreased 
the number of steps it takes to look up an item, we have increased the number of steps 
it takes to iterate over every item. if this were troublesome, you could overcome this 
problem with an implementation of set<t> that used both a linked list (to facilitate 
iteration) and a hash table (for fast lookup). however, the complexity of the code is 
significantly increased using such an approach. you are asked to explore the hash table 
implementation in  programming project  17.10 . 
 self-test exercise 
23 .  write a function named  difference for the  set class that returns the 
difference between two sets. the function should return a pointer to a new 
set that has items from the fi rst set that are not in the second set. 
for example, if seta contains {1, 2, 3, 4} and  setb contains {2, 4, 5}, then 
seta.difference(setb) should return the set {1, 3}. 
 17.3 iterators 
 the white rabbit put on his spectacles. “where shall i begin, please your 
majesty?” he asked. 
 “begin at the beginning,” the king said, very gravely, “and go on till you 
 come to the end: then stop.” 
lewis carroll,  alice in wonderland 
 an important notion in data structures is that of an iterator. an  iterator is a construct 
(typically an object of some iterator class) that allows you to cycle through the data 
items stored in a data structure so that you can perform whatever action you want on 
each data item. 
iterator


 
 

____________________________________________

Absolute.pdf page: 839

____________________________________________

•  nodes in a doubly linked list have two links—one to the previous node in the list and 
one to the next node. this makes operations such as insertion and deletion slightly 
easier. 
•  a stack is a first-in/last-out data structure. a queue is a first-in/first-out data structure. 
both can be implemented using a linked list. 
•  a hash table is a data structure that is used to store objects and retrieve them effi-
ciently. a hash function is used to map an object to a value that can then be used to 
index the object. 
•  linked lists can be used to implement sets, including common operations such as 
union ,  intersection , and  set membership. 
•  an iterator is a construct (typically an object of some iterator class) that allows you to 
cycle through data items stored in a data structure. 
•  a tree is a data structure whose nodes have two (or more) member variables for 
pointers to other nodes. if a tree satisfies the binary search tree storage rule, then a 
function can be designed to rapidly find data in the tree. 
 answers to self-test exercises 
 1. sally
  sally 
  18 
  18 
  note that  (*head).name and  head->name mean the same thing. similarly, 
(*head).number and  head->number mean the same thing. 
 2.  the best answer is 
  head->next = null; 
  however, the following is also correct: 
  (*head).next = null;
 3.  head->item = "wilbur's brother orville";
 4.  class nodetype 
  {
  public:
      nodetype( ){} 
      nodetype(char thedata, nodetype* thelink) 
              : data(thedata), link(thelink){} 
      nodetype* getlink( ) const { return link; } 
answers to self-test exercises      809


 
 

____________________________________________

Absolute.pdf page: 889

____________________________________________

iterators      859
 if you have not already done so, you should read  section  7.3 of  chapter  7 , which 
covers the vector template class of the stl.  although the current chapter does not 
use any of the material in  chapter  17 , most readers will find that reading  chapter  17 
before reading this one will aid his or her comprehension of this chapter by giving 
sample concrete implementations of some of the abstract ideas intrinsic to the stl. 
 this chapter does not use any of the material in  chapters  12 to  15 . 
 19.1 iterators 
 to iterate is human, and programmers are human. 
 anonymous 
 if you have not yet done so, you should read  chapter  10 on pointers and arrays 
and also read  section  7.3 of  chapter  7 , which covers vectors. vectors are one of the 
container template classes in the stl. iterators are a generalization of pointers. this 
section shows how to use iterators with vectors. other container template classes 
that we introduce in  section  19.2 use iterators in the same way. so, all that you learn 
about iterators in this section will apply across a wide range of containers rather than 
applying solely to vectors. this reflects one of the basic tenets of the stl philosophy: 
the semantics, naming, and syntax for iterator usage should be (and is) uniform across 
different container types. 
 iterator basics 
 an  iterator is a generalization of a pointer, and in fact is typically even implemented 
using a pointer, but the abstraction of an iterator is designed to spare you the 
details of the implementation and give you a uniform interface to iterators that is 
the same across different container classes. each container class has its own iterator 
types, just like each data type has its own pointer type. but just as all pointer types 
behave essentially the same for dynamic variables of their particular data type, so too 
does each iterator type behave the same, but each iterator is used only with its own 
container type. 
 an iterator is not a pointer, but you will not go far wrong if you think of it and use 
it as if it were. like a pointer variable, an iterator variable is located at (meaning, it 
points to) one data entry in the container. you manipulate iterators using the following 
overloaded operators that apply to iterator objects: 
■  prefix and postfix increment operators ( ++ ) for advancing the iterator to the next 
data item. 
■  prefix and postfix decrement operators ( -- ) for moving the iterator to the 
previous data item. 
■  equal and unequal operators (== and  != ) to test whether two iterators point to 
the same data location. 
iterator


 
 

____________________________________________

Absolute.pdf page: 919

____________________________________________

generic algorithms      889
 19.3 generic algorithms 
 “and if you take one from three hundred and sixty-five, what remains?” 
“three hundred and sixty-four, of course.” 
 humpty dumpty looked doubtful. “i'd rather see that done on paper,” he said. 
 lewis carroll,  through the looking-glass 
 this section covers some basic function templates in the stl. we cannot give you a 
comprehensive description of them all here, but we will present a large enough sample 
to give you a good feel for what is contained in the stl and to give you sufficient 
detail to start using these template functions. 
 these template functions are sometimes called  generic algorithms . the term 
algorithm is used for a reason. recall that an algorithm is just a set of instructions 
for performing a task. an algorithm can be presented in any language, including a 
programming language like c++. but, when using the word algorithm , programmers 
typically have in mind a less formal presentation given in english or pseudocode. 
as such, it is often thought of as an abstraction of the code defining a function. it 
iterating through all planets: 
earth - home 
jupiter - largest planet in our solar system 
mars - the red planet 
mercury - hot planet 
neptune - 1500 mile-per-hour winds 
pluto - dwarf planet 
saturn - has rings 
uranus - tilts on its side 
venus - atmosphere of sulfuric acid 
display 19.14 program using the map template class (part 2 of 2)
 self-test exercises 
 14.  why are the elements in the  set template class stored in sorted order? 
 15.  can a  set have elements of a class type? 
 16.  suppose  s is of the type  set<char> . what value is returned by  s.find('a') if 
'a' is in  s ? what value is returned if  'a' is not in  s ? 
 17.  how many elements will be in the map  mymap after the following code executes? 
map<int, string> mymap; 
mymap[5] = "c++"; 
cout << mymap[4] << endl; 
generic
algorithm


 
 

____________________________________________

Absolute.pdf page: 920

____________________________________________

890     chapter 19    standard template library
gives the important details but not the fine details of the coding. the stl specifies 
certain details about the algorithms underlying the stl template functions, which is 
why they are sometimes called generic algorithms . these stl function templates do 
more than just deliver a value in any way that the implementers wish. the function 
templates in the stl come with minimum requirements that must be satisfied by 
their implementations if they are to satisfy the standard. in most cases, they must be 
implemented with a guaranteed running time. this adds an entirely new dimension to 
the idea of a function interface. in the stl, the interface not only tells a programmer 
what the function does and how to use the functions, but also how rapidly the task 
will be done. in some cases, the standard even specifies the particular algorithm that 
is used, although not the exact details of the coding. moreover, when it does specify 
the particular algorithm, it does so because of the known efficiency of the algorithm. 
the key new point is the specification of an efficiency guarantee for the code. in this 
chapter, we will use the terms generic algorithm ,  generic function , and  stl function 
template to all mean the same thing. 
 in order to have some terminology to discuss the efficiency of these template 
functions or generic algorithms, we first present some background on how the 
efficiency of algorithms is usually measured. 
 running times and big- o notation 
 if you ask a programmer how fast his or her program is, you might expect an answer 
like “two seconds.” however, the speed of a program cannot be given by a single 
number. a program will typically take a longer amount of time on larger inputs than 
it will on smaller inputs. you would expect that a program for sorting numbers would 
take less time to sort 10 numbers than it would to sort 1000 numbers. perhaps it takes 
2 seconds to sort 10 numbers, but 10 seconds to sort 1000 numbers. how then should 
the programmer answer the question “how fast is your program?” the programmer 
would have to give a table of values showing how long the program takes for different 
sizes of input. for example, the table might be as shown in  display  19.15 . this table 
does not give a single time, but instead gives different times for a variety of different 
input sizes. 
 the table is a description of what is called a  function in mathematics. just as a (non-
void ) c ++ function takes an argument and returns a value, so too does this function 
take an argument, which is an input size, and returns a number, which is the time the 
program takes on an input of that size. if we call this function t , then  t (10) is 2 seconds, 
t (100) is 2.1 seconds,  t (1000) is 10 seconds, and  t (10,000) is 2.5 minutes. the 
table is just a sample of some of the values of this function t . the program will take 
some amount of time on inputs of every size. so although they are not shown in the 
table, there are also values for t (1),  t (2), . . .,  t (101),  t (102), and so forth. for 
any positive integer n ,  t (n ) is the amount of time it takes for the program to sort 
n numbers. the function  t is called the  running time of the program. 
  so far we have been assuming that this sorting program will take the same amount 
of time on any list of n numbers. that need not be true. perhaps it takes much less 
time if the list is already sorted or almost sorted. in that case, t (n ) is defined to be the 
time taken by the “hardest” list—that is, the time taken on that list of n numbers that 
mathematical
function
running time 
www.itpub.net


 
 






female is *****
____________________________________________

SecretsAndLies.pdf page: 48

____________________________________________

like the censorship laws or computer crime statutes in your country? find
a country more to your liking. countries like singapore have tried to
limit their citizens’ abilities to search the web, but the way the internet is
built makes blocking off parts of it unfeasible. as john gilmore opined,
“the internet treats censorship as damage and routes around it.”
this means that internet attackers don’t have to be anywhere near
their prey. an attacker could sit behind a computer in st. petersburg and
attack citibank’s computers in new york. this has enormous security
implications. if you were building a warehouse in buffalo, you’d only
have to worry about the set of criminals who would consider driving to
buffalo and breaking into your warehouse. since on the internet every
computer is equidistant from every other computer, you have to worry
about all the criminals in the world.
the global nature of the internet complicates criminal investigation
and prosecution, too. finding attackers adroit at concealing their where-
abouts can be near impossible, and even if you do find them, what do you
do then? and crime is only defined with respect to political borders. but
if the internet has no physical “area” to control, who polices it?
so far, every jurisdiction that possibly can lay a claim to the internet
has tried to. does the data originate in germany? then it is subject to
german law. does it terminate in the united states? then it had better
suit the american government. does it pass through france? if so, the
french authorities want a say in qu’il s’est passé. in 1994, the operators of
a computer bulletin board system (bbs) in milpitas, california—where
both the people and the computers resided—were tried and convicted in
a tennessee court because someone in tennessee made a long-distance
telephone call to california and downloaded dirty pictures that were
found to be acceptable in california but indecent in tennessee. the bul-
letin board operators never set foot in tennessee before the trial. in july
1997, a 33-year old woman was convicted by a swiss court for sending
pornography across the internet—even though she had been in the
united states since 1993. does this make any sense?
in general, though, prosecuting across jurisdictions is incredibly diffi-
cult. until it’s sorted out, criminals can take advantage of the confusion as
a shield. in 1995, a 29-year-old hacker from st. petersburg, russia, made
$12 million breaking into citibank’s computers. citibank eventually dis-
covered the break and recovered most of the money, but had trouble
extraditing the hacker to stand trial.
20 c h a p t e r  t w o
453803_ch02.qxd:453803_ch02.qxd  4/12/13  9:27 am  page 20


 
 

____________________________________________

SecretsAndLies.pdf page: 71

____________________________________________

a wealthy adversary is the most flexible, since he can trade his
resources for other things. he can gain access by paying off an insider, and
expertise by buying technology or hiring experts (maybe telling them the
truth, maybe hiring them under false pretenses). he can also trade
money for risk by executing a more sophisticated—and therefore more
expensive—attack.
the rational adversary—not all adversaries are sane, but most are
rational within their frames of reference—will choose an attack that gives
him a good return on investment, considering his budget constraints:
expertise, access, manpower, time, and risk. some attacks require a lot of
access but not much expertise: a car bomb, for example. some attacks
require a lot of expertise but no access: breaking an encryption algorithm,
for example. each adversary is going to have a set of attacks that is
affordable to him, and a set of attacks that isn’t. if the adversary is paying
attention, he will choose the attack that minimizes his cost and maximizes
his benefits. 
hackers
the word hacker has several definitions, ranging from a corporate system
administrator adept enough to figure out how computers really work to
an ethically inept teenage criminal who cackles like beavis and butthead
as he trashes your network. the word has been co-opted by the media
and stripped of its meaning. it used to be a compliment; then it became
an insult. lately, people seem to like “cracker” for the bad guys, and
“hacker” for the good guys. i define a hacker as an individual who exper-
iments with the limitations of systems for intellectual curiosity or sheer
pleasure; the word describes a person with a particular set of skills and not
a particular set of morals. there are good hackers and bad hackers, just as
there are good plumbers and bad plumbers. (there are also good bad
hackers, and bad good hackers . . . but never mind that.)
hackers are as old as curiosity, although the term itself is modern.
galileo was a hacker. mme. curie was one, too. aristotle wasn’t. (aristo-
tle had some theoretical proof that women had fewer teeth than men.
a hacker would have simply counted his wife’s teeth. a good hacker
would have counted his wife’s teeth without her knowing about it, while
adversaries 43
453803_ch04.qxd:453803_ch04.qxd  4/11/13  3:27 pm  page 43


 
 

____________________________________________

SecretsAndLies.pdf page: 94

____________________________________________

anonymity. banks have no reason to give it to them, especially while the
government is pressuring them not to.
medical anonymity
and then there are medical databases. on the one hand, medical data
are only useful if shared. doctors need to know the medical history of
their patients, and aggregate medical data is useful for all sorts of
research. on the other hand, medical information is about as personal as
it gets: genetic predisposition to disease, abortions and reproductive
health, emotional health and psychiatric care, drug abuse, sexual behav-
iors, sexually transmitted diseases, hiv status, physical abuse. people have
a right to keep their medical information private. people have been
harassed, threatened, and fired after personal medical information was
made public.
and it’s not hard to get this information. nicole brown simpson’s
medical records were leaked to the press within a week after her 1994
murder. in 1995, the sunday times of london reported that the going
price for anyone’s medical record in england was £200. and these cases
are from wealthy countries; just imagine what kinds of abuses are possible
in countries like india or mexico, where a $10 bill can tempt even the
most virtuous civil servant.
computerized patient data is bad for privacy. but it’s good for just
about everything else, so it’s inevitable. hipaa (the health insurance
portability and accessibility act) now has standards for computerized
medical records. it makes it easier to provide information when and
where it is needed, for a population that is less likely to have a family doc-
tor and more likely to move around the country, visiting different doctors
and hospitals when necessary. specialists can easily call up vital data. insur-
ance companies like it because it allows more automation, greater stan-
dardization, and cheaper processing: if all the data are electronic, then it
will be cheaper to process claims. and researchers like it because it allows
them to make better use of the available data: for the first time they can
look at everything, in standard form.
this is a big deal, probably as important as the financial and credit
databases mentioned previously. we as a society are going to have to
balance the need for access (which is much more evident for
medical information than financial information) with the need for pri-
66 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 66


 
 

____________________________________________

SecretsAndLies.pdf page: 98

____________________________________________

verifone reader, which would dial into a central server and make sure the
account was valid and had enough credit for the purchase. the deli man
would be expected to examine the card to make sure it isn’t a forgery, and
check the signature against the one on the back of the card. (most
merchants don’t bother, especially for low-value transactions.)
if the customer paid by check, there would be another authentication
dance. the deli man would look at the check, and possibly ask the
customer for some identification. then he might write the customer’s
driver’s license number and phone number on the back of the check, or
maybe the customer’s credit card number. none of this will actually help
the deli man collect on a bad check, but it does help him track the
customer down in the event of a problem.
attacking authentication can be very profitable. in 1988, thompson
sanders was convicted of defrauding the chicago board of trade. he
synthesized a nonexistent trader, complete with wig, beard, and fake cre-
dentials. this fake trader would place large risky orders, then claim those
that were profitable and walk away from those that were not. the brokers
on the other side of the losing transactions, unable to prove who they
made the trade with, would be responsible for the losses.
back to the deli. another customer walks in. she and the deli man are
old friends. they recognize each other—authenticating each other by
face. this is a robust authentication system; people recognize each other
even though she has a new hairstyle and he is wearing a new toupee and
glasses. superheroes realize this, and wear masks to hide their secret iden-
tity. that works better in comic books than in real life, because face-to-
face authentication isn’t only face recognition (otherwise the blind would
never recognize anyone). people remember each other’s voice, build,
mannerisms, and so forth. if the deli man called his friend on the phone,
they could authenticate each other without any visual cues at all.
commissioner gordon ought to figure out that bruce wayne is really
batman, simply because they talk on the phone so often.
in any case, our bratwurst-filled customer finishes eating. he says
goodbye to the deli man, sure in the knowledge that he is saying good-
bye to the same deli man who served him his bratwurst. he leaves
through the same door that he came in by, and goes home.
70 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 70


 
 

____________________________________________

SecretsAndLies.pdf page: 104

____________________________________________

swedish. paul schliemann (heinrich schliemann’s grandson) claimed to
have discovered the secret of atlantis in the ancient mayan troano
codex, which he read in the british museum. never mind that no one
could read mayan, and that the codex was stored in madrid. bismarck’s
rewrite of the 1870 ems telegram effectively started the franco-prussian
war. in 1996, when david selbourne tried to pass off his translation of a
thirteenth-century italian traveler’s visit to china (beating marco polo by
three years); he used the “owner of the manuscript allowed him to
translate it only if he swore himself to secrecy” trick to avoid having to
produce a suitable forgery.
the problem is that the digital world makes this kind of thing easier,
because it is so easy to produce a forgery and so hard to verify the accu-
racy of anything. in may 1997, a 13-year-old brooklynite won a national
spelling bee. when the new york post published the associated press
photo of her jumping for joy, it erased the name of her sponsoring news-
paper, the new york daily news, from a sign around her neck. video, too:
when cbs covered the 2000 new year celebration, they digitally
superimposed their own logo over the 30-by-40-foot nbc logo in
times square. and fake essays and speeches, like the vonnegut speech,
are posted on the internet all the time.
images can have powerful effects on people. they can change minds
and move foreign policy. desert storm pictures of trapped iraqis being
shot up by coalition airpower played a large part in the quick cease-fire:
americans didn’t like seeing the lopsided carnage. and remember soma-
lia? all it took was a 30-second video clip of a dead marine being dragged
through the streets of mogadishu to undermine the american will to
fight. information is power. and next time, the video clip could be a fake.
it sounds spooky, but unless we pay attention to this problem we will
lose the ability to tell the real thing from a fake. throughout human
history, we’ve used context to verify integrity; the electronic world has no
context. in the movie the sting, newman and redford hired a cast of
dozens and built an entire fake horseracing-betting parlor in order to con
one person. a more recent movie, the spanish prisoner, had a similar big
con. cons this involved were popular around the time of the depression;
for all i know it’s still done today. the mark is taken because he can’t
imagine that what he’s seeing—the rooms, the people, the noise, the
action—is really only a performance enacted solely for his benefit.
76 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 76


 
 

____________________________________________

SecretsAndLies.pdf page: 108

____________________________________________

designers figure out how to fix the flaw, or at least minimize the risk. the
criminals learn that their attack doesn’t work, and then go on to some
other attack. and the process continues.
you can see this in credit cards. originally, card verification was
offline. merchants were given books of bad credit card numbers every
week, and they had to manually check the number against the book.
now, card verification is done online, in real time. people were stealing
new cards out of mailboxes, so the credit card companies started requir-
ing you to call in to activate your card. now, the card and the activation
notice are mailed from different points. companies also have artificial
intelligence programs checking for irregular spending patterns. (“good
morning, sir, sorry to bother you. you’ve been a good customer for years.
we’d like to confirm that you suddenly moved to hong kong and spent
your entire credit limit on krugerands.”)
when atms were first introduced by citicorp in 1971, you would
put your card into a slot and type in your pin. the machine would ver-
ify your pin, spit the card back out at you, and then you could finish your
transaction. enterprising new york criminals would dress up in suits and
wait near these machines. after a customer’s pin was verified, she would
be approached by a suited criminal and be told that this machine was bro-
ken, or being tested, or just out of money, and  wouldn’t she please use the
machine over there. people in suits can be trusted, after all. after the cus-
tomer left, the suit would finish the first transaction and pocket the cash.
the work-around was to hold the card until the end of the transac-
tion, but that required rebuilding the hardware. the banks needed a solu-
tion fast, and they figured out a fix that could be quickly installed at the
atms: they had the nearby machines communicate with each other. as
they installed the fix throughout the branches, they could watch the crim-
inals migrate across the city looking for machines where the attack still
worked. they then retrofitted the atms to hold the card until the end of
the transaction. the long-term solution was to modify the back-end net-
work to make sure that only one transaction per card is active at any time.
this has been done, so now it doesn’t matter if the card is held by the
machine anymore. now many atms have you swipe your card instead
of inserting it, but back then there was considerable fraud while the
problem was being fixed.
80 c h a p t e r  f i v e
453803_ch05.qxd:453803_ch05.qxd  4/12/13  9:48 am  page 80


 
 

____________________________________________

SecretsAndLies.pdf page: 114

____________________________________________

tography is a boatload of acronyms that accomplish various security tasks.
ipsec, for example, secures ip traffic across the internet. it secures virtual
private networks (vpns). secure sockets layer (ssl) secures www
connections. pretty good privacy (pgp) and s/mime secure e-mail;
they prevent others from reading e-mail that isn’t addressed to them, and
from forging e-mail to look like it came from someone else. set secures
internet credit card transactions. these are all protocols. there are proto-
cols for digital content protection (music, movies, etc.), cell phone
authentication (to stop fraud), electronic commerce, and just about every-
thing else. to build these protocols, cryptographers use different algo-
rithms: encryption algorithms, digital signature algorithms, and so forth.
symmetric encryption
historically, cryptography has been used for one thing: to keep secrets.
written language itself has been used as a form of cryptography—in
ancient china only the upper classes were allowed to learn to read and
write—but the first documented use of cryptography was around 1900
b.c. in egypt: a scribe used nonstandard hieroglyphs in an inscription.
there were other examples: a mesopotamian tablet from 1500 b.c. con-
taining an enciphered formula for making pottery glazes, the hebrew
atbash cipher from 500–600 b.c., the greek skytale from 486 b.c.,
and julius caesar’s simple substitution cipher from 50–60 b.c. the kama
sutra of vatsyayana even lists secret writing as the 44th, and secret talking
as the 45th, of 64 arts (yogas) men and women should know and practice.
the main idea behind cryptography is that a group of people can use
private knowledge to keep written messages secret from everyone else.
there is a message, sometimes called the plaintext, that someone wants to
keep secure. maybe the someone (we’ll call her alice) wants to send it to
someone else (we’ll call him bob); maybe she wants to be able to read it
herself at some later date. what she doesn’t want is for anyone other than
(possibly) bob to be able to read the message.
so alice encrypts the message. she invents some transformation,
called an algorithm, of the plaintext message into a ciphertext message.
this ciphertext message is gibberish, so that an eavesdropper (we’ll call
her eve) who gets her hands on this ciphertext cannot figure out the
86 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 86


 
 

____________________________________________

SecretsAndLies.pdf page: 115

____________________________________________

plaintext, and therefore cannot figure out what the message means. bob
knows how to reverse the transformation—how to turn the ciphertext
message back into plaintext.
this works, more or less. alice can use an algorithm of her own
devising to keep her pottery glazes secret. alice and bob can agree on an
algorithm to share their thoughts on the kama sutra. and an entire class
of chinese nobles (even though none of them is called bob) can use their
written language to keep state secrets safe from the peasants.
but there are complications. first, the algorithm has to be good. eve
isn’t going to look at the ciphertext message, shrug her shoulders, and
wander off. she’s going to try to figure out what the plaintext is. if she’s
the world war ii british government, she is going to hire the best math-
ematicians, linguists, and chess players in the country, stick them and
10,000 others in a secret compound at bletchley park, and invent the
computer—just so she can break the algorithm and recover the plaintexts.
even today, the national security agency (nsa) is the single largest
consumer of computer hardware and the single largest employer of math-
ematicians in the world. alice had better be a pretty smart cryptographer
if she is going to outsmart these sorts of eves. i’ll talk more about this later.
second, it’s hard to bring people in and out of the fold. to exchange
secret messages with chinese noblemen, you had to learn how to become
literate. this took time. if you later fell out of favor with the government,
there was no way for them to prevent you from reading all the messages.
you knew how the encryption worked, and they had to kill you if they
didn’t want you reading their messages. (during world war ii, the
american military used the navajo language as a code. these navajo
code talkers kept their language secret from the japanese in world war ii,
but the whole system would have collapsed if a single navajo switched
allegiances.) 
these two problems, left unsolved, would make cryptography almost
useless today. you’re one of the whatever-million people on the internet,
and you want to communicate securely with 100 of your closest friends.
you don’t want to share a common secret language with the 100 people;
you want 100 separate secure algorithms. (you need security pairwise.)
and so do all the other whatever-million internet users. this means that
you have to invent 100 different encryption algorithms, exchange one
with each of your close friends, program them all into your computers
cryptography 87
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 87


 
 

____________________________________________

SecretsAndLies.pdf page: 116

____________________________________________

yourself (you wouldn’t trust anyone else to do it), and hope you’re
smarter than everyone who might try to break your algorithm.
not bloody likely.
such is the beauty of a key. your front door lock is mass-produced by
some faceless company that hasn’t the faintest idea how valuable your
vintage pez collection is, but you don’t have to trust them. they don’t
say: “remember, anyone else who has the same brand lock can open the
lock.” you have a key. the pin settings inside your lock, which match
your key, make your lock different from all the other locks in the neigh-
borhood, even though they might be exactly the same make and model
number. (actually, the example is simplistic. you do have to trust them
to install the lock correctly, and not to pocket an extra copy of the key.
but never mind that.)
this is the same security model that leon battista alberti, the famous
italian renaissance architect, brought to cryptography in 1466 when he
invented the cryptographic key. everyone can have the same brand lock,
but everyone has a different key. the design of the lock is public—lock-
smiths have books with detailed diagrams, and most of the good designs
are described in public patents—but the key is secret. you have a key, so
you can get in your front door. if you give a key to your friend, he can
get in your front door. someone without a key cannot. (the locksmiths
are the cryptanalysts; we’ll get to them later.)
applying this model to cryptography solves both of the preceding
problems. algorithms, like locks, can be standardized. the data encryp-
tion standard (also called des) has been a standard cryptographic algo-
rithm, worldwide, since 1977. it’s been used in thousands of different
products for all sorts of applications. the innermost workings of des
have been public from day one; they were published even before it was
adopted as a standard. the public nature of the algorithm doesn’t affect
security, because each different group of users chooses its own secret key.
alice and bob share the same key, so they can communicate. eve doesn’t
know the key, so she can’t read their communications—even though she
has a copy of the exact same encryption software that alice and bob have.
keys solve the problem of people moving in and out of a private
group. if alice and bob share a key, and they want to let kim philby join
their conversations, they just give him a copy of the key. if they later learn
that philby is passing secrets to the soviet union, they can simply agree
88 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 88


 
 

____________________________________________

SecretsAndLies.pdf page: 119

____________________________________________

internal to the program and are not displayed on the screen.) if an analyst
can use that known plaintext to recover the key, then she can read the
entire word file. known-plaintext attacks were used to great effect
against the german enigma. analysts would have a single known plain-
text: sometimes it was the daily weather report; for a while, one german
outpost in norway would dutifully send the same message every day:
“nothing to report.” (probable known plaintexts are also called cribs.)
they would use that to break the day’s key, and then use the key to read
the rest of the day’s encrypted messages.
even more powerful is a chosen-plaintext attack. here the analyst gets
to choose the message that will be encrypted. then she gets the encrypted
message and recovers the key. this kind of attack worked against the
german codes: allies would deliberately introduce certain messages into
the system in order to learn the ciphertext, or create events in cities with
obscure names that are particularly useful cribs. it also works well against
some smart card systems, where the attacker can feed arbitrary messages
onto the card. it works in a lot of instances.
the one thing that is constant in all of these attacks is that the analyst
knows the details of the algorithm. (the only modern exception i know
of is the japanese purple code.) this is not just an academic shortcut;
this is good design. if an algorithm is used in products, it will be reverse
engineered. once-secret algorithms that have been reverse engineered
include rc4, all the digital cellular encryption algorithms, the dvd and
divx video-encryption algorithms, and the firewire encryption algo-
rithm. even algorithms buried deep in military hardware will be captured
and reverse engineered: the enigma during world war ii, and just about
every nato and warsaw pact algorithm during the cold war.
(we don’t know those, but the respective militaries do.) it is good design
to assume the enemy knows the details of your algorithm, because even-
tually they will. auguste kerckhoffs first stated this thesis in 1883:
there is no secrecy in the algorithm, it’s all in the key.
recognizing plaintext
one question that often comes up about attacks is: how does the
cryptanalyst recognize plaintext? the answer is simple: because it looks
cryptography 91
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 91


 
 

____________________________________________

SecretsAndLies.pdf page: 121

____________________________________________

integrity. they ensure that the message came from the person from
whom it purports to have come from (authentication), and that the
message was not altered in transit (integrity).
you can think of a mac as a tamperproof coating on a message.
anyone can read the message; the coating doesn’t provide privacy. but
someone who knows the mac key can verify that the message has not
been altered. more specifically, a mac is a number that is appended to a
digital message.
macs use a shared secret key, just like symmetric encryption algo-
rithms. first, alice shares a key with bob. then, when she wants to send
a message to bob, she computes the mac of the message (using the secret
key) and appends it to the message. every message has a unique mac for
each possible key.
when bob receives the message, he computes its mac (again, using
the same shared secret key) and compares it with the mac he received
from alice. if they match, then he knows two things: the message really
does come from alice (or someone who knows the secret of the shared
key), because only that key could be used to compute the mac; and that
the message is complete and unaltered, because the mac could only be
computed from the entire and exact message. if eve (remember our
eavesdropper?) was listening in on the communications, she could read
the message. however, if she tried to modify either the message or the
mac, then bob’s calculated mac would not equal the mac he
received. eve would have to modify the message and then modify the
mac to be correct for the new message, but she can’t do that because she
doesn’t know the key. banks have used this simple authentication system
for decades.
alice can use this same trick to authenticate information stored in a
database. when she adds the information to the database, she calculates
the mac and stores it with the information. when she retrieves the
information, she again calculates the mac and compares it with the
mac stored in the database. if they match, she knows that no one has
modified the information.
macs are used on the internet all the time. they’re used in the ipsec
protocol, for example, to ensure that ip packets have not been modified
between when they are sent and when they reach their final destination.
they’re used in all sorts of interbank transfer protocols to authenticate
messages. most macs are constructed using symmetric algorithms or
cryptography 93
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 93


 
 

____________________________________________

SecretsAndLies.pdf page: 123

____________________________________________

so they can use a symmetric encryption algorithm or a mac function?
public-key cryptography (a.k.a. asymmetric encryption) solves this. it allows you
to send secret messages to people you haven’t met yet, and with whom
you haven’t agreed on a secret key. it allows two people to engage in a
publicly visible data exchange and, at the end of that exchange, compute
a shared secret that someone listening in on the discussion can’t learn. in
real-world terms, it allows you and a friend to shout numbers at each
other across a crowded coffeehouse filled with mathematicians so that
when you are done, both you and your friend know the same random
number, and everyone else in the coffeehouse is completely clueless.
if this sounds ridiculous, it should. it sounds impossible. if you were
to survey the world’s cryptographers in 1975, they would all have told
you it was impossible. so you can imagine the surprise in 1976, when
whitfield diffie and martin hellman explained how to do it. or the sur-
prise in the british intelligence community when james ellis, clifford
cocks, and m.j. williamson figured out the same thing a few years
before.
the basic idea is to use a mathematical function that is easy to com-
pute in one direction and hard to compute in the other. integer factoriza-
tion is one. given two prime numbers, it’s easy to multiply them together
to find the product. but given a single product, it can be impracticable to
factor the number and recover the two factors. this is the kind of math
that can be used to create public-key cryptography; it involves modular
arithmetic, exponentiation, and large prime numbers thousands of bits
long, but you can elide the details. today, there are a good half-dozen
algorithms, with names like rsa, elgamal, and elliptic curves. (algo-
rithms based on something called the knapsack problem were another
early contender, but over the course of about 20 years they were broken
every which way.) the mathematicals are different for each algorithm,
but conceptually they are all the same.
instead of a single key that alice and bob share, there are two keys:
one for encryption and the other for decryption. the keys are different,
and it is not possible to compute one key from the other. that is, if you
have the encryption key, you can’t figure out what the decryption key is.
now, here’s the cool part. bob can create a pair of these keys. he can
take the encryption key and publish it. he can send it to his friends, post
it on his web site, publish it in a phone book, whatever. alice can find
this key. she can take it and encrypt a message to bob. then, she can send
cryptography 95
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 95


 
 

____________________________________________

SecretsAndLies.pdf page: 124

____________________________________________

the message to him. bob can use his decryption key (which he astutely did
not post on his web site) to decrypt and read alice’s message. notice that
alice did not have to meet bob in some dark alley and agree on 
a shared secret. bob doesn’t even have to know alice. actually, alice
 doesn’t even have to know bob. if alice can find bob’s public key, she
can send him a secret message that can’t be read by anyone but bob. this
happens to pgp users all the time; one of their keys is uploaded to a server
somewhere, and then a perfect stranger sends them an encrypted message.
even if you understand the mathematics, it can be startling.
the particulars are a whole lot more subtle. for example, i left out
how bob creates his public and private keys, and how bob keeps his
private key secret. (he can’t remember it; it’s over a thousand random
digits long.) and i skipped over the incredibly complicated problem of
how alice knows that she has bob’s key and not some old key or, worse
yet, some impostor’s key. we’ll get back to this later.
for now, i just want to point out that no one uses public key encryp-
tion to encrypt messages. all operational systems use a hybrid approach
that uses both kinds of cryptography. the reason is performance. what
alice really does, when she wants to send a message to bob, is to use a
symmetric algorithm to encrypt the message with a random key that she
creates out of thin air (called a session key). she encrypts that random key
with bob’s public key, and then sends both the encrypted key and the
encrypted message to bob. when bob receives the encrypted message
and key, he does the reverse. he uses his private key to decrypt the ran-
dom symmetric key, and then uses the random symmetric key to decrypt
the message.
this might sound weird, but it isn’t. it’s perfectly normal. nobody
uses public-key cryptography to directly encrypt messages. everyone uses
this hybrid approach. it’s in every e-mail security program: pgp, pem,
s/mime, whatever. it’s how encryption works with web security,
tcp/ip security, secure telephones, and everything else.
digital signature schemes
public-key encryption was amazing enough, but digital signatures are
even more splendiferous—and more important. digital signatures
provide a level of authentication for messages, similar to macs. and in
96 c h a p t e r  s i x
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 96


 
 

____________________________________________

SecretsAndLies.pdf page: 125

____________________________________________

modern business, authentication is far more important than secrecy.
like public-key encryption, digital signatures use a pair of keys, the
public key and the private key. you still can’t derive one key from the
other. but this time we’re going to reverse them.
alice has a plaintext message. using her private key, she encrypts the
message. because her private key is only hers, only alice’s key can encrypt
the message in precisely this way. thus, the encrypted message becomes
alice’s signature on the message. alice’s public key is public. anyone can
get alice’s public key and decrypt the message, thereby verifying that
alice signed (i.e., encrypted) it. the signature is a function of the message,
so it is unique to the message: a malicious forger can’t lift alice’s signa-
ture from one document and paste it onto another. and it’s a function of
alice’s private key, so it is unique to alice.
of course, real systems are more complicated. just as alice doesn’t
encrypt messages with public-key encryption algorithms (she encrypts a
message key), she also doesn’t sign messages directly. instead, she takes a
one-way hash of a message and then signs the hash. again, signing the
hash is a few orders of magnitude faster, and there are mathematical secu-
rity problems with signing messages directly.
also, most digital signature algorithms don’t actually encrypt the
messages that are signed. the idea is the same, but the mathematics is dif-
ferent. alice makes some calculation based on the message and her private
key to generate the signature. this signature is appended to the message.
bob makes another calculation based on the message, the signature, and
alice’s public key to verify the signature. eve, who doesn’t know alice’s
private key, can verify the signature but cannot forge the message and a
valid signature.
several digital signature algorithms are currently in use. rsa is the
most popular. the u.s. government’s digital signature algorithm
(dsa), used in the digital signature standard (dss), sees a lot of use, too.
elgamal signatures are another you’ll see occasionally. and there are sig-
nature algorithms based on elliptic curve cryptography, which are similar
to all the others but are more efficient in some situations.
although public-key digital signature algorithms are similar to
macs, they are better in one important respect. with a mac, alice and
bob share a secret key that they use to authenticate messages. if alice
receives a message that she verifies, she knows it came from bob. but she
cannot convince a judge of that fact. all a judge can be convinced of is
cryptography 97
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 97


 
 

____________________________________________

SecretsAndLies.pdf page: 127

____________________________________________

several broad classes of random number generators are out there.
some random number generators make use of physical processes that
seem pretty random. the nsa likes to use electrically noisy diodes in its
hardware circuits to create random numbers. other possibilities are
geiger counters and radio-noise receivers. one system on the internet
uses a digital camera focused on a choir of lava lamps. other systems use
the air turbulence in disk drives, or the seemingly random arrival time of
successive network packets.
some random number generators use random movements from the
user. a program might ask the user to type a large string of random char-
acters on the keyboard; it might use the sequence of characters, or even
the timing between successive keystrokes, to create random numbers.
another program might ask the user to make random mouse movements,
or to gargle into a microphone.
some random number generators use these inputs directly. others
use them as seeds for mathematical random number generators. this
process works best when the system needs more random numbers than
the input provides. whatever the source of randomness, the generator
will then generate a series of random bits. these can then be used as cryp-
tographic keys, and for whatever else the system needs.
key length
one of the easiest ways to compare cryptographic algorithms is key
length. the press likes to focus on this because it’s easy to describe and
compare. like most of security, the reality is more complicated. a short
key is bad, but a long key is not automatically good. in the next chapter i
discuss why, but it’s worth explaining key length and its importance.
let’s start at the beginning. a cryptographic key is a secret value that
makes a cryptographic algorithm unique for those who share the key. if
alice and bob share a key, they can use the algorithm to communicate
securely. if eve, an eavesdropper, does not know the key, she is forced to
try and break the algorithm.
one obvious thing she can do is try every possible key. this is called
a brute-force attack. if the key is n bits long, then there are 2n possible keys.
so, if the key is 40 bits long, there are about a trillion possible keys. this
would be impossibly boring for eve, but computers are indefatigable;
cryptography 99
453803_ch06.qxd:453803_ch06.qxd  4/12/13  10:20 am  page 99


 
 

____________________________________________

SecretsAndLies.pdf page: 138

____________________________________________

this protocol works to protect alice, but the bank does not protect
bob against buying a forged title and a stolen car. for that, we need
another protocol:
1. alice gives the title and keys to a lawyer.
2. bob gives the check to the lawyer.
3. the lawyer deposits the check.
4. after waiting a specified time period for the check to clear and for bob to
register the car, the lawyer gives the title to bob. if the check does not
clear within a specified time period, the lawyer returns the title to alice. if
bob cannot get a clean title for the car (because alice gave him a bad title),
bob shows proof of this to the lawyer and gets his money back.
as in the previous protocol, a trusted third party gets involved. in this
case, the trusted third party is a lawyer. alice does not trust bob and bob
does not trust alice, but both trust the lawyer to act fairly in the final step.
the lawyer is completely disinterested in the transaction; he does not care
whether he gives the title to bob or alice. he will keep the money in
escrow and do whatever is required, based on the agreement between
alice and bob.
other protocols are more mundane, and might not involve compli-
cated exchanges. for example, here’s a protocol a bank can use to verify
that a check was signed by alice:
1. alice signs the check.
2. the bank compares the signature on the check with the signature it has on
file for alice.
3. if they match, the bank gives alice her money. if they don’t match, the
bank doesn’t.
in theory, the protocol is secure against bob cheating and getting
alice’s money, but of course reality is more complicated. bob could learn
forgery. the bank could make risky loans in paraguay and go under. alice
could pull a gun. there are probably hundreds of ways to break this pro-
tocol, but given a reasonable set of assumptions on people’s behavior, the
protocol works.
protocols in the digital world are much the same as the preceding
examples. digital protocols use cryptography to do the same sorts of
110 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 110


 
 

____________________________________________

SecretsAndLies.pdf page: 139

____________________________________________

things: keep secrets, authenticate things, enforce fairness, provide audit,
whatever.
the internet is full of security protocols, which i discuss in the next
section. other digital networks have their own security protocols. the
cell phone industry uses a bunch of protocols, both for privacy and fraud
prevention, with varying degrees of success. set-top television boxes have
security protocols. smart cards do, too.
protocols involving digital signatures can be particularly useful in
different authentication situations. for example, digital signature schemes
can produce signatures that only the designated recipient can authenticate.
this is useful for informants or whistle-blowers, since the receiver of the
message can verify who sent it, but cannot prove this fact to a third party.
(think of a secret whispered in your ear. you know who said it, but
there’s nothing you can do to prove to someone else who said it.) digital
signature protocols can be used to sign software so that only a person who
buys the software package legitimately can verify the signature and know
that it is authentic; anyone who pirates a copy can’t be sure of this. we
can create group signatures, so that outside the group each signature
appears to come from the group as a whole, but people inside the group
can determine who signed what.
more complex protocols can make cryptography jump through all
sorts of hoops. we can do something called zero-knowledge proofs,
where alice can prove to bob that she knows something without reveal-
ing to him what it is. cryptographic protocols can also support a system
for simultaneous contract signing over the internet, such that neither
party is bound by the contract unless the other is. we can create the
digital equivalent of certified mail, where alice can’t read the mail unless
she sends back a receipt.
using a protocol called secret sharing, we can enforce requirements
for collusion in access: secrets that cannot be revealed unless multiple people
act in concert. this is a really neat notion. think of a nuclear missile silo.
in order to launch the missile, two people have to simultaneously turn
keys and unlock the system. and the keyholes (or in this case, the digital
equivalent) are far enough apart that a single rogue soldier can’t kill every-
one else and turn all the keys himself: at least two people must act in
concert to launch the missile. or think of a corporate checking account
that requires two signatures on high-value checks: any two of the five
cryptography in context 111
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 111


 
 

____________________________________________

SecretsAndLies.pdf page: 142

____________________________________________

who calls whom and how long they talk, is often just as valuable. in a
military setting, for example, you can learn a lot from traffic analysis: who
talks to whom, at what time, and for how long.
more complex attacks are known as active attacks: inserting, deleting,
and changing messages. these can be much more powerful.
consider a smart card digital cash system. people put money onto the
cards, and then use the cards to buy things. this system will have a lot of
different protocols: protocols for adding money onto the card, protocols
for transferring money from the card to another device, protocols for
querying the card, and so on.
active attacks can do a lot of damage here. maybe you can manipu-
late the protocol between the bank and the card that adds money onto the
card. if you can replay old messages, you can add more money onto the
card. or maybe you can delete a message in the protocol for transferring
money out of the card when you buy something, so that the money never
gets decremented from the card.
one powerful attack is the man-in-the-middle attack. alice wants to
talk securely with bob, using some public-key algorithm to establish 
a key. eve, the eavesdropper, intercepts alice’s communication. she pre-
tends to be someone named bob to alice, completing the key-exchange
protocol. then she contacts bob and pretends to be alice, completing a
second key-exchange protocol with bob. now she can eavesdrop on the
communications. when alice sends a message to bob, eve intercepts it,
decrypts it, re-encrypts it, and sends it on to bob. when bob sends a mes-
sage to alice, eve performs a similar procedure. this is a powerful attack.
of course, good protocol designers take these attacks into account
and try to prevent them. better communications protocols don’t permit
man-in-the-middle attacks, and certainly don’t allow eavesdropping of
passwords. better electronic commerce protocols don’t allow malicious
users to arbitrarily add cash to smart cards. but people make mistakes, and
lots of protocols have problems.
and again, it’s not always apparent what kinds of attacks need to 
be prevented. there was a public-key authentication protocol that
appeared in the literature, designed so users could authenticate themselves
to hosts. the protocol was made secure against passive eavesdropping
attacks and against active insertion/deletion attacks. as it turned out, the
protocol was not secure against a malicious host. alice could authenticate
114 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 114


 
 

____________________________________________

SecretsAndLies.pdf page: 143

____________________________________________

herself to a host, and no eavesdropper could masquerade as alice. but the
host could.
this is an interesting attack. in some circumstances, the host is
assumed to be trusted and this is not a problem. in others, it is. we can
certainly imagine malicious hosts on the web. if an online bank used this
protocol (as far as i know, none does), a criminal could set up a phony
bank web site with a slightly different url. an unsuspecting user could
authenticate himself to this phony site, which could then masquerade as
the user to the bank.
a lot of this has been formalized. there are automatic tools for
analyzing protocols: formal logics, computer programs that examine the
details of protocols, and others. these tools are useful, and regularly find
security problems in existing protocols, but cannot be used to “prove” the
security of a protocol.
choosing an algorithm or protocol
choosing a cryptographic algorithm or protocol is difficult because there
are no absolutes. we can’t compare encryption algorithms the way we
can compare compression algorithms. compression is easy: you can
demonstrate that one algorithm compresses better—faster, smaller, what-
ever—than another. security is hard; while you can show that a particu-
lar algorithm is weak, you can’t show that one algorithm you don’t know
how to break is more secure than another. in the absence of absolutes, we
use the evidence we have: expert consensus.
the problem can be best illustrated with a story. suppose your
doctor said, “i realize we have antibiotics that are good at treating your
kind of infection without harmful side effects, and that decades of research
support this treatment. but i’m going to give you a pulverized pretzel
instead, because, um, it might work.” you’d get a new doctor.
practicing medicine is difficult. the profession doesn’t rush to
embrace new drugs; it takes years of testing before benefits can be proven,
dosages established, and side effects cataloged. a good doctor won’t treat
a bacterial infection with a medicine she just invented when proven
antibiotics are available. and a smart patient wants the same drug that
cured the last person, not something different.
cryptography in context 115
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 115


 
 

____________________________________________

SecretsAndLies.pdf page: 146

____________________________________________

• triple-des, which has been analyzed by pretty much everyone in the
cryptographic community since the mid-1970s.
• aes, which (when it is chosen) will be the result of a three-year public
selection process that involved pretty much everyone in the cryptographic
community.
• algorithm x, which was published at an academic conference two years
ago; there’s been one analysis paper published that seems to imply that it is
strong.
• algorithm y, which someone recently posted on the internet and assures
you is strong.
• algorithm z, which a company is keeping secret until the patent issues;
maybe they paid a couple of cryptographers to analyze it for three weeks.
this isn’t a hard choice. there may be performance constraints that
prevent you from choosing the algorithm you want (the primary reason
aes exists is that triple-des is too slow for many environments), but the
choice is acutely clear.
it continuously amazes me how often people don’t make the obvious
choice. instead of using public algorithms, the digital cellular companies
decided to create their own proprietary ones. over the past few years, all
the algorithms have become public. and once they became public, they
have been broken. every one of them. the same thing has happened to
the dvd encryption algorithm, the firewire encryption algorithm, vari-
ous microsoft encryption algorithms, and countless others. anyone who
creates his or her own cryptographic primitive is either a genius or a fool.
given the genius/fool ratio for our species, the odds aren’t very good.
the counter-argument you sometimes hear is that secret cryptogra-
phy is stronger because it is secret, and public cryptography is riskier
because it is public. this sounds plausible, but when you think about it for
a minute, the dissonance becomes obvious. public primitives are designed
to be secure even though they are public; that’s how they’re made. so
there’s no risk in making them public. if a primitive is only secure if it
remains secret, then it will only be secure until someone reverse engineers
and publishes it. proprietary primitives that have been “outed” include all
the algorithms in the preceding paragraph, various smart card electronic-
commerce protocols, the secret hash function in securid cards, and the
protocol protecting motorola’s mobile mdc-4800 police data termi-
nal.
118 c h a p t e r  s e v e n
453803_ch07.qxd:453803_ch07.qxd  4/12/13  10:33 am  page 118


 
 

____________________________________________

SecretsAndLies.pdf page: 157

____________________________________________

disk, how do we prevent one user reading what another user writes?
what if one user wants another user to read what she writes? is it possible
for a user to use interrupts to do something he shouldn’t? how can we
secure access to the printer? can one person eavesdrop on another via the
keyboard? what if the trusted computing base crashes? how do you
implement a disk defragmenter if you can only access your own files?
the historical example that got this the most nearly correct is an oper-
ating system called multics, developed in the late 1960s by mit, bell
labs, and honeywell. multics implemented the bell-lapadula model
from the ground up. (in fact, the multics project was the impetus for the
bell-lapadula model.) the designers used the mathematical formalism of
the model to show the security of the system, and then mapped the con-
cepts of the model into the operating system. no code was ever written
until specifications had been approved. multics worked, although the
security was way too cumbersome. by now, almost everyone has forgot-
ten multics and the lessons learned from that project.
one of the lessons people have forgotten is that the kernel needs to
be simple. (even the multics kernel, with only 56,000 lines of code, was
felt to be too complex.) the kernel is defined as the software that is
trusted. chapter 13 talks about software reliability, the moral being that it
is unreasonable to expect software not to have security bugs. the simpler
the software is, the fewer bugs it will have.
unfortunately, modern operating systems are infected with a disease
known as “kernel bloat.” this means that a lot of code is inside the ker-
nel instead of outside. when unix was first written, it made a point of
pushing nonessential code outside the kernel. since then, everyone has
forgotten this lesson. all current flavors of unix have some degree of
kernel bloat: more commands inside the kernel, inexplicable utilities
running with root permissions, and so forth.
windows nt is much worse. the operating system is an example of
completely ignoring security lessons from history. things that are in the
kernel are defined as secure, so smart engineering says to make the kernel
as small as possible, and make sure everything in it is secure. windows
seems to take the position that since things in the kernel are defined as
secure, than you should put everything in the kernel. when they can’t
figure out how to secure something, they just put it into the kernel and
define it as secure. obviously, this doesn’t work in the long run.
computer security 129
453803_ch08.qxd:453803_ch08.qxd  4/12/13  10:42 am  page 129


 
 

____________________________________________

SecretsAndLies.pdf page: 163

____________________________________________

9
identification 
and authentication
no matter what kind of computer security system you’re using,
the first step is often identification and authentication: who are
you, and can you prove it? once a computer knows that, it can
figure out what you are and are not allowed to do. in other words, access
control can’t start until identification and authentication is finished.
let’s talk about the problem. alice has some ability on a computer,
and we want to make sure that only she has that ability. sometimes the
ability is access to some information: files, account balances, and so forth.
sometimes the ability is access to the entire computer; no one else can
turn the computer on and use her data or programs. sometimes the abil-
ity is more explicit: withdraw money from an atm, use a cell phone,
stop a burglar alarm from ringing. sometimes the ability is on a web site:
access to her calendar or her brokerage account, for example. sometimes
the ability is access to a cryptographic key that is just too large for her to
remember. (pgp uses access control measures to protect private keys.) it
doesn’t matter what the ability is; what’s important is that some access
control measure is required to identify alice.
actually, the access control measure has to do two things. one, it has
to allow alice in. and two, it has to keep others out. doing only one is
easy—an open door will let alice, and everyone else, in; a bricked-over
door will keep others, as well as alice, out—but doing both is harder. we
135
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 135


 
 

____________________________________________

SecretsAndLies.pdf page: 164

____________________________________________

need something that will recognize alice and let her in, but will be hard
for others to duplicate. we need to be able to identify alice and then
authenticate that identification. (actually, the access control measure has
to do a third thing: keep a good audit record of what happened.)
traditionally, identification and authentication measures have cen-
tered on one of three things: something you know, something you are, or
something you have. these roughly translate to “passwords,” “biomet-
rics,” and “access tokens.” sometimes systems use two of these things
together. paranoid systems use all three.
passwords
the traditional approach to authentication is a password. you see it
everywhere. when you log on to a computer system, you type in a user-
name and password. to make a telephone call using a calling card, you
type in your account number and password (often, it is given as a single
string). to withdraw money from an atm, you put your card in the slot
and type in your pin (a password).
the two steps in each of those examples mirror the title of this chap-
ter. the first step is called identification: you tell the computer who you
are (the username). the second step is called authentication: you prove
to the computer that you are who you say you are (the password).
the computer at the other end of these transactions has a list of user-
names and passwords. once you have entered in your username and pass-
word (or your account number and pin), the computer compares your
input against the entries stored on the list. if you enter a valid username
and the correct corresponding password, you’re in. if you don’t, you’re
out. sometimes the system will again prompt you for a username and
password. sometimes the system will lock up after a certain number of
bad attempts. (you wouldn’t want someone to be able to steal an atm
card and then try all 10,000 possible pins, one after another, in an
attempt to find the correct one.)
unfortunately the system of username and password works less well
than people believe.
the whole notion of passwords is based on an oxymoron. the idea is
to have a random string that is easy to remember. unfortunately, if it’s
easy to remember, it’s something nonrandom like “susan.” and if it’s
random, like “r7u2*qnp,” then it’s not easy to remember.
136 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 136


 
 

____________________________________________

SecretsAndLies.pdf page: 165

____________________________________________

in chapter 7, where i talked about key length and security, i
discussed the problems of user-generated and user-remembered keys. a
password is a form of user-remembered key, and dictionary attacks against
passwords are surprisingly effective.
how does this attack work? think about an access control system for
a computer or web site. the computer has a file of usernames and pass-
words. if an attacker got her hands on that file, she would learn every
password. in the mid-1970s, computer security experts came up with a
better solution: instead of storing all the passwords in a file, they would
store a cryptographic hash of the password. now, when alice types her
password into the computer or web site, the software computes the hash
of the password and compares that hash with the hash stored in a file. if
they match, alice is allowed in. now there is no file of passwords to steal;
there is only a file of hashed passwords to steal. and since a hash function
prevents someone from going backward, the attacker can’t recover the
passwords from the hashed passwords.
here’s where dictionary attacks come in. assume that an attacker has
a copy of the hashed password file. he takes a dictionary, and computes
the hash of every word in the dictionary. if the hashed word matches any
of the password entries, then he has found a password. after he tries all
words, he tries reversed dictionary words, dictionary words with some
letters capitalized, and so forth. eventually he tries all character combina-
tions shorter than some length.
dictionary attacks used to be hard, because computers were slow.
they’re much easier now, because computers are a lot faster. l0phtcrack
is an example of a password recovery hacker tool that is optimized for
windows nt passwords. windows nt contains two password func-
tions: a stronger one designed for nt, and a weaker one that is backward-
compatible with older networking login protocols. the weaker one is
case-insensitive, and passwords can’t be much stronger than seven charac-
ters (even though they may be longer). l0phtcrack makes easy work of
this password space. on a 400-mhz quad pentium ii, l0phtcrack can
try every alphanumeric password in 5.5 hours, every alphanumeric pass-
word with some common symbols in 45 hours, and every possible
keyboard password in 480 hours. this is not good.
some have dealt with this problem by requiring stronger and stronger
passwords. what this means is that the password is harder to guess, and less
likely to appear in a password dictionary. the old racf mainframe sys-
tem required users to change passwords monthly, and wouldn’t permit
identification and authentication 137
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 137


 
 

____________________________________________

SecretsAndLies.pdf page: 167

____________________________________________

and they’ll choose the same password for multiple applications. want
to steal a bunch of passwords? put up a web site with something interest-
ing on it: porn, hockey scores, stock tips, or whatever will appeal to the
demographic you’re after. don’t charge for it, but make people register a
username and password in order to see the information. then, sit back
and collect usernames and passwords. most of the time you’ll get the same
username and password that the user chose last time, maybe the one that
lets you into his bank or brokerage accounts. save incorrect passwords as
well; people sometimes enter the password for system a into system b by
mistake. make the user fill out a little questionnaire during registration:
“what other systems do you use regularly? bank x? brokerage firmy?
news service z?” a researcher i know did something like this in 1985;
he got dozens of system administrator passwords.
and even when they choose good passwords and change them regu-
larly, people are much too willing to share their passwords with others in
and out of the organization, especially when they need help to get the
work done. clearly this represents one of the greatest security risks of all,
but, in people’s minds, the risk is minimal and the need to get work done
imperative.
this is not to say that there are not better or worse passwords. the
preceding example pgp passphrase is still secure against dictionary attacks.
generally, the easier a password is to remember, the worse it is. dictio-
nary attacks generally try common passwords before uncommon ones:
dictionary words, reversed dictionary words, dictionary words with some
letters capitalized, dictionary words with minor modifications—like the
number “1” instead of the letter “l”—and so forth.
unfortunately, many systems are only as secure as the weakest pass-
word. when an attacker wants to gain entry into a particular system, she
might not care which account she gets access to. in operational tests,
l0phtcrack recovers about 90 percent of all passwords in less than a day,
and 20 percent of all passwords in a few minutes. if there are 1,000
accounts, and 999 users choose amazingly complicated passwords that
l0phtcrack just can’t possibly recover, it will break the system by recov-
ering that last weak ordinary password.
on the other hand, from the user’s point of view this can be an exam-
ple of “not having to outrun the bear; only having to outrun the people
you’re with.” any dictionary attack will succeed against so many accounts
whose passwords are “susan” that if your password is “hammerbutterfly,”
identification and authentication 139
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 139


 
 

____________________________________________

SecretsAndLies.pdf page: 170

____________________________________________

who signed it. your photograph identifies you as the person who owns a
particular passport.
for most applications, biometrics need to be stored in a database like
passwords. alice’s voice only works as a biometric identification on the
telephone if you already know who she is; if she is a stranger, it doesn’t
help. it’s the same with alice’s handwriting; you can recognize it only if
you already know it. to solve this problem, banks keep signature cards on
file. alice signs her name on a card when she opens her account, and it is
stored in the bank (the bank needs to maintain its secure perimeter in
order for this to work right). when alice signs a check, the bank verifies
alice’s signature against the stored signature to ensure that the check is
valid. (in practice, that rarely happens. manual signature checking is so
costly that the bank doesn’t bother checking for amounts less than about
$1,000. if there is a problem, they assume, someone will complain. and
making good on the occasional problem is cheaper than paying someone
to do the checking.) you could do the same thing with alice’s voice—
compare her voiceprint to the one stored in some central database.
the exceptions are situations where the biometric is only verified as
part of an involved and uncommon protocol. when alice signs a con-
tract, for example, bob does not have a copy of her signature on file. the
protocol still works because bob knows that he can verify the signature at
some later time, if necessary.
there are many different types of biometrics. i’ve mentioned hand-
writing, voiceprints, face recognition, and fingerprints. there is also hand
geometry, typing patterns, retinal scans, iris scans, signature geometry (not
just the look of the signature, but the pen pressure, signature speed, and
so forth), and others. the technologies behind some of them are more
reliable than others—fingerprints are much more reliable than face recog-
nition—but that may change as technology improves. some are more
intrusive than others; one failed technology was based on lip pattern, and
required the user to kiss the computer. as a whole, biometrics will only
get better and better.
“better and better” means two different things. first, it means that it
will not incorrectly identify an impostor as alice. the whole point of the
biometric is to prove that the claimant alice is the actual alice, so if an
impostor can successfully fool the system, it isn’t working very well. this
is called a false positive. second, it means that the system will not incor-
rectly identify alice as an impostor. again, the point of the biometric is to
142 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 142


 
 

____________________________________________

SecretsAndLies.pdf page: 171

____________________________________________

prove that alice is alice, and if alice can’t persuade the system that she is
herself, then it’s not working very well, either. this is called a false
negative.
over the years, biometric identification systems have gotten better at
detecting both false positives and false negatives. for example, they
include checks for liveness, so that neither a plastic finger nor a severed
real finger fools the fingerprint reader. they do a better job of correcting
for day-to-day variations in an individual’s biometric better. they’re just
easier to use.
in general, you can tune a biometric system to err on the side of a false
positive or a false negative. this is all shades of gray here; if the system gets
a fingerprint that it is pretty sure belongs to alice, does it let the finger in?
it depends on whether the system is more concerned with false positives
or false negatives. if the system is authorizing alice to take pencils out of
a stockroom, then it should err on the side of false negatives; it’s much
worse to annoy a legitimate user than to lose a few pencils. if the system
is protecting large amounts of money, then false positives are preferable:
keeping unauthorized users out is more important than occasionally
denying access to a legitimate user. if the system initiates a launch
sequence for nuclear missiles, both are dire.
biometrics are great because they are really hard to forge: it’s hard to
put a false fingerprint on your finger, or make your retina look like some-
one else’s. some people can do others’ voices (performers who do imita-
tions, for example), and hollywood can make people’s faces look like
someone else, but in general those biometrics are hard to forge, too.
on the other hand, biometrics are lousy because they are so easy to
forge: it’s easy to steal a biometric after the measurement is taken. in all of
the applications discussed previously, the verifier needs to verify not only
that the biometric is accurate but that it has been input correctly. imagine
a remote system that uses face recognition as a biometric. “in order to
gain authorization, take a polaroid picture of yourself and mail it in. we’ll
compare the picture with the one we have in file.” what are the attacks
here?
easy. to masquerade as alice, take a polaroid picture of her when
she’s not looking. then, at some later date, use it to fool the system. this
attack works because while it is hard to make your face look like alice’s,
it’s easy to get a picture of alice’s face. and since the system does not ver-
identification and authentication 143
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 143


 
 

____________________________________________

SecretsAndLies.pdf page: 172

____________________________________________

ify that the picture is of your face, only that it matches the picture of
alice’s face on file, we can fool it.
similarly, we can fool a signature biometric using a photocopier or a
fax machine. it’s hard to forge the vice president’s signature on a letter
giving you a promotion, but it’s easy to cut his signature out of another
letter, paste it on the letter giving you a promotion, and fax it to the
human resources department. they won’t be able to tell that the signature
was cut from another document.
the moral is that biometrics work great only if the verifier can verify
two things: one, that the biometric came from the person at the time of
verification, and two, that the biometric matches the master biometric on
file. if the system can’t do both, it is insecure.
here’s another possible biometric system: thumbprints for remote
login authorizations. alice puts her thumbprint on a reader embedded
into the keyboard (don’t laugh, a lot of companies want to make this hap-
pen, and the technology already exists). the computer sends the digital
thumbprint to the host. the host verifies the thumbprint and lets alice in
if it matches the thumbprint on file. this won’t work because it’s so easy
to steal alice’s digital thumbprint, and once you have it, it’s easy to fool
the host, again and again.
tamper-resistant hardware helps (within the limitations of chapter
14), as long as the tamper-resistant hardware includes both the biometric
reader and the verification engine. it doesn’t work if a tamper-resistant
fingerprint reader sends the fingerprint data across an insecure network.
encryption can help, too, though.
anyway, this brings us to the second major problem with biometrics:
it doesn’t handle failure well. imagine that alice is using her thumbprint
as a biometric, and someone steals it. now what? this isn’t a digital cer-
tificate (we’ll get to those in chapter 15), where some trusted third party
can issue her another one. this is her thumb. she only has two. once
someone steals your biometric, it remains stolen for life; there’s no getting
it back.
this is why biometrics don’t work as cryptographic keys (even if you
could solve the fuzzy biometric logic versus absolute mathematical logic
problem). occasionally i see systems that use cryptographic keys gener-
ated from biometrics. this works great, until the biometric is stolen. and
i don’t mean that the person’s finger is physically cut off, or the fingerprint
is mimicked on someone else’s finger; i mean that someone else steals the
144 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 144


 
 

____________________________________________

SecretsAndLies.pdf page: 173

____________________________________________

digital fingerprint. once that happens, the system does not work any-
more. (well, maybe it will work until all ten fingers are stolen. . . .)
biometrics can be good authentication mechanisms, but they need to
be used properly.
access tokens
the third solution to proving identity is to use something you have: a
physical token of some sort. this is an old form of access control: a phys-
ical key restricted access to a chest, a room, a building. possession of the
king’s seal authorized someone to act on his behalf. more modern systems
can be automated—electronic hotel room keys—or manual—corporate
badges that allow access into buildings. the basic idea is the same; a phys-
ical token serves to authenticate the holder of it.
there are several basic ways this can be done. most simply, the holder
can simply prove that he is holding the token. computers that require a
physical key to turn them on work in this manner; so do computers that
require a smart card. the basic idea is that you insert the token into some
slot somewhere, and then the computer verifies that it is really there. if it
is, you’re in.
the most serious problem with this system is that tokens can be
stolen. if someone steals your house keys, for example, she can unlock
your house. so the system doesn’t really authenticate the person; it
authenticates the token. most computer systems combine access tokens
with passwords—sometimes called pins—to overcome this vulnerability.
you can think of bank atm cards. the atm authenticates the card, and
also asks for a pin to authenticate the user. the pin is useless without the
access token. some cellular phone systems work the same way: you need
the physical phone and an access code to make calls on a particular cellu-
lar account.
in addition to stealing a token, someone can copy it. some tokens can
be easily copied—physical keys, for example—so they can be stolen,
copied, and replaced without the owner knowing about it.
another problem is that there needs to be some authenticated way of
determining that the token is really there. think of a token as a remov-
able, changeable biometric, and you’ve got all the problems of a secure
identification and authentication 145
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 145


 
 

____________________________________________

SecretsAndLies.pdf page: 174

____________________________________________

verification path from the previous section. at least the token can be
changed if necessary.
this problem can be illustrated using credit cards. it’s difficult to forge
a physical credit card, which makes it risky to use a forged credit card to
purchase things at a store. the clerk might notice that the card is forged.
it’s far easier to use a forged credit card over the telephone, however. at
the store, the clerk authenticates both the account number on the credit
card and the credit card itself—the token. over the phone, the operator
cannot authenticate the physical token, only the account number.
there’s another, relatively minor, problem that shows up with some
tokens. if users can leave the token in the slot, they often do. if the users
need to have a smart card inserted in a slot before it will boot, they’re
likely to leave the smart card there all day and night . . . even when
they’re not there. so much for authentication.
all of this discussion assumes that there’s some kind of reader associ-
ated with the token, and the user can insert the token into the reader. this
often isn’t the case: most computers don’t have the required reader, or the
system might have to work for mobile users who could be sitting some-
where other than at their normal computers. two different technologies
deal with this situation.
the first is challenge/reply. the token is a pocket calculator, with a
numeric keypad and small screen. when the user wants to log in, the
remote host presents him with a challenge. he types that challenge into
his token. the token calculates the appropriate reply, which he types into
the computer and sends to the host. the host does the same calculation;
if they match, he is authenticated. the second technology is time-based.
this token is the same pocket calculator, with just a screen. the numbers
on the screen change regularly, generally once per minute. the host asks
the user to type in what is showing on his screen. if it matches what the
host expects, he is authenticated. the securid token works this way.
of course, the full system also includes a password—the challenge/
reply token might even require a second password to get it working—and
there are other, ancillary, security measures. the basic idea, though, is that
some secret calculation is going on inside the token that can’t be imper-
sonated. an attacker can’t pretend to have the token, because she doesn’t
know how to calculate replies based on challenges, or doesn’t know how
146 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 146


 
 

____________________________________________

SecretsAndLies.pdf page: 175

____________________________________________

to calculate values based on the time. the only way to do this is to actu-
ally have the token.
this works, more or less. cryptographic techniques, encrypting or
hashing, provide the security. the host knows how to do the calculations,
so the system is only as secure as the host’s source code. anyone who can
reverse engineer the token can figure out how to do the calculations, so
the system is only as secure as the tokens (see chapter 14). but it’s pretty
good, and certainly a lot better than passwords alone. the security prob-
lems arise in the network, and the authenticating computer.
one last token needs discussion: the password, written down. there
is a knee-jerk reaction to writing passwords down in the security com-
munity, but if done properly this can improve security considerably.
someone who writes his password down turns something he knows (the
password) into something he has (the piece of paper). this trick does
allow him to use longer passwords, which can make passwords actually
secure again. it does have all the problems of a simple token: it can be
copied or stolen. it doesn’t work if alice writes her password on a yellow
sticky attached to her monitor. much better is for her to put her pass-
words in her wallet; this can be secure. probably the best solution is to
have two parts to the password: one part remembered by alice, and the
other part written down in her wallet.
similarly, there are systems of one-time passwords. the user has a list
of passwords, written down, and uses each one once. this is certainly a
good authentication system—the list of passwords is the token—as long as
the list is stored securely.
authentication protocols
authentication protocols are cryptographic ways for alice to authenticate
herself across a network. the basic authentication protocol is pretty sim-
ple:
1. alice types in her username and password on the client. the client sends
this information to the server.
2. the server looks up alice’s username in a database and retrieves the corre-
sponding password. if that password matches the password alice typed,
alice is allowed in.
identification and authentication 147
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 147


 
 

____________________________________________

SecretsAndLies.pdf page: 176

____________________________________________

the problem with this is that the password database has to be pro-
tected. the solution is to not store the passwords, but to store hashes of
the passwords:
1. alice types in her username and password on the client. the client sends
this information to the server.
2. the server hashes alice’s typed-in password.
3. the server looks up alice’s username in a database and retrieves the corre-
sponding password hash. if that password hash matches the hash of the
password alice typed, alice is allowed in. 
better. the main problem with the second protocol is that passwords
are sent over the network in the clear. anyone sniffing the network can
collect usernames and passwords. solutions involved hashing passwords
before sending them (older versions of windows nt did this), but dic-
tionary attacks can deal with that as well.
as dictionary attacks became more powerful, systems started adding
salt to their passwords. (actually, they did this very early, a good example
of designer foresight.) a salt is a known random constant hashed with the
password. the effect is to make dictionary attacks harder; instead of a sin-
gle hash for the password “cat,” there would be 4,096 different hashes for
“cat” plus 12 bits of random salt. dictionaries of prehashed passwords
would have to be four thousand times larger. but the ability to do fast dic-
tionary attacks in real time makes this countermeasure obsolete; the dic-
tionary simply includes all possible salt values.
kerberos is a more complicated authentication protocol. to make
this work, alice has to share a long-term key with a secure server on the
network, called a kerberos server. to log on to a random server on the
network, which we’ll call the bob server, the following procedure is car-
ried out:
1. alice requests permission from the kerberos server to log on to the bob
server.
2. the kerberos server checks to make sure alice is allowed to log on to the
bob server. (note that the kerberos server does not need to know that
alice is who she says she is. if she isn’t, the protocol will fail in step 6.)
3. the kerberos server sends alice a “ticket” that she is supposed to give to
the bob server, and a session key she can use to prove to bob that she is
alice.
4. alice uses the session key from the kerberos server to create an “authenti-
148 c h a p t e r  n i n e
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 148


 
 

____________________________________________

SecretsAndLies.pdf page: 177

____________________________________________

cator” that she will use to prove to bob that she is alice.
5. alice sends bob both the ticket and the authenticator.
6. bob validates everything. if it all checks out, he lets alice in. (bob also
shares a long-term key with the kerberos server. the ticket is a message
from the server encrypted in bob’s long-term key.) 
this protocol is secure in the same way that physical ticket protocols are
secure. the kerberos server prints tickets. it gives alice a ticket that she
can present to bob. bob can validate the ticket, so he knows that alice
received it from the kerberos server.
this protocol has some nice properties. the long-term secrets of
alice and bob, which are kind of like passwords, are never sent through
the network. on the minus side, this system needs a kerberos server to
operate; the kerberos server is a trusted third party. this can mean a bot-
tleneck in the system at 9:00 in the morning, when everyone is trying to
log on to their computer.
kerberos was invented at mit in 1988, and has been used in the
unix world ever since. kerberos is part of windows 2000, but
microsoft’s implementation differs from the standard and is incompatible
with the rest of the kerberos world. i can only assume this was done for
deliberate marketing reasons (at this writing, microsoft only allowed you
to open the file with the modification details if you first clicked on a
screen agreeing to treat the information as proprietary, so third-party
developers can’t build interoperable systems), but it makes for bad secu-
rity. you can’t just modify a security protocol and assume that the modi-
fied protocol is also secure.
other, more byzantine, login authentication protocols use public-
key cryptography. ipsec and ssl, for example, use public-key authenti-
cation protocols. some systems use simple, but esoteric, protocols. the
protocol by which a cell phone proves that it should be allowed to make
telephone calls in a particular network is one of these.
single sign-on
one thing that has annoyed computer users in large secure environments
is the large number of passwords. users might have to type in one pass-
word to log on to their computers, another to log on to the network, a
third to log on to a particular server on the network, and so on and on and
identification and authentication 149
453803_ch09.qxd:453803_ch09.qxd  4/12/13  10:52 am  page 149


 
 

____________________________________________

SecretsAndLies.pdf page: 185

____________________________________________

code-signing system have used this idea.) they can do just about anything
you can think of, and a lot of things you’d never think of. the distributed
denial-of-service attacks on the internet first use trojan horses to infect
intermediate computers.
the hard part of these attacks is getting the trojan horse onto the
computer of some unsuspecting victim. you can break into the victim’s
office and install it on her computer; in the next chapter, we’ll talk about
some defenses against that sort of attack. you can cajole her to install the
trojan herself; we’ll talk about social engineering in chapter 17. you can
attack the victim’s computer via the network; we’ll talk about that in
chapter 11. or you can use the malicious software itself to attack the
computer, creating a virus.
modern malicious code
the year 1999 was a pivotal year for malicious software. the different
strains—viruses, worms, and trojan horses—blurred and amalgamated.
and malware has gotten nastier. malware that automatically propagates
over e-mail is not new—christma.exec in 1987 (through the profs 
e-mail system) and sharefun in 1997—but 1999 was the first year that 
e-mail-propagating malware infected large swaths of the internet. this
strain of malware ignores corporate defenses and tunnels right through
firewalls. this is a really big deal.
viruses survive by reproducing on new computers. before the inter-
net, computers communicated mostly through floppy disks. hence, most
viruses propagated on floppy disks, and occasionally on computer bulletin
board systems.
there are some ramifications of floppies as a vector. first, malware
propagates relatively slowly. one computer shares a disk with another,
which shares a disk with five more, and over the course of weeks or
months a virus turns into an epidemic. or maybe someone puts a virus-
infected program on a bulletin board, and thousands get infected in a
week or two.
second, it’s easy to block disk-borne malware. most antivirus pro-
grams can automatically scan all floppy disks. malware is blocked at the
gate. bulletin boards can still be a problem, but many computer users are
trained never to download software from an untrusted bulletin board.
even so, antivirus software can automatically scan new files for malware.
networked-computer security 157
453803_ch10.qxd:453803_ch10.qxd  4/12/13  11:04 am  page 157


 
 

____________________________________________

SecretsAndLies.pdf page: 205

____________________________________________

of choosing a common protocol that is widely attacked by hackers, and
hence whose security is constantly improving, or one that is obscure and
little-known, and is possibly even less secure. keep that question in mind
while reading this chapter.
how networks work
computer networks are bunches of computers connected to each other.
that is, either physical wires run between computers—wires in an office
lan, dedicated phone lines (possibly isdn or dsl), dial-up connec-
tions, fiber optic, or whatever—or there is an electromagnetic connec-
tion: radio links, microwaves, and so forth.
simply, when one computer wants to talk to another, it creates a
message, called a packet, with the destination computer’s name on it and
sends it to the computer over this network. this is fundamentally unlike
telephone conversations. when alice wants to call bob, she tells the
phone company’s computer network bob’s network name (commonly
known as his telephone number) and the network hooks up different
communications circuits—copper wire, satellite, cellular, fiber, what-
ever—to make an unbroken connection. alice and bob talk through this
circuit until one of them hangs up. then, the telephone network disas-
sembles this connection and lets other people use the same pieces for
other phone calls. the next time alice calls bob, they will be connected
through a completely different set of links. (well, mostly different; the
line between the telephones and the first switches will be the same.)
computers don’t use circuits to talk to each other. they don’t have
conversations like people do—they send short data packets back and
forth. these packets are broken-up pieces of anything: e-mail messages,
gifs of naked ladies, streaming audio or video, internet telephone calls.
computers divide large files into packets for easier transmission. (think of
a ten-page letter being divided up and mailed in ten different envelopes.
at the recipient’s end, someone opens all the envelopes and reassembles
the letter in its proper order. the packets don’t have to arrive in order,
and they don’t have to travel along the same route to their destination.)
these packets are sent through the network by routers. there are
bunches of protocols—ethernet, tcp, whatever—but they all work
basically (for large values of “basically”) the same way. routers look at the
network security 177
453803_ch11.qxd:453803_ch11.qxd  4/12/13  1:03 pm  page 177


 
 

____________________________________________

SecretsAndLies.pdf page: 218

____________________________________________

attacker will just go around the firewall and attack some undefended
connection.
and fourth point: castles need gates. it’s futile and absurd to build a
castle that can’t be penetrated by anyone under any circumstances: even
kings need to go outside and perambulate sometimes. merchants, mes-
sengers, even common townsfolk need to be able to go in and out regu-
larly. hence, castles had gatekeepers whose job it was to admit or turn
away people who wanted to enter the castle.
the great wall of china didn’t impress genghis khan. “the
strength of a wall depends on the courage of those who defend it,” he
supposedly said. letting the good stuff in while keeping the bad stuff out
is the central problem that any computer firewall needs to solve. it has to
act as gatekeeper. it has to figure out which bits are harmful and deny
them entry. it has to do this without unreasonably delaying traffic. (and
to your average internet user, an unreasonable delay is defined as one that
is noticeable.) it has to do this without irritating legitimate users. (your
average internet user will not tolerate not being able to do something, like
downloading a new internet game from suspicious software™ or con-
necting remotely and reading e-mail from an untrusted machine.) but if
the firewall’s gatekeeper makes a mistake, some hacker can sneak in and
own the network.
there are three basic ways to defeat a firewall. the first i talked about:
go around it. a large network has lots of connections. large photocopiers
often come with internet connections, and some network equipment
comes with dial-up maintenance ports. companies often hook their net-
works to the networks of suppliers, customers, and so forth; sometimes
those networks are much less protected. employees will hook personal
modems up to their computers so they can work at home. there’s a story
of a married couple in silicon valley who occasionally worked from
home. he was checking his e-mail while his wife was doing some pro-
gramming, both of them on their small home network. suddenly, his
company’s computers started showing up on her company’s network and
vice versa.
the second, and more complicated attack, is to sneak something
through the firewall. to do this, you have to fool the firewall into think-
ing you are good, honorable, and authorized. depending on how good
the firewall is and how well it has been installed, this is either easy,
difficult, or next to impossible. 
190 c h a p t e r  t w e l v e
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 190


 
 

____________________________________________

SecretsAndLies.pdf page: 223

____________________________________________

ing: “that looks like a sucking chest wound. i’d get that checked if i were
you.” an ids is not a substitute for good proactive security.
okay then, what do idss do? they alert you of a successful attack, or
maybe even an attack in progress. the good ones are accurate in both
senses: they don’t cry wolf and claim an attack where there is none, and
they don’t miss real attacks. the good ones are timely: they alert you of
the attack while it is still going on. the good ones give some kind of diag-
nosis—what the attack is and where it is coming from—and suggest some
kind of remedial action.
current product offerings fall far short in every dimension, but
they’re trying. the hardest problem is the false alarms. to explain it, i’m
going to have to digress into statistics and explain the base rate fallacy.
suppose a doctor had a disease test that was 99 percent accurate. that
is, if someone has the disease, there is a 99 percent chance that the test
would signify “disease,” and if someone does not have the disease, there
is a 99 percent chance that the test would signify “healthy.” assume that
one in ten thousand people, on average, have the disease. is the test any
good?
no. if the doctor administers the test to a random person and she tests
positive, there is only a one percent chance she actually has the disease.
because the population of healthy people is so much larger than the num-
ber of diseased, the test is useless. (it’s not as simple as retesting the person.
assume false positives are consistent for a particular person.) this result is
counterintuitive and surprising, but it is correct.
what this means is that if you assume that network attacks are com-
paratively rare, the base rate fallacy implies that your tests have to be really
good to screen out all of the false positives. an ids that habitually pages
you at 3:00 a.m. with a problem that turns out not to be a problem—an
all-night quake game, or a new internet application, or whatnot—is
going to get turned off pretty quickly.
there are other problems. timely notification is one. i mentioned
slow attacks in the previous section. when does an ids decide that it’s an
attack and notify you? what if the ids thinks something looks like an
attack, sort of? does it notify you? when? again, remember the false pos-
itive problem. if it guesses wrong too often, you’re going to stop listening
to it.
and will you even know what to do when the alarm goes off? hor-
tatory messages of the general form “you’re under attack” are useless
network defenses 195
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 195


 
 

____________________________________________

SecretsAndLies.pdf page: 228

____________________________________________

e-mail security
these days, e-mail is everywhere. anyone who has any presence at all 
in cyberspace has an e-mail address, and probably receives far too many
e-mail messages every day. e-mail has no built-in security.
like any network packet, any machine between the source and the
destination can read e-mail. (you can even see the names of some of those
machines in the headers of your received mail.) the common metaphor
used for internet e-mail is postcards: anyone—letter carriers, mail sorters,
nosy delivery truck drivers—who can touch the postcard can read what’s
on the back. and there’s no way of verifying the signature on a letter or
the return address (you do know that the “from” field in your mail
header can easily be forged?), so there’s no way of knowing where a mes-
sage really came from. (spammers use this feature to hide the origin of
their mass mailings.) if an attacker wants to be subtle, he can actually con-
nect (without an account) to the forged machine of origin and send the
mail from there. if he doesn’t care, he can just forge the “from” line.
we want two things for e-mail. one, we want to make sure that no
one other than the intended recipient can read the message. two, we
want to make sure that an e-mail message came from the person it
purports to have come from, and that no one can forge e-mail messages.
the cryptography to protect e-mail is simple and straightforward, and
dozens of products on the market deal with the problem. here’s the basic
protocol:
1. alice gets bob’s public key.
2. alice signs her message with her private key.
3. alice encrypts her message with bob’s public key.
4. alice sends the encrypted and signed message to bob.
5. bob decrypts the message using his private key.
6. bob verifies alice’s signature using her public key.
where you’re going to see difficulties is in the public keys: how you
get them, store them, verify them. i’ll talk about this a lot more in
chapter 15.
200 c h a p t e r  t w e l v e
453803_ch12.qxd:453803_ch12.qxd  4/12/13  1:17 pm  page 200


 
 

____________________________________________

SecretsAndLies.pdf page: 235

____________________________________________

such catastrophic effects—it is inconceivable that a large internet applica-
tion will be bug-free.
we’ve seen this kind of thing with windows nt. hardly a day goes
by without some new announcement about a security hole in this pro-
gram. we’re already seeing the same trend with windows 2000.
buffer overflows
buffer overflows (sometimes called stack smashing) are the most common
form of security vulnerability in the last ten years. they’re also the easiest
to exploit; more attacks are the result of buffer overflows than any other
problem. and they can be devastating; attacks that exploit this vulnerabil-
ity often result in the complete takeover of the host. many high-profile
attacks exploit buffer overflows. since they show no sign of abating, it’s
worth explaining in some detail what they are and how they work.
let’s start with an analogy. in order to steal something from your local
7-11, you’re going to have to get past the sales clerk. this clerk isn’t a cre-
ative thinker. in fact, she will only do what her employee manual says
she’s supposed to do. this employee manual is a big binder filled with
protocols. things like “dealing with someone claiming to be an
employee”: 
step 1: ask to see the person’s badge. 
step 2: make sure the badge is not a forgery. 
step 3: compare the picture on the badge with the face of the person. 
step 4: if they match, let the person in. if they don’t, don’t. 
or “dealing with a federal express driver”: 
step 1: take the package. 
step 2: sign for the package. 
step 3: make sure the driver leaves.
there’s no way the federal express person is going to get by the clerk
to the back of the store, because the employee manual explicitly says that
the driver has to leave after receiving a signed receipt.
this is pretty much the way computers work. programs are like the
steps in manuals; computers do what the program says and nothing else.
software reliability 207
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 207


 
 

____________________________________________

SecretsAndLies.pdf page: 236

____________________________________________

networked computers work the same way. the computer has a set of
protocols that it follows—logon procedures, access restrictions, password
protections—that it uses to figure out who can come in and who can’t.
someone who follows the protocols correctly can get in. someone who
doesn’t, can’t.
one way to defeat a protocol like this is to modify the actual com-
puter program. or, back to our analogy, it’s like slipping a page into the
clerk’s employee manual. imagine that the manual is written for people
who are none too bright. each page is a step, kind of like a “choose your
own adventure” novel: “if the customer gives you a credit card, go to the
next page. if the customer gives you cash, go to page 264.” the dealing
with a federal express driver” steps might look like this:
page 163: take the package. if the driver has one, go to the next page. if the
driver doesn’t have one, go to page 177.
page 164: take the signature form, sign it, and return it. go to the next page.
page 165: ask the driver if he or she would like to purchase something. if the
driver would, go to page 13. if not, go to the next page.
page 166: ask the driver to leave. if he or she does . . . and so on.
there’s one last piece of setup. whenever the 7-11 clerk gets some-
thing, she puts it on top of the open page in her manual. she can’t look at
the new thing any other way.
here’s the attack: we’re going to dress up like a fedex driver, and
then slip a page into the clerk’s manual when we give her the signature
form. what we’ll do is give the clerk two pages instead of one. top page
will be a signature form. the bottom page will be a fake employee-man-
ual page:
page 165: give the driver all the money in the cash register. go to the next
page.
this will work. the clerk takes the package on page 163. she goes to
page 164 and takes the signature form (and our fake page). she puts them
both on top of the open manual. she signs and returns the form (leaving
the fake page on top of the manual), and when she returns to the manual
she gets our fake page instead. she gives us all the money in the register
and turns to the next page (the real page 165). we can tell her we don’t
want to buy anything, and leave. if the 7-11 clerk is really as dumb as a
208 c h a p t e r  t h i r t e e n
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 208


 
 

____________________________________________

SecretsAndLies.pdf page: 237

____________________________________________

computer system, we can get away with it. we can use this trick to
persuade the 7-11 clerk to let us into the stockroom or to do whatever
else we want. by slipping a page into her employee manual, we can give
her arbitrary instructions.
essentially, this is the way to exploit a buffer overflow bug in a com-
puter system. computers store everything, programs and data, in mem-
ory. if the computer asks a user for an 8-character password and receives
a 200-character password, those extra characters may overwrite some
other area in memory. (they’re not supposed to—that’s the bug.) if it is
just the right area of memory, and we overwrite it with just the right char-
acters, we can change a “deny connection” instruction to an “allow
access” command or even get our own code executed.
the morris worm is probably the most famous overflow-bug exploit.
it exploited a buffer overflow in the unix fingered program. it’s
supposed to be a benign program, returning the identity of a user to
whomever asks. this program accepted as input a variable that is supposed
to contain the identity of the user. unfortunately, the fingered program
never limited the size of the input. input larger than 512 bytes overflowed
the buffer, and morris wrote a specific large input that allowed his rogue
program to execute as root and install itself on the new machine. (this
particular bug has, of course, been fixed.)
what makes this worm especially relevant for this section is that it
itself had a programming bug. it was supposed to hop between comput-
ers on the internet, copy itself onto each server, and then move on. but a
typo in the code made the worm copy itself not once, but indefinitely, on
each computer. the result was that computers infected by the worm
crashed. over 6,000 servers crashed as a result; at the time that was about
10 percent of the internet.
skilled programming can prevent this kind of attack. the program
can truncate the password at 8 characters, so those extra 192 characters
never get written into memory anywhere. it’s easy to do, but it’s hard to
do everywhere. the problem is that with any piece of modern, large,
complex code, there are just too many places where buffer overflows 
are possible (and they’re not all as simple as this example) that it is difficult
to squash them all. it’s very difficult to guarantee that there are no over-
flow problems, even if you take the time to check. the larger and more
complex the code is, the more likely the attack.
software reliability 209
453803_ch13.qxd:453803_ch13.qxd  4/12/13  1:23 pm  page 209


 
 

____________________________________________

SecretsAndLies.pdf page: 241

____________________________________________

that he can write whatever number he wants on the paper, the system falls
apart.
however, this was almost exactly the system that precomputer banks
used to keep track of depositors’ accounts. each depositor had a bank-
book stored in a file cabinet in the bank, and another in his possession.
the bankbooks had a number that represented the amount of money the
person had stored in the bank. when he deposited or withdrew money,
the bank wrote a new number in both books. the system  didn’t fall apart,
because one of the books was kept within the secure perimeter of the
bank. and that was the real book; the book the depositor got was just a
copy for his mollification. if a depositor forged a line in his bankbook, it
would not match with the book stored in the bank. the bank teller
would notice the discrepancy, presumably check other records to make
sure there actually was attempted fraud, and prosecute accordingly. the
customer could not modify the book in the bank because he could not get
through the secure perimeter. (the teller, of course, had many more
opportunities to commit fraud.)
this example illustrates the benefit of a secure perimeter; the security
wouldn’t work without one.
we can build an anonymous cash card system the same way. cus-
tomers walk around with smart cards in their wallets. the smart card con-
tains a memory location with a dollar amount stored in it, much the same
as the bankbook. smart cards talk to each other through some kind of
point-of-sale terminal. when a customer buys something, her smart card
subtracts the amount of purchase from the amount in memory and writes
the lower number back into memory. when a merchant sells something,
his smart card adds the amount of purchase into the memory location.
the cards only do this in pairs (secret keys in the cards can easily enforce
this), so that everything balances out at the end. and to stop someone
from just going into the card and changing his balance, the cards are tam-
perproof.
wasn’t that easy? the secure perimeter around the card—secrets
within the card stay within the card, and people outside the card can’t
affect those secrets—makes a lot of security problems go away. without
it, the only way to make a system like this work is through a tedious back-
end processing system.
checks work rather like the first example i talked about: people
keeping a paper in their wallet listing their current account balance. peo-
secure hardware 213
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 213


 
 

____________________________________________

SecretsAndLies.pdf page: 243

____________________________________________

on private conversations even when people use encryption devices. or
for a cable tv decoder.
the basic problem is that tamperproof hardware does not exist. you
can’t make a device that cannot be tampered with. you can make a
device that most people can’t tamper with. you might possibly even
make a device that can’t be tampered with given a level of technology.
but you can’t make a device that’s absolutely tamperproof.
i could spend an entire book on the details, but they change so regu-
larly that it would be pointless. suffice it to say that there are several ded-
icated laboratories in the united states that can defeat any tamperproof
technology that they’ve ever seen. many more laboratories in various cor-
porations can be used to defeat tamper resistance, even though the labs
were created for other purposes. the chip laboratories at intel, for exam-
ple, have equipment that could be used to reverse engineer pretty much
any tamperproof chip on the market.
in response to this reality, many companies implemented the seman-
tic fix of calling their technology tamper resistant, which is something like
“tamperproof for almost everybody.” i suppose this is reasonable: a letter
sealed in an envelope could be viewed as tamper resistant, even though
the cia and others have a surprising amount of expertise in tampering
with the mail.
the problem with tamper-resistant hardware is figuring out exactly
how tamper resistant it really is. imagine that you are implementing a
smart card commerce system that uses a tamper-resistant chip for its secu-
rity. and it’s an anonymous system, so the tamper resistance is all the pro-
tection you have against widespread counterfeiting. how much tamper
resistance do you need? how do you know when you’ve gotten that?
what do you do when technology marches on?
figuring out how much tamper resistance you need might be doable.
maybe you can estimate the value of a break: how much money some-
one could counterfeit if she were able to defeat the tamper resistance. if
you’ve designed a good system, maybe you can cap the amount of money
that can be stolen from a single smart card: let’s say $100. the next prob-
lem is harder: how do you know when you’ve implemented enough
tamper-resistance measures so that the cost to defeat them is more than
$100?
nobody really knows how effective different tamper-resistance mea-
sures are. sure, a laboratory can tell you how much time they spent
secure hardware 215
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 215


 
 

____________________________________________

SecretsAndLies.pdf page: 251

____________________________________________

attacks by the terminal against the cardholder or data owner.
these are the easiest attacks to understand. when a cardholder puts her card
into a terminal, she trusts the terminal to relay any input and output from
the card accurately. security in most smart card systems centers around the
fact that the terminal only has access to a card for a short period of time.
the real security, though, has nothing to do with the smart card/terminal
exchange; it is the back-end processing systems that monitor the cards and
terminals and flag suspicious behavior.
attacks by the cardholder against the terminal. more subtle are
attacks by the cardholder against the terminal. these involve fake or modi-
fied cards running rogue software with the intent of subverting the protocol
between the card and the terminal. good protocol design mitigates the risk
of these kinds of attacks. the threat is further reduced when the card con-
tains hard-to-forge physical characteristics (e.g., the hologram on a visa
card) that can be manually checked by the terminal owner.
attacks by the cardholder against the data owner. in many smart
card–based commerce systems, data stored on a card must be protected from
the cardholder. in some cases, the cardholder is not allowed to know that
data. if the card is a stored-value card, and the user can change the value,
she can effectively mint money. there have been many successful attacks
against the data inside a card.
attacks by the cardholder against the issuer. many financial attacks
appear to be targeting the issuer, but in fact are targeting the integrity and
authenticity of data or programs stored on the card. if card issuers choose to
put bits that authorize use of the system in a card, they should not be sur-
prised when those bits are attacked. these systems rest on the questionable
assumption that the security perimeter of a smart card is sufficient for their
purposes.
attacks by the cardholder against the software manufacturer.
generally, in systems where the card is issued to an assumed hostile user, the
assumption is that the user will not load new software onto the card. this
turns out not to be the case.
attacks by the terminal owner against the issuer. in some systems,
the terminal owner and card issuer are different parties. this split introduces
several new attack possibilities. the terminal controls all communication
between the card and card issuer, and can always falsify records or fail to
complete one or more steps of a transaction in an attempt to facilitate fraud
or create customer service difficulties for the issuer.
attacks by the issuer against the cardholder. in general, most systems
presuppose that the card issuer has the best interests of the cardholder at
heart. but this is not necessarily the case. these attacks are typically privacy
invasions of one kind or another. smart card systems that serve as a substi-
secure hardware 223
453803_ch14.qxd:453803_ch14.qxd  4/12/13  1:30 pm  page 223


 
 

____________________________________________

SecretsAndLies.pdf page: 253

____________________________________________

15
certificates 
and credentials
the notions of a public-key certificate and a public-key infrastructure are
central to much of modern internet cryptography. before get-
ting into that, though, it is worth recalling what a digital signa-
ture is. a digital signature is a mathematical operation on a bucket of bits
that only a certain key can do. this operation can be verified with
another, corresponding, key. the signing key is only known by alice.
hence, the argument goes, only alice could have performed the mathe-
matical operation and therefore alice “signed” the bucket of bits.
the problem with this model is that it assumes that the signing key is
a secret only known by alice. all we can really stipulate by verifying the
signature is that alice’s key signed the message; we cannot say anything
about whether or not alice did. we don’t know if alice’s key was stolen
by someone else. we don’t know if a trojan horse snuck into alice’s
computer and fooled her into signing something else. we don’t know
anything about alice’s intentions. when we see alice’s handwritten sig-
nature on a paper document, we can make statements about her volition:
she read and signed the document, she understood the terms. when we
get a document signed with alice’s private key, we don’t even know if
alice ever saw the document in the first place. “digital signature” is a ter-
rible name for what is going on, because it is not a signature.
225
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 225


 
 

____________________________________________

SecretsAndLies.pdf page: 254

____________________________________________

this will become important later in this chapter. but first, let’s talk
about trusted third parties.
trusted third parties
cryptographers define a trusted third party as someone trusted by every-
one involved in a protocol to help complete the protocol fairly and
securely. a friend at the nsa once said (with remarkable perspicuity):
“someone whom you know can violate your security policy without get-
ting caught.” oddly enough, these definitions are basically the same.
remember the various trusted third party protocols from chapter 7?
all commerce, with the exception of direct barter, uses trusted third par-
ties in some way. even cash transactions: the seller is trusting the gov-
ernment to back the currency he is accepting. when the transaction
involves an interesting financial instrument—a check, a credit card, a
debit card, a traveler’s check—both the buyer and the seller are relying on
the bank or financial company to behave properly. the merchant and the
customer don’t necessarily trust each other, but the trusted third party is
able to successfully mediate a transaction between them. things would
fall apart pretty quickly if a credit card company started capriciously refus-
ing to accept merchant slips for certain cardholders.
lawyers act as trusted third parties in more personal roles: executors
of wills, that sort of thing. when someone announces to her captors, “if
you kill me, my lawyer will mail a copy of the evidence to the fbi,
cnn, and the new york times,” she is using her attorney as a trusted
third party. lawyer jokes aside, the profession makes a pretty good trusted
third party.
the entire civil court system can be viewed as a trusted third party,
ensuring that contracts are fulfilled and that business is conducted prop-
erly. here’s the fair contract protocol: alice and bob negotiate and sign a
contract. if one of them feels that the other is not upholding his or her end
of the contract, he or she calls in the trusted third party: the judge. the
judge listens to the evidence from both sides, and then makes a ruling.
this works because both alice and bob believe that the judge will be
fair. in jurisdictions where the legal system is corrupt or incompetent, you
see a much smaller reliance on contracts and a radically different set of
rules for conducting commerce.
226 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 226


 
 

____________________________________________

SecretsAndLies.pdf page: 255

____________________________________________

many other trusted third parties populate everyday life. consignment
shops, either storefront or over the net, are trusted third parties. so are
auction houses. ever buy something to be delivered cod? the delivery
service is acting as a trusted third party. they do the same thing for certi-
fied mail. notary publics act as trusted record keepers, verifying the iden-
tities of people signing legal documents and providing audit evidence in
the event of a dispute. the un sends “observers” to act as trusted third
parties in parts of the world where the parties involved don’t trust each
other (and have way too many guns). on the net, auction escrow ser-
vices have appeared, acting as trusted third parties between buyers and
sellers for high-priced items.
in the united states, an entire industry of trusted third parties medi-
ates real estate transactions: title companies. these companies act as
trusted third parties between the various parties involved in buying and
selling a house: the buyer, the seller, the buyer’s bank, the seller’s bank,
the buyer’s real estate agent, and the seller’s real estate agent. all of these
parties rely on the title company to complete the transaction fairly.
trusted third parties will become more important in the electronic
world. in a world without face-to-face (or even voice-to-voice) transac-
tions, in one of mediocre cryptography and horrible computer security,
they are the only real certification anyone is likely to have.
remember the whole system of public keys that i talked about in
chapter 6? alice wants to send an encrypted message using bob’s public
key, so she goes to a public-key database to find it. she gets bob’s public-
key certificate. this is a message, signed by someone else, that certifies
that the particular key belongs to bob. the person who signed that
certificate: that’s a trusted third party.
secure systems leverage the trusted third parties that are inherent in
the systems that they are securing. badly designed systems introduce
trusted third parties without understanding the security ramifications.
awfully designed systems mandate trusted third parties by law.
credentials
open up your wallet. inside you will see all sorts of credentials. you have
a bank card. this is a credential issued to you by your bank; you use it to
prove your identity to an atm so that it dispenses cash. you have credit
certificates and credentials 227
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 227


 
 

____________________________________________

SecretsAndLies.pdf page: 258

____________________________________________

remember chapter 6 and public-key cryptography? alice uses pub-
lic-key cryptography to digitally sign things. she signs documents with
her private key, and sends the signed document to bob. now bob needs
alice’s public key to verify the signature. where does he get it and how
is he sure it’s alice’s?
in the early days of public-key cryptography, people envisioned vast
databases of public keys, kind of like telephone books. bob could look
alice’s name up in the online database of public keys, and then retrieve
the public key associated with that name.
well, if everyone’s public key is going to be stored in a vast database
somewhere, what about the security of that database? an attacker can do
lots of malicious things if he can substitute one public key for another. he
can create a new public key, sign a bunch of checks with it, and then slip
it in the database next to alice’s name. suddenly, alice signed all of those
checks. if bob is using alice’s public key to encrypt a message to her, the
attacker can swap his public key for alice’s; now bob’s secret message to
alice can be decrypted by the attacker, and not by alice.
we might be able to secure the public-key database, but the whole
idea was to have public keys freely and widely available. this just isn’t
going to work.
certificates were the solution. a certificate is a binding between a
public key and an identity. a mutually trusted entity—call him god for
now—takes alice’s name and alice’s public key, sticks them together,
and then signs the whole mess. now bob has no worries. he gets alice’s
public-key certificate from somewhere—he doesn’t much care where—
and verifies god’s signature on it. bob trusts god, so if the signature is
valid he knows that the public key belongs to alice and not to some
imposter. problem solved; the world is now safe for electronic commerce.
well, not exactly. note that we haven’t actually solved the problem.
all we’ve done is taken the original problem, “how does bob know that
alice’s public key is really hers?” and changed it to: “how does bob
know that god’s public key is really his?” bob has to verify god’s signa-
ture on the certificate before he can use alice’s key, so he needs god’s
public key. and where is he going to get that?
but we did solve something. bob presumably wants to communicate
with a lot of people, not just with alice. and if god has signed everyone’s
certificate, we’ve reduced bob’s problem from verifying everyone’s pub-
230 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 230


 
 

____________________________________________

SecretsAndLies.pdf page: 259

____________________________________________

lic key to verifying just one public key: god’s. but let’s save that problem
for later.
a real certificate is a little more complicated. it contains information
about the person (his name, possibly his job title, possibly his e-mail
address, and other things about him), information about the certificate
(when it was issued, when it expires), information about the issuer or
signer (who he is, what algorithm he used to sign the certificate), and
information about the public key (what algorithm it is for) . . . as well as
the public key itself.
the basic idea is that alice gets a public key certificate signed by god
somehow. either she generates her own public-key/private-key key pair
and sends the public key to god, who returns the public-key certificate,
or god generates a public-key/private-key key pair for alice and sends
her both the private key and the public-key certificate. (now we have the
problem of securing this exchange, but never mind that for now.)
this all works great, until alice loses her private key. maybe some-
one stole it. maybe she just forgot it. (or, more likely, her computer
crashed and didn’t have a backup.) bob is going to try to send her
encrypted 
e-mail in that lost key. or, worse yet, bob is going to try to verify signa-
tures created after someone stole the key. what do we do now?
we tell god, and he revokes alice’s certificate. he declares it no
longer valid, no longer good, no longer correct. how does he do this? he
can’t go through every nook and cranny of the net and erase every copy
of the certificate. (well, maybe god can, but this is only an analogy.) he
probably doesn’t even know bob has a copy of it.
so, god puts alice’s certificate on the certificate revocation list, or crl.
the crl is a list of revoked certificates. (remember 20 years ago when
merchants had newsprint books listing bad credit card numbers? that’s a
crl.) god issues a crl at regular intervals (the credit card companies
did it once a week), and it is bob’s job to make sure that alice’s certificate
is not on the current crl before he uses it. he should also make sure that
it hasn’t expired, and that the certificate really does belong to alice.
how does he do that last one? he compares alice’s name with the
name on the certificate. if they match, then the certificate is hers. it
sounds simple, except that it doesn’t work.
this idea has several problems. first, there is no one to act as god.
or, more properly, there is no one organization or entity that everyone
certificates and credentials 231
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 231


 
 

____________________________________________

SecretsAndLies.pdf page: 260

____________________________________________

can agree on and whose judgment is unassailable. the second is that alice
has no single name that everyone can agree on.
first problem first. remember, for this whole system to work, alice
has to have her certificate issued by someone that both she and bob trust.
in reality, we use hierarchies of trust to establish the validity of certificates.
a military organization is probably the best example of this. the platoon
leader signs the certificates of everyone in his platoon. the division com-
mander signs the certificate of every platoon leader under him. the army
general signs the certificates of his divisional commanders. and so on, up
to the commander-in-chief.
alice now has a chain of certificates, from the commander-in-chief to
the army general to the divisional commander to the platoon leader to
her. she keeps them all, and presents them to bob. if she and bob are in
the same platoon, then bob also has the platoon leader’s certificate. he
knows that it is valid, so he can verify alice’s certificate directly. if bob is
in the same division as alice but in a different platoon, they share the same
divisional commander certificate. bob can use it to verify alice’s platoon
leader’s certificate, and then alice’s certificate. since alice and bob are in
the same military, someone is in both of their chains of command. it
might even be the commander-in-chief, who is “god” in this example.
this system works great in the military, but less well in the civilian
world. the internet uses certificates to fuel a lot of protocols: ipsec and
various vpn systems, ssl, a few electronic commerce protocols, some
login protocols. these certificates are issued to users by someone called a
certificate authority (ca). a ca can be a corporate security office. it can be
a government. it can be a private company that is in the business of issu-
ing certificates to internet users.
these cas also need certificates. (remember, there’s a hierarchy
here.) these ca certificates are issued by other cas (probably verisign).
eventually you get to the god in this system, or in reality a pantheon of
gods. the highest-level cas have what are known as root certificates;
they are not signed by anyone else. these certificates are embedded in the
software you buy: your browser, your vpn software, and so forth. this
is all called a public-key infrastructure (pki). it works, but only sort of.
second problem: alice’s name.
back in ancient times (the mid-1980s), someone dreamed about a
world where every individual, every process, every computer, every
communications device—anything connected to digital communica-
232 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 232


 
 

____________________________________________

SecretsAndLies.pdf page: 261

____________________________________________

tions—had a unique name. these names would be held in a vast distrib-
uted database, held by multiple people in multiple locations. this was
called x.500.
certificates generally associate a public key with a unique name
(called a distinguished name in x.500 talk), but few people talk about how
useful that association is. imagine that you receive the certificate belong-
ing to joan robinson. you may know only one joan robinson person-
ally, but how many does the ca know? how do you find out if the
particular joan robinson certificate you received is your friend’s certifi-
cate? you could have received her public key in person or verified it in
person (pgp allows this), but more likely you received a certificate in e-
mail and are simply trusting that it is the correct joan robinson. the cer-
tificate’s common name will probably be extended with some other
information, in order to make it unique among names issued by that one
ca.
do you know that other information about your friend? do you
know what ca her certificate should come from?
remember the phone directory metaphor for public keys. if you
wanted to find joan robinson’s public key you would look her up in the
directory, get her public key, and send her a message for her eyes only
using that public key. this might have worked with the stanford com-
puter science department phone directory in 1976, but how many joan
robinsons are in the new york city phone book, much less in a hypo-
thetical phone book for the global internet?
we grow up in small families where names work as identifiers. by the
time we’re five years old, we know that lesson. names work. that is false
in the bigger world, but things we learn as toddlers we never forget. in
this case, we need to think carefully about names and not blindly accept
their value by the five-year-old’s lessons locked into our memories.
the idea also assumes that alice and bob have an existing relationship
in the physical world, and want to transfer that relationship into cyber-
space. remember back when “cyberspace” was just a science  fiction
term, and any relationship worth talking about—business, social, banking,
commercial—was formed in the flesh-and-blood world? today, people
are meeting on the net and forming relationships all the time. sometimes
they meet in person long after they’ve become friends; sometimes they
never meet in person. in this brave new world, a system designed to map
relationships from the physical world into cyberspace seems limiting.
certificates and credentials 233
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 233


 
 

____________________________________________

SecretsAndLies.pdf page: 263

____________________________________________

certificate with those contents. of course, the ca would sign a contract
promising not to do so, but that does not remove the capability. mean-
while, since security of this model depends on the security of both pieces
and the interaction between them (they have to communicate somehow),
the ra+ca is less secure than either the ra or the ca, no matter how
strong the ca or how good the contract with the ca. of course, the
model with a ca at the authority’s desk (not at the vendor’s site) violates
some pki vendors’ business models.
another problem involves the protection of the private key. re -
member, for the whole digital-signature system to work, you have to be
sure that only you know your private key. okay then, how do you pro-
tect it? you almost certainly don’t own a secure computing system with
physical access controls, tempest shielding, “air wall” network secu-
rity, and other protections; you store your private key on a conventional
computer. there, it’s subject to attack by viruses and other malicious pro-
grams. even if your private key is safe on your computer, is your com-
puter in a locked room, with video surveillance, so that you know no one
but you ever uses it? if it’s protected by a password, how hard is it to guess
that password? if your key is stored on a smart card, how attack-resistant
is the card? if it is stored in a truly attack-resistant device, can an infected
computer convince the trustworthy device to sign something you didn’t
intend to sign?
this matters mostly because of the term nonrepudiation. like
“trusted,” this term is taken from the literature of academic cryptography.
there it has a specific meaning: that the digital-signature algorithm is not
breakable, so a third party cannot forge your signature. pki vendors have
latched onto the term and used it in a legal sense, lobbying for laws to the
effect that if someone uses your private signing key, then you are not
allowed to repudiate the signature. in other words, under some digital sig-
nature laws (e.g., utah’s and washington’s), if your signing key has been
certified by an approved ca, then you are responsible for whatever that
private key does. it does not matter who was at the computer keyboard
or what virus did the signing; you are legally responsible.
the way it’s supposed to work is that when you know your key is
compromised, you put it on a crl. anything signed after that time is
automatically repudiated. this sounds plausible, but the system is funda-
mentally flawed. bob wants to know that alice’s key hasn’t been com-
promised before he accepts her digital signature. the attacker is not going
certificates and credentials 235
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 235


 
 

____________________________________________

SecretsAndLies.pdf page: 264

____________________________________________

to announce the compromise to alice. so, alice’s first clue that her key
was compromised will come when she gets some notice from bob show-
ing evidence of the fraudulent signature. in most schemes, this will hap-
pen only after bob accepts the signature.
contrast this with the practice regarding credit cards. under mail-
order/telephone-order (moto) rules, if you object to a line item on
your credit card bill, you have the right to repudiate it—to say you  didn’t
buy that—and the merchant is required to prove that you did.
there are similar vulnerabilities in the computer that does the verifi-
cation. certificate verification does not use a secret key, only public keys.
but to verify a certificate, you need one or more “root” public keys: the
public keys of the cas. if the attacker can add his own public key to that
list, then he can issue his own certificates, which will be treated exactly
like the legitimate certificates. they can even match legitimate certificates
in every other field except that they would contain a public key of the
attacker instead of the correct one.
some pki vendors claim that these keys are in root certificates, and
hence secure. such a certificate is self-signed and offers no increased secu-
rity. the only answer is to do all certificate verification on a computer sys-
tem that is invulnerable to penetration by hostile code or to physical
tampering.
and finally, how did the ca identify the certificate holder? whether
a certificate holds just an identifier or some specific authorization, the ca
needs to identify the applicant before issuing the certificate.
several credit bureaus thought they would get into the ca business.
after all, they had a vast database on people, so, the thinking ran, they
should be able to establish someone’s identity online with ease. if you
want to establish identity online, you can do that provided you have a
shared secret with the subject and a secure channel over which to reveal
that secret. ssl provides the secure channel.
the trouble with a credit bureau serving this role is that they don’t
have a secret shared only with the subject. in other words, there isn’t a
secure offline id that can be used to bootstrap the process. this is because
credit bureaus are in the business of selling their information to people
other than the subject. worse, because credit bureaus do such a good job
at collecting and selling facts about people, others who might have infor-
mation about a subject are probably hard pressed to find any datum shared
with the subject that is not already available through some credit bureau.
236 c h a p t e r  f i f t e e n
453803_ch15.qxd:453803_ch15.qxd  4/12/13  1:36 pm  page 236


 
 

____________________________________________

SecretsAndLies.pdf page: 272

____________________________________________

often infer information about individuals by making queries about groups.
one example: alice queries the database for summary information on
detailed groups. if she can ask the database queries like this—summary
information on every narcoleptic female, between ages 35 and 45, with
one diabetic parent, and living in a particular zip code—then alice is
likely to be able to isolate individuals.
a possible solution to this problem is to scrub the data beforehand.
data from the 1960 u.s. census, for example, was secured in this manner.
only one record in a thousand was made available for statistical analysis,
and those records had names, addresses, and other sensitive data deleted.
the census bureau also used a bunch of other tricks: data with extreme
values were suppressed, and noise was added to the system. these sorts of
protections are complicated, and subtle attacks often remain. if you want
to know the income of the one wealthy family in a neighborhood, it
might still be possible to infer it from the data if you make some reason-
able assumptions.
the other possible solution is to limit the types of queries that some-
one can make to the database. this is also difficult to get right. in one
famous research paper, the author calculated her boss’s salary based on
legitimate queries to the 1970 census database, despite controls that were
put in place precisely to stop this kind of thing. the new zealand
national health information system tries to defeat these kinds of attacks
by not providing summary information on groups smaller than six people.
(a technique known to be insufficient.)
attacks are still possible. alice is going to know the kinds of queries
that are allowed, and will do her best to figure out some mathematical
way of inferring the information she wants from the information she’s
allowed to get. and things are exacerbated further if alice is allowed to
add and delete data from the database. if she wants to learn about a par-
ticular person, she might be able to add a couple hundred records into the
database and then make general queries about the population she added
plus her target. since she knows all the data she added, she can infer data
about her target. a whole set of related attacks follow from this idea.
this was an active research area in the 1980s, but less so today.
(although the new medical privacy regulations may bring about a resur-
gence.) the problems are not solved, though.
244 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 244


 
 

____________________________________________

SecretsAndLies.pdf page: 273

____________________________________________

steganography
steganography is the science of hiding messages in messages. herodotus
talks of the ancient greek practice of tattooing a secret message on the
shaved head of a messenger, and letting his hair grow back before sending
him through enemy territory. (the latency of this communications sys-
tem was measured in months.) invisible ink is a more modern technique.
microdots were invented by the germans during world war i, and
stayed in vogue for many years. spies would photograph an image such
that the image on the negative was small enough to cut out and place over
a period of a book. the spy would carry the book around, secure that no
one would find the microdot hidden on one of its many pages.
in the computer world, steganography has come to mean hiding
secret messages in graphics, pictures, movies, or sound. the sender hides
the message in the low-order bits of one of these file types—the quality
degrades slightly, but if you do it right it will hardly be noticeable—and
the receiver extracts it at the other end. several commercial and freeware
programs offer steganography, either by themselves or as part of a com-
plete communications security package.
steganography offers a measure of privacy beyond that provided by
encryption. if alice wants to send bob an e-mail message securely, she
can use any of several popular e-mail encryption programs. however, an
eavesdropper can intercept the message and, while she might not be able
to read it, she will know that alice is sending bob a secret message.
steganography allows alice to communicate with bob secretly; she can
take her message and hide it in a gif file of a pair of giraffes. when the
eavesdropper intercepts the message, all she sees is a picture of two
giraffes. she has no idea that alice is sending bob a secret message. alice
can even encrypt it before hiding it, for extra protection.
so far, so good. but that’s not how the system really works. the
eavesdropper isn’t stupid; as soon as she sees the giraffe picture she’s going
to get suspicious. why would alice send bob a picture of two giraffes?
does bob collect giraffes? is he a graphics artist? have alice and bob been
passing this same giraffe picture back and forth for weeks on end? do they
even mention the picture in their other correspondence?
the point here is that steganography isn’t enough. alice and bob
must hide the fact that they are communicating anything other than
security tricks 245
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 245


 
 

____________________________________________

SecretsAndLies.pdf page: 274

____________________________________________

innocuous photographs. this only works when steganography can be
used within existing communications patterns. i’ve never sent or received
a gif in my life. if someone suddenly sends me one, it won’t take a
rocket scientist to realize that there might be a steganographic message
hidden somewhere in it. if alice and bob already regularly exchange suit-
able files, then an eavesdropper won’t know which messages—if any—
contain the messages. if alice and bob change their communications
patterns to hide the messages, it won’t work. an eavesdropper will figure
it out.
this is important. i’ve seen steganography recommended for secret
communications in oppressive regimes, where the simple act of sending
an encrypted e-mail could be considered subversive. this is bad advice.
the threat model assumes that you are under suspicion and want to look
innocent in the face of an investigation. this is hard. you are going to be
using a steganography program that is available to your eavesdropper. she
will have a copy. she will be on the alert for steganographic messages.
don’t use the sample image that came with the program when you
downloaded it; your eavesdropper will quickly recognize that one. don’t
use the same image over and over again; your eavesdropper will look for
the differences that indicate the hidden message. don’t use an image that
you’ve downloaded from the net; your eavesdropper can easily compare
the image you’re sending with the reference image you downloaded.
(you can assume she monitored the download, or that she searched the
net and found the same image.) and you’d better have a damn good
cover story to explain why you’re sending giraffes back and forth. and
that cover story should exist before you start sending steganographic mes-
sages, or you haven’t really gained anything.
steganography programs exist to hide files on your hard drive. this
can work, but you still need a good cover story. still, there’s some advan-
tage here over straight encryption—at least in free countries you can
argue that the police have no real evidence—but you have to think it out
carefully.
subliminal channels
one issue with steganography is bandwidth. it’s easy to hide a few bits of
information; hiding an entire e-mail message is a lot harder. here, for
246 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 246


 
 

____________________________________________

SecretsAndLies.pdf page: 278

____________________________________________

imagine that every copy of the little mermaid is watermarked with the
identity of the buyer. how does the merchant verify the buyer’s identity?
unless we have hard-to-forge identity documents—either real or vir-
tual—this system won’t work. and there’s nothing to stop a counterfeiter
from paying $10 to a homeless drunk to walk into the video store and buy
the movie for him. he now has a movie with the embedded watermark
of someone who probably doesn’t care if disney knows his identity, and
who doesn’t have any assets if disney tries to sue.
watermarking can help convict grandma when she duplicates a single
copy of the little mermaid for all her grandchildren, but it can’t stop the
taiwanese pirates from ripping out the watermarks and selling half a mil-
lion pirate copies on the black market. or someone using a fictitious
identity to purchase the legitimate copy and then not worrying about it.
copy protection
this problem is easy to describe, and much more difficult to solve. soft-
ware companies want people to buy their products; they hate it when
someone makes a copy of a business program that costs hundreds of dol-
lars and gives it to a friend. (actually, these days they kind of like it. they
realize that the friend probably wouldn’t have bought it anyway, that he’ll
use the software and get “hooked,” and when he eventually goes legit,
either he or his boss will buy a legal copy of the same program—and not
a competitor’s. wordperfect used this scheme to increase its popularity.)
this is especially important with computer games and in countries with
little respect for intellectual property: in these cases, lots of users will pirate
rather than buy a legitimate copy. (this same problem applies to people
who want to distribute content—books, movies, videos, and so forth—
that they don’t want copied.)
there are all sorts of solutions—embedded code in the software that
disables copying, code that makes use of non-copyable aspects of the orig-
inal disk, hardware devices that the software needs to run—and i’m not
going to talk about them in detail. they all suffer from the same basic
conceptual flaw: it is impossible to copy-protect software on a  general-
purpose computer.
in the hands of joe average computer user, any copy protection sys-
tem works. he can copy ordinary files by following the directions, but has
250 c h a p t e r  s i x t e e n
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 250


 
 

____________________________________________

SecretsAndLies.pdf page: 279

____________________________________________

no idea how to defeat a reasonably sophisticated copy protection scheme.
in the hands of jane hacker, no copy protection system works.
the problem is that jane controls her computer. she can run debug-
gers, reverse engineer code, analyze the protected program. if she’s smart
enough, she can go into the software and disable the copy protection
code. the manufacturer can’t do a thing to stop her; all it can do is make
her task harder. but to jane, the challenge entices her even more.
there are a bunch of janes out there who break copy protection
schemes as a hobby. they hang out on the net, trading illegal software.
there are also those who do it for profit. they work in china, taiwan,
and elsewhere, removing copy protection code and reselling the software
on cd-rom for less than a tenth of the retail price. they can disable the
most sophisticated copy protection mechanisms. the lesson from these
people is that any copy protection scheme can be broken.
the dongle is the current state of the art in copy protection. it’s a piece
of hardware that plugs into the computer, usually into the parallel port.
(conflicts with other devices using the port, and other dongles, are only
problems occasionally.) the protected software calls the dongle at various
points during execution; for example, every thousand keypresses or
mouseclicks, when a user tries to save, or every time he selects the nail
gun as his weapon. if the dongle doesn’t respond to a call, or responds
incorrectly, the software stops running. or, more effectively, it keeps run-
ning but gives subtly wrong answers. (a 1992 version of autodesk’s 3d
studio used the dongle to create a table in memory that was required to
correctly mirror three-dimensional geometry. removing the dongle
caused the program to fail over the course of a few hours, imperceptibly
at first, but eventually dramatically. autodesk had to field a lot of calls
from unregistered users complaining about a strange bug in their version
of 3d studio.)
calls to the dongle are all encrypted, and the dongle itself is protected
from hardware reverse engineering by a variety of tricks. still, programs
that use dongles are routinely broken without attacking either the
cryptography or the tamper resistance.
how? instead of defeating the dongle, hackers go through the code
and remove all calls to it. it’s painstaking work: hackers have to go
through the code line by line, function by function, call by call. they may
have to hook a logic analyzer up to the dongle and correlate execution
addresses to dongle accesses. a sophisticated program could contain tens
security tricks 251
453803_ch16.qxd:453803_ch16.qxd  4/12/13  1:47 pm  page 251


 
 

____________________________________________

SecretsAndLies.pdf page: 285

____________________________________________

etc., etc., etc. they overestimate risks for things that are (1) out of their
control (getting poisoned in restaurants), and (2) sensationalized in the
media (being the victim of a terrorist attack). they underestimate risks for
things that are mundane and ordinary (falling off a ladder, being in a car
accident). certainly not having enough information exacerbates the
problem.
probabilities permeate cryptography, computer security, risk assess-
ment, countermeasures . . . everything this book is about. risk is a
probability. security is a probability.
to illustrate probability, let’s play a gambling game with alice. it’s a
simple game: heads she wins, and tails you win. but you’d like to check
out the coin first, just to make sure that it is fair. sure, she says, look at it
all you want.
you flip the coin once, and it comes out tails. this is a single event,
so it gives you no real information except that “tails” is on at least one of
the faces. so, you flip it ten times. the coin comes up heads on six of
them. does this mean the coin is unfair? maybe. alice is quick to point
out that flipping a fair coin ten times would result in at least six heads 38
percent of the time. this means that if you took 100 fair coins, flipped
them each ten times, then 38 of them would come up heads six or more
times. hardly evidence of fraud.
so you flip the coin 100 times and get 60 heads. alice reminds you
that a fair coin will show at least 60 heads in 100 flips 2.3 percent of the
time. the coin could still be fair.
so you flip the coin 1,000 times. the most likely outcome would be
500,000 heads and 500,000 tails, but you come up with 600,000 heads
and 400,000 tails. despite alice’s assertion that there is a 1 in 10 billion
chance that a fair coin would produce this biased an outcome, you choose
to believe that the coin is weighted. but your belief is based on probabil-
ity; the likelihood that the coin is fair is de minimis.
at this point you probably decide not to use this coin to bet with
alice, despite alice’s protests that the coin is fair. your decision is wise,
even though technically she is right. in fact, you can never prove that the
coin is not fair without cutting it into pieces and weighing them. all you
can do is collect evidence that the coin is not fair that is more and more
convincing.
a lot of beliefs work this way. you believe that the sun rises in the
east because it has done so for the last few trillion mornings. the odds of
this happening without some explanation other than random chance are
the human factor 257
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 257


 
 

____________________________________________

SecretsAndLies.pdf page: 287

____________________________________________

computers make all sorts of mistakes all the time, and malicious hackers
are happy to lead computers down a mistake-riddled garden path, and to
take advantage of those mistakes.
a friend installed a burglar alarm system in his home. the alarm was
wired into the burglar alarm company’s switchboard; when it went off,
the company was automatically alerted, and then they would call the
police. my friend had a secret code that he could use to call the alarm
company and register a false alarm (the police didn’t want to send a squad
car out whenever someone accidentally tripped the alarm). he also had a
second secret code, a duress code. this code meant: “there is a gun being
held to my head, and i am being forced to call you and claim that this is a
false alarm. it isn’t. help!”
one day my friend accidentally tripped the burglar alarm, and he
dutifully called the alarm company to register it as a false alarm. acciden-
tally, he gave them the duress code instead of the false alarm code. almost
immediately he realized his mistake and corrected it. the woman on the
other end gave a huge sigh of relief and said: “thank god. i had no idea
what i was supposed to do.”
when an alarm condition, or even an error condition, appears a few
times a week, people know what to do. if it only happens once every few
years, there could be an entire office that has never seen the alarm, and
hence has no idea what to do. many attacks target complacent users. dur-
ing the attack, those involved can’t imagine that the system is failing, and
attribute the problem to something else. remember chernobyl? “i’ve
never seen that red blinking light before. i wonder what it means. . . .”
this is why we all went through fire drills in primary school. we had
to practice the failure conditions, less so we would be prepared for what
happened—drills can only prepare someone so well for a panic situa-
tion—but as a constant reminder that the failure could occur. i’ve never
been in a real fire, but i’ve been drilled so often in what to do, i’ll proba-
bly be all right. it’s the same with airplanes. when oxygen masks drop
from the ceiling, you don’t want the passengers glancing up from their
novels, wondering what those silly things are, and then going back to
their reading. nor do you want bank tellers ignoring warning signs. “the
bank computer said that i should give him one million dollars in cash.
who am i to second-guess the computer?” or a nuclear power plant
operator wondering what that flashing red light means.
the human factor 259
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 259


 
 

____________________________________________

SecretsAndLies.pdf page: 290

____________________________________________

the program’s easy-to-use graphical interface (although, to be fair, the
pgp versions 6.0 or later have a better user interface).
and they can’t be trusted to make intelligent security decisions. after
the melissa and worm.explorezip scares of 1999, you might think peo-
ple learned not to open attachments they weren’t expecting. but the
infection rate from the iloveyou worm (and its dozens of variants)
taught us that no, people cannot be trained not to open attachments . . .
especially when so many companies are trying to make a business getting
users to send each other interesting attachments.
browsers use digital certificates in order to make secure ssl connec-
tions. when they accept the certificates, they optionally display the iden-
tification of the certificate on the other end. this is essential to the
security; it makes no sense to have a secure connection unless you are sure
who is on the other end of that connection. most people don’t bother
looking at the certificates, and don’t even know they should (or how to).
the same browsers have an option to display warnings when down-
loading java applets. the user is asked whether he trusts the particular
web site that is sending the applet. the user has no idea whether or not
he trusts the web site. nor does he care. if j. random websurfer clicks
on a button that promises dancing pigs on his computer monitor, and
instead gets a hortatory message describing the potential dangers of the
applet—he’s going to choose dancing pigs over computer security any
day. if the computer prompts him with a warning screen like: “the applet
dancing pigs could contain malicious code that might do perma-
nent damage to your computer, steal your life’s savings, and impair your
ability to have children,” he’ll click “ok” without even reading it. thirty
seconds later he won’t even remember that the warning screen even
existed.
human–computer transference
when i introduced cryptography in chapter 6, i wrote about alice and
bob encrypting, decrypting, signing, and verifying messages and docu-
ments. i wrote, for example, that alice could use public-key cryptogra-
phy to send a message to bob by finding bob’s key in a phone book, and
then encrypting a message to bob using this key. this is actually a
complete lie. alice never encrypts messages to bob. she never decrypts
262 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 262


 
 

____________________________________________

SecretsAndLies.pdf page: 291

____________________________________________

messages, or signs messages. she never does any cryptography at all. what
alice does is click a button on her computer, and the computer signs or
encrypts or does whatever alice wants. this is a critical distinction.
imagine the future, when we all habitually sign digital documents.
how might this work? alice will write a digital document in some appli-
cation—a word processor, an e-mail program, or whatever—and click on
some icon to indicate that she is ready to sign it. the application will call
whatever signature software program is standard business practice, and
that software will create the signature. alice will type in her password (or
passphrase), put her finger down on some fingerprint reader, and do
whatever else is required to prove to the software that she is alice. the
signature software will calculate the digital signature on the document,
and hand the application a signature string to append to the document.
voilà—it will appear. alice can probably even verify the signature herself
(again, using the computer), just to make sure it is genuine.
this is what i call human–computer transference. alice knows what
she wants to do: sign a particular document. she has to securely transfer
this volition to the computer with some assurance that the computer will
actually do what alice wants it to do. but secure human–computer trans-
ference is not so easy to do.
our goal is to get alice to sign something she doesn’t want to sign.
since alice is accepting the computer’s word that she is actually signing
the document on the screen, this should be easy. all we have to do is get
the computer to lie to alice.
we write a trojan horse to sit inside the digital signature software.
this trojan horse will contain the document that we want alice to
sign—something either embarrassing or profitable, no doubt—and code
to sign it. the only thing the trojan horse needs is alice’s key. when
alice types in her passphrase to sign a different message to us—the trojan
horse feeds the digital signature software the embarrassing document
instead. the digital signature software returns a signature, and the original
application places that signature on the document alice thinks she is sign-
ing. if alice tries to verify the signature, the trojan horse feeds the embar-
rassing document to the digital signature software. the signature software
returns the fact that the signature is correct; that is, the trojan horse forces
the computer to lie to her. then, alice sends us her document with the
wrong signature; that is, the signature calculated for the other document.
we take the signature, attach it to a copy of the embarrassing document,
the human factor 263
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 263


 
 

____________________________________________

SecretsAndLies.pdf page: 292

____________________________________________

and call the washington post. meanwhile, the trojan horse erases itself and
everything is back to normal.
there’s an easy implementation in windows: a malicious macro
could simply watch for pgp’s “open file” dialog, see what file alice is
about to sign, and copy its own file to that filename, then restore the old
file afterward. word’s macro language can do this, so it could easily be a
payload for a word macro virus.
and that’s just one example. the trojan horse could sign both doc-
uments and transmit the embarrassing signature at some opportune time.
or it could just steal alice’s private key.
nothing here is difficult; the programming is easy. in any case, if we
are successful we could have possession of a damaging document, signed
by alice. we could wave it around in court or pass it to a reporter, cor-
rectly claiming that alice’s valid digital signature is on the bottom of the
document. what is more likely to happen is the reverse. as soon as some-
one writes a fake signature trojan horse, it will be assumed to be every-
where. whenever a document appears in court, one side or the other will
find an expert witness that will testify as to the existence of the trojan
horse and how easy it would be to get someone to unknowingly sign just
about anything. can the court trust this digital signature? it doesn’t
depend on the mathematics; it depends on the circumstances.
the fundamental problem is that you have no idea what the com-
puter is actually doing when you tell it to do something. when you tell
the computer to save a document, or encrypt a file, or calculate the sum
of a column of numbers, you really have no assurance that the computer
did it correctly, or even at all. you’re making a leap of faith. just as it is
hard to catch a thieving employee, it’s hard to catch a malicious computer
program. actually, it’s worse. think of it as a malicious employee who
works alone, with no one watching. all of the monitoring equipment
you might install to catch the employee—hidden cameras, hidden micro-
phones—are controlled by the malicious employee. all you can do is look
at what inputs the employee accepts and what outputs he produces. and
even then you can’t be sure.
if alice can’t trust the computer she is working on, then she can’t
trust it to do what she asks. just because she asked it to sign a particular
document doesn’t mean that it can’t sign another document. the meta-
solution is for alice to only sign documents on a trusted computer, but
264 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 264


 
 

____________________________________________

SecretsAndLies.pdf page: 293

____________________________________________

that’s hard to do. if alice is working on a general-purpose computer, i do
not believe it can ever be trusted enough to avoid this problem.
if alice is using a small, single-purpose, digital signature computer,
then there is hope. i can imagine a hand-held device with a small key-
board and screen. the document can be downloaded into this device by
a general-purpose computer. alice will be able to view the document
from the small screen—there’s no guarantee that the computer will
download what you ask it to—and enter her passphrase on the small key-
board. the device will sign the document and upload the signature back
to the general-purpose computer. we have a prayer of making that sys-
tem secure. we can design it so that only factory software is ever installed
on the computer, and we can have some independent auditing company
certify that the software is correct and behaves well.
but if you are working on an insecure computer—which will be
almost all of the time—there is no assurance that what you see is what you
get, or that what you get actually works as you expect.
malicious insiders
in chapter 4 i talked about malicious insiders. it’s worth recalling the
problems with them. the main problem is that they are often implicitly
trusted. they can steal money out of the cash register, mess with the audit
logs to cover their tracks, photocopy military secrets and send them to the
chinese, steal stacks of blank credit cards, pocket casino chips, look the
other way when the crooks drive off with the truck full of goods, and
anything else they can think of. often, no amount of computer security
can prevent these attacks (although good audit mechanisms can often
determine the guilty parties after the fact).
cyberspace is particularly susceptible to insiders, because it is rife with
insider knowledge. the person who writes a security program can put a
back door in it. the person who installs a firewall can leave a secret open-
ing. the person whose job it is to audit a security system can deliberately
overlook a few things.
one example: chicago’s transit system used both tokens and passes.
riders would either give the clerk a token or show their pass, and the
clerk would let them onto the subway platform. for years, clerks would
the human factor 265
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 265


 
 

____________________________________________

SecretsAndLies.pdf page: 295

____________________________________________

enough to give someone free rein. wandering around and asking if there
is a place to “park and work” for a while will often result in a desk and a
network connection; that person is obviously a corporate visitor.
most social engineering is done on the telephone, which makes the
perpetrator harder to catch. one attacker called people and said, “this is
the operator. i have a collect call from <insert name> in <insert city>.” if
the victim accepted the call, the operator continued: “your collect call
option is blocked. please give me your calling card number and i will con-
nect the call.” this really happened. the attacker found people on usenet
newsgroups and invented collect calls from people they corresponded
with in the newsgroup, an extra touch of verisimilitude.
when kevin mitnick testified before congress in 2000 he talked
about social engineering: “i was so successful in that line of attack that i
rarely had to resort to a technical attack,” he said. “companies can spend
millions of dollars toward technological protections and that’s wasted if
somebody can basically call someone on the telephone and either con-
vince them to do something on the computer that lowers the computer’s
defenses or reveals the information they were seeking.”
another social-engineering attack, this one against credit cards: alice
steals bob’s credit card number. she could charge purchases to bob’s
account, but she’s wilier than that. she advertises merchandise—cameras,
computers, whatever—at a very cheap price. carol sees the advertisement
and buys a product from alice. alice orders the product from a legitimate
retailer, using bob’s credit card number. the retailer ships the product to
carol—there’s so much drop-shipping going on that the packing slip
doesn’t have the price—and is stuck when bob notices the charge. even
worse: carol is inculpated, not alice.
automated social engineering can work against large groups; you can
fool some of the people all the time. in 1993, subscribers to the new
york isp phantom access received this portentous, forged, e-mail mes-
sage: “it has been brought to my attention that your account has been
‘hacked’ by an outside source. the charges added were significant, which
is how the error was caught. please temporarily change your password to
‘dph7’ so that we can judge the severity of the intrusion. i will notify
you when the problem has been taken care of. thank you for your help
in this matter. —system administrator.” and in 1999, aol users were
persistently receiving messages like: “a database error has deleted the
information for over 25,000 accounts, and yours is one. in order for us to
the human factor 267
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 267


 
 

____________________________________________

SecretsAndLies.pdf page: 296

____________________________________________

access the backup data, we do need your password. without your pass-
word, we will not be able to allow you to sign onto america online
within the next 24 hours after your opening of this letter.”
plausibility plus dread plus novelty equals compromise.
modern e-mail-borne viruses and worms use automatic social engi-
neering to entice people to open them. the iloveyou worm cloaked
itself in e-mail from people the recipient knew. it had a plausible subject
line and message body, enticing the recipient to open the attachment. it
hid the fact that it was a vbscript file, and pretended to be a harmless text
file. i talked about this in chapter 10; people don’t stand a chance against
these social-engineered viruses.
in some of these instances, technology can help. if the helpful
employees had access tokens (or biometric readers) in addition to pass-
words, they couldn’t give everything away to the nice man on the tele-
phone. if the computers had biometric fingerprint readers, there would be
no passwords to give away. if the computer system were smart enough to
recognize that someone was logging in from a remote location when the
job description states that he only works in the office, maybe someone
could have been alerted.
sometimes simple procedures can prevent social engineering. the
u.s. navy has safes with two locks (with different combinations, of
course); each combination is known by a different person. it’s much
harder to social engineer those combinations. there are probably other
tricks that the computers could have done, all designed to limit what a
duped legitimate user could give to a social engineer. technology can
certainly make the job of the social engineer harder, in some cases a lot
harder.
in the end, social engineering will probably always work. look at it
from the view of the victim, bob. bob is a good guy. he works at this
company, doing whatever low-level or mid-level job he was hired to do.
he’s not a corporate security officer. sure, he’s gotten some security
training, and might even know to be on the watch for those churlish
hackers. but bob is basically clueless. he doesn’t understand the security
of the system. he doesn’t understand the subtleties of an attack. he just
wants to get his job done. and he wants to be helpful.
the social engineer, alice, comes to bob with a problem. alice is just
like bob, a cog in the big company machine. she needs to get her job
done, too. all she wants is for bob to tell her his username and password,
268 c h a p t e r  s e v e n t e e n
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 268


 
 

____________________________________________

SecretsAndLies.pdf page: 297

____________________________________________

or give her information about a phone number, let her install this hard-
ware box, or do one of any number of perfectly reasonable things. sure,
it might not be technically allowed, but alice has her butt on the line and
just has to do this one thing. everyone bypasses security procedures once
in a while in order to get the job done. won’t bob help? isn’t he a team
player? doesn’t he know what it’s like to have to get something done and
for there to be a stupid corporate rule in the way? of course he does. he’s
human.
and this is why social engineering works. people are basically helpful.
and they are easily duped. by appealing to bob’s natural tendencies, alice
will always be able to cozen what she wants. she can persuade bob that
she is just like him. she can telephone bob when he least expects it. she
knows that security just gets in the way of bob doing the job he was hired
for, and she can play to that. and if she gets it wrong, and bob doesn’t fall
for it, she can call on the tens or hundreds of other bobs in the organiza-
tion that can give her what she wants.
the human factor 269
453803_ch17.qxd:453803_ch17.qxd  4/12/13  1:51 pm  page 269


 
 

____________________________________________

SecretsAndLies.pdf page: 317

____________________________________________

because they found the blind spot in the store’s video surveillance system
doesn’t mean they start shoplifting—but they can’t help looking.
threat modeling is a lot like this, and the only way to learn it is to do
it. so let’s start by stealing some pancakes.
our goal is to eat, without paying, at the local restaurant. and we’ve
got a lot of options. we can eat and run. we can pay with a fake credit
card, a fake check, or counterfeit cash. we can persuade another patron
to leave the restaurant without eating and eat his food. we can imperson-
ate (or actually become) a cook, a waiter, a manager, or the restaurant
owner (who might be someone that few workers have ever met). we
could snatch a plate off someone’s table before he eats it, or from under
the heat lamps before the waiters can get to it. we can wait at the dump-
ster for the busboy to throw away the leftovers. we can pull the fire alarm
and sneak in after everyone evacuates. we can even try to persuade the
manager that we’re some kind of celebrity who deserves a free breakfast,
or maybe we can find a gullible patron and talk her into paying for our
food. we could mug someone, nowhere near the restaurant, and buy the
pancakes. we can forge a coupon for free pancakes. and there’s always
the time-honored tradition of pulling a gun and shouting, “give me all
your pancakes.”
there are probably even more possibilities, but you get the idea.
looking at this list, most of the attacks have nothing to do with the point
where money changes hands. this is interesting, because it means that
securing the payment system does not prevent illicit pancake stealing.
it’s similar in the digital world. if this were a web-based digital pan-
cake store, most of the attacks would have nothing to do with the
electronic payment scheme. there are many other areas of vulnerability.
(remember the beautiful web page hack against shopping cart software
from chapter 10, where an attacker could change the price of an item to
an arbitrary amount. this brings up another possible attack: change the
menu so the pancakes cost $0.00.) the most fruitful attacks are rarely the
physical ones.
fair elections
let’s move on to bigger and better things. let’s rig an election. it’s a local
election—mayor of a town. cheating in elections is almost as old as elec-
tions themselves. how hard could it be?
threat modeling and risk assessment 289
453803_ch19.qxd:453803_ch19.qxd  4/12/13  2:20 pm  page 289


 
 

____________________________________________

SecretsAndLies.pdf page: 346

____________________________________________

318
21
attack trees
danaë was the daughter of acrisius. an oracle warned acrisius
that danaë’s son would someday kill him, so acrisius shut
danaë in a bronze room, away from anything even remotely
masculine. zeus had the hots for danaë, so he penetrated the bronze
room through the roof, in the form of a shower of gold that poured down
into her lap. danaë gave birth to perseus, and you can probably guess the
end of the story. 
threat modeling is, for the most part, ad hoc. you think about the
threats until you can’t think of any more, then you stop. and then you’re
annoyed and surprised when some attacker thinks of an attack you did-
n’t. my favorite example is a band of california art thieves that would
break into people’s houses by cutting a hole in their walls with a chain-
saw. the attacker completely bypassed the threat model of the defender.
the countermeasures that the homeowner put in place were door and
window alarms; they didn’t make a difference to this attack.
to help the process, i invented something called an attack tree. attack
trees provide a methodical way of describing threats against, and counter-
measures protecting, a system. by extension, attack trees provide a
methodical way of representing the security of systems. they allow you
to make calculations about security, compare the security of different sys-
tems, and do a whole bunch of other cool things.
basically, you represent attacks against a system in a tree structure,
with the goal as the root node and different ways of achieving that goal as
leaf nodes. by assigning values to the nodes, you can do some basic cal-
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 318


 
 

____________________________________________

SecretsAndLies.pdf page: 352

____________________________________________

to make this work, you must marry attack trees with knowledge
about attackers. different attackers have different levels of skill, access, risk
aversion, money, and so on. if you’re worried about organized crime, you
have to worry about expensive attacks and attackers who are willing to go
to jail. if you are worried about terrorists, you also have to worry about
attackers who are willing to die to achieve their goal. if you’re worried
about bored graduate students studying the security of your system, you
usually don’t have to worry about illegal attacks such as bribery and black-
mail. the characteristics of your attacker determine which parts of the
attack tree you have to worry about.
attack trees also let you play “what if” games with potential counter-
measures. in figure 21.6, for example, the goal has a cost of $20,000. this
is because the cheapest attack requiring no special equipment is bribing
the person who knows the combination. what if you implemented a
countermeasure—paying that person more so that he or she is less sus-
ceptible to bribes? if you assume that the cost to bribe that person is now
$80,000 (again, this is an example; in the real world you’d be expected to
research exactly how a countermeasure affects the node value), then the
cost increases to $60,000 (presumably to hire the thugs to do the threat-
ening).
pgp attack tree
figure 21.7 is an attack tree for the pgp e-mail security program. since
pgp is a complex program, this is a complex tree, and it’s easier to write
it in outline form than graphically. pgp has several security features, so
this is only one of several attack trees for pgp. this particular attack tree
has “read a message encrypted with pgp” as its goal. other goals might
be: “forge someone else’s signature on a message,” “change the signature
on a message,” “undetectably modify a pgp-signed or pgp-encrypted
message,” and so on.
if software can be modified (trojan horse) or corrupted (virus), it can
be used to have pgp generate an insecure public/private key pair (e.g.,
with a modulus whose factorization is known to the attacker). 
what immediately becomes apparent from the attack tree is that
breaking the rsa or idea encryption algorithms is not the most prof-
itable attack against pgp. there are many ways to read someone’s pgp-
324 c h a p t e r  t w e n t y - o n e
453803_ch21.qxd:453803_ch21.qxd  4/12/13  2:52 pm  page 324


 
 

____________________________________________

SecretsAndLies.pdf page: 377

____________________________________________

ability heard about the contest. maybe they were too busy doing other
things. maybe they weren’t able to break into the home, but they figured
out how to forge the real estate title to put the property in their name.
maybe they did break into the home, but took a look around and decided
to come back when there was something more valuable than a $10,000
prize at stake. the contest proved nothing. 
cryptanalysis contests are generally nothing more than a publicity
tool. sponsoring a contest, even a fair one, is no guarantee that people will
analyze the target. surviving a contest is no guarantee of no flaws in the
target.
evaluating and choosing security products
it’s generally not possible for average people—or the average company, or
the average government, for that matter—to create their own security
products. most often they’re forced to choose between an array of off-
the-shelf solutions and hope for the best. the lessons of this book, that it’s
practically impossible to design secure products and that most commercial
products are insecure, aren’t heartening. what can the harried system
administrator, charged with securing his embassy’s e-mail system or his
company’s network, do? what about the average citizen, concerned
about the security of different electronic commerce systems or the privacy
of her personal medical information?
the first question to ask is whether or not it really matters. or, more
exactly, whose security problem is this anyway? i care about my personal
privacy. i don’t really care about visa’s credit card fraud problems. they
limit my liability to $50, and will even waive that if i complain. i do care
about the pin on my atm card; if someone cleans out my account, it’s
my problem and not the bank’s.
similarly, some systems matter but are not within my control. i can’t
control what kind of firewalls and database security measures the irs uses
to protect my tax information, or my medical insurer uses to protect my
health records. maybe i can change insurers, but generally i don’t have
that kind of freedom. (i suppose that if i were wealthy enough, i could
choose banks in better regulatory environments—switzer land, for exam-
ple—but that option is out of reach of most people.) even if laws demand
product testing and verification 349
453803_ch22.qxd:453803_ch22.qxd  4/15/13  9:22 am  page 349


 
 

____________________________________________

SecretsAndLies.pdf page: 402

____________________________________________

detection and response
detection is much more important than prevention. as i have said
repeatedly in this book, it is fundamentally impossible to prevent attacks.
we can do demonstrably better than we are, but everything we know
about complex systems tells us that we cannot find and fix every vulnera-
bility. there will always be attackers; we just have to catch and punish
them.
i’m continuously amazed by how many computer-security vendors
are oblivious to this. you never see a door lock with the advertising slo-
gan: “this lock prevents burglaries.” but computer-security vendors
make those kinds of claims all the time: “firewalls prevent unauthorized
traffic from entering your internal network.” “authentication mecha-
nisms prevent unauthorized people from logging on to your computers.”
“encryption prevents unauthorized people from reading files.” all of
these claims are spurious. prevention mechanisms are good, but preven-
tion is only one part of a security solution—and the most fragile part.
effective security also includes detection and response.
in the real world, people understand this. banks don’t say: “we have
a vault, so we don’t need an alarm system.” museums don’t fire their
night guards because they have door and window locks. in the best of
worlds, all prevention buys you is time. in the real world, prevention can
often be bypassed completely.
in a few isolated cases, all you can rely on is prevention. against
eavesdropping attacks against a radio circuit, encryption (a prevention
countermeasure) has to work perfectly. there’s no way to detect the
eavesdropping, so no response is possible. most of the time, though,
detection and reaction are possible.
and they provide much more security. most home-security sys-
tems—door locks—can be defeated by a brick through a window. why
are more houses not robbed, then? why isn’t the public clamoring for
polycarbonate windows? because of detection and response.
detect attacks
modern society doesn’t prevent crime. it’s a myth. if alice wanted to kill
bob, she could. the police couldn’t stop her (unless she were a complete
idiot, i suppose). they can’t protect every bob in the world; they don’t
374 c h a p t e r  t w e n t y - f o u r
453803_ch24.qxd:453803_ch24.qxd  4/15/13  9:30 am  page 374


 
 

____________________________________________

SecretsAndLies.pdf page: 425

____________________________________________

i came to security from cryptography, and thought of the problem in
a military-like fashion. most writings about security come from this
perspective, and it can be summed up pretty easily: security threats are to
be avoided using preventive countermeasures.
this is how encryption works. the threat is eavesdropping, and
encryption provides the prophylactic. this could all be explained with
block diagrams. alice is communicating with bob; both are identified by
boxes, and there is a line between them signifying the communication.
eve is the eavesdropper; she also is a box and has a dotted line attached to
the communications line. she is able to intercept the communication.
the only way to prevent eve from learning what alice and bob are talk-
ing about is through a preventive countermeasure: encryption. there’s no
detection. there’s no response. there’s no risk management; you have to
avoid the threat.
for decades we have used this approach to computer security. we
draw boxes around the different players and lines between them. we
define different attackers—eavesdroppers, impersonators, thieves—and
their capabilities. we use preventive countermeasures like encryption and
access control to avoid different threats. if we can avoid the threats, we’ve
won. if we can’t, we’ve lost.
imagine my surprise when i learned that the world doesn’t work this
way. i had my epiphany in april 1999: that security was about risk man-
agement, that the process of security was paramount, that detection and
response was the real way to improve security, and that outsourcing was
the only way to make this happen effectively. it suddenly all made sense.
so i rewrote this book and reformed my company. counterpane systems
is now counterpane internet security, inc. we provide managed secu-
rity monitoring services—detection and response—for networks.
in the world of alice and bob and eve, that answer made no sense.
when the model was invented, communication was over radio or long
wires. detection isn’t possible. response isn’t possible. but in today’s
electronic world, it’s a lot more complicated. an attacker doesn’t passively
monitor a communication. he breaks into a firewall. he tries to steal
money using a forged smart card. he manipulates a digital network.
today’s world is much more like the physical world, with all its potential
for rich interaction.
and it’s not all or nothing. if eve could eavesdrop, she could eaves-
drop on everything. if she could not eavesdrop, she could not eavesdrop
afterword 397
453803_afterword.qxd:453803_afterword.qxd  4/12/13  11:08 am  page 397


 
 

____________________________________________

SecretsAndLies.pdf page: 427

____________________________________________

resources
the ideas in this book have been heavily influenced by the ideas
and writings of others. i deliberately did not disrupt the flow of
text with footnotes or citations. what follows is a list of some of
my more useful sources.
all urls are guaranteed accurate as of 1 july 2000. some internet
pundits have decried the web as useless for scholarly archives, claiming
that urls move or disappear regularly. consider this list to be an ongo-
ing experiment to prove or disprove that thesis.
ross anderson’s writing are always interesting and worth reading.
his web site is www.cl.cam.ac.uk/users/rja14/. look for his new book,
coming out next year: security engineering: a comprehensive guide to
building dependable distributed systems (john wiley & sons, 2001).
dorothy denning has written about cryptography, computer and
database security, and (more recently) information warfare. i used her
most recent book, information warfare and security (addison-wesley,
1999), as well has her classic cryptography and data security (addison-wes-
ley, 1982).
whit diffie’s writings and speeches have affected my thinking. i
recommend the book he co-wrote with susan landau: privacy on the line
(mit press, 1998).
carl ellison has continued to write common-sense essays and papers
on public-key infrastructure. much of his writing can be found on his
web site, world.std.com/~cme/.
ed felton has spoken on the insecurities inherent in software modu-
larity, and on java security. i always learn something when i hear him. i
first saw the figures on page 160 in one of his talks.
dan geer’s speeches have been similarly educational.
399
453803_resources.qxd:453803_resources.qxd  4/12/13  11:15 am  page 399


 
 

____________________________________________

Absolute.pdf page: 33

____________________________________________

introduction to c++      3
 the c language is peculiar because it is a high-level language with many of the 
features of a low-level language. c is somewhere in between the two extremes of a very 
high-level language and a low-level language, and therein lies both its strengths and 
its weaknesses. like (low-level) assembly language, c language programs can directly 
manipulate the computer’s memory. on the other hand, c has the features of a high-
level language, which makes it easier to read and write than assembly language. this 
makes c an excellent choice for writing systems programs, but for other programs 
(and in some sense even for systems programs) c is not as easy to understand as other 
languages; also, it does not have as many automatic checks as some other high-level 
languages.
 to overcome these and other shortcomings of c, bjarne stroustrup of at&t bell 
laboratories developed c++ in the early 1980s. stroustrup designed c++ to be a better 
c. most of c is a subset of c++ and so most c programs are also c++ programs. (the 
reverse is not true; many c++ programs are definitely not c programs.) unlike c, c++ 
has facilities for classes and so can be used for object-oriented programming. 
 c++ and object-oriented programming 
 object-oriented programming (oop) is a currently popular and powerful programming 
technique. the main characteristics of oop are encapsulation, inheritance, and polymorphism. 
encapsulation is a form of information hiding or abstraction. inheritance has to do 
with writing reusable code. polymorphism refers to a way that a single name can have 
multiple meanings in the context of inheritance. having made those statements, we must 
admit that they will hold little meaning for readers who have not heard of oop before. 
however, we will describe all these terms in detail later in this book. c++ accommodates 
oop by providing classes, a kind of data type combining both data and algorithms. 
c++ is not what some authorities would call a “pure oop language.” c++ tempers its 
oop features with concerns for efficiency and what some might call “practicality.” this 
combination has made c++ currently the most widely used oop language, although not 
all of its usage strictly follows the oop philosophy. 
 the character of c++ 
 c++ has classes that allow it to be used as an object-oriented language. it allows for 
overloading of functions and operators. (all these terms will be explained eventually, so 
do not be concerned if you do not fully understand some terms.) c++’s connection to 
the c language gives it a more traditional look than newer object-oriented languages, 
yet it has more powerful abstraction mechanisms than many other currently popular 
languages. c++ has a template facility that allows for full and direct implementation 
of algorithm abstraction. c++ templates allow you to code using parameters for types. 
the newest c++ standard, and most c++ compilers, allow multiple namespaces to 
accommodate more reuse of class and function names. the exception handling facilities 
in c++ are similar to what you would find in other programming languages. memory 
management in c++ is similar to that in c. the programmer must allocate his or her
own memory and handle his or her own garbage collection. most compilers will allow 


 
 

____________________________________________

Absolute.pdf page: 158

____________________________________________

128     chapter 3    function basics
if a function is well designed, the programmer can use the function as if it were a 
black box. all the programmer needs to know is that if he or she puts appropriate 
arguments into the black box, then it will take some appropriate action. designing a 
function so that it can be used as a black box is sometimes called information hiding
to emphasize the fact that the programmer acts as if the body of the function were 
hidden from view. 
 writing and using functions as if they were black boxes is also called  procedural
abstraction . when programming in c++ it might make more sense to call it  functional
abstraction . however,  procedure is a more general term than  function and computer 
scientists use it for all “function-like” sets of instructions, and so they prefer the 
 term  procedural abstraction . the term  abstraction is intended to convey the idea that 
when you use a function as a black box, you are abstracting away the details of the code 
contained in the function body. you can call this technique the black box principle or 
the principle of procedural abstraction or  information hiding . the three terms mean the 
same thing. whatever you call this principle, the important point is that you should 
use it when designing and writing your function definitions. 
 information 
hiding 
 procedural 
abstraction 
 procedural abstraction 
when applied to a function definition, the principle of procedural abstraction means that 
your function should be written so that it can be used like a black box . this means that the 
programmer who uses the function should not need to look at the body of the function 
definition to see how the function works. the function declaration and the accompanying 
comment should be all the programmer needs to know in order to use the function. to 
ensure that your function definitions have this important property, you should strictly adhere 
to the following rules: 
 how to write a black-box function definition 
 ■  the function declaration comment should tell the programmer any and all conditions 
that are required of the arguments to the function and should describe the result of a 
function invocation. 
 ■  all variables used in the function body should be declared in the function body. 
(the formal parameters do not need to be declared, because they are listed in the 
 function heading.) 
 global constants and global variables 
 as we noted in  chapter  1 , you can and should name constant values using the  const
modifier. for example, in  display  3.5 we used the  const modifier to give a name to 
the rate of sales tax with the following declaration: 
 const double taxrate = 0.05; //5%  sales tax 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 169

____________________________________________

programming projects      139
since you will use the previous formula, the gravitational force will be in dynes. 
one dyne equals a 
 g • cm/sec 2 
   you should use a globally defined constant for the universal gravitational constant. 
embed your function definition in a complete program that computes the gravita-
tional force between two objects given suitable inputs. your program should allow 
the user to repeat this calculation as often as the user wishes. 
 5.  write a program that asks for the user’s height, weight, and age, and then computes 
clothing sizes according to the following formulas. 
  •  hat size = weight in pounds divided by height in inches and all that multiplied 
by 2.9. 
   •  jacket size (chest in inches) = height times weight divided by 288 and then 
adjusted by adding one-eighth of an inch for each 10 years over age 30. (note 
that the adjustment only takes place after a full 10 years. so, there is no adjust-
ment for ages 30 through 39, but one-eighth of an inch is added for age 40.) 
   •  waist in inches = weight divided by 5.7 and then adjusted by adding one-tenth 
of an inch for each 2 years over age 28. (note that the adjustment only takes 
place after a full 2 years. so, there is no adjustment for age 29, but one-tenth 
of an inch is added for age 30.) 
   use functions for each calculation. your program should allow the user to repeat 
this calculation as often as he or she wishes. 
  6. write a function that computes the average and standard deviation of four scores. 
the standard deviation is defined to be the square root of the average of the four 
values: (s i  -  a ) 2 , where  a is the average of the four scores s 1 , s 2 , s 3 , and  s4 . the func-
tion will have six parameters and will call two other functions. embed the function 
in a program that allows you to test the function again and again until you tell the 
program you are finished. 
 7.  in cold weather, meteorologists report an index called the  wind chill factor , which 
takes into account the wind speed and the temperature. the index provides a 
measure of the chilling effect of wind at a given air temperature. wind chill may 
be approximated by the following formula, 
   w = 33 -
1102v - v + 10 .52 133 - t2
23 .1
   where 
    v = wind speed in m/sec 
    t = temperature in degrees celsius: t 6= 10 
    w = wind chill index (in degrees celsius) 
   write a function that returns the wind chill index. your code should ensure that 
the restriction on the temperature is not violated. look up some weather reports 
in back issues of a newspaper in your library and compare the wind chill index you 
calculate with the result reported in the newspaper. 


 
 

____________________________________________

Absolute.pdf page: 170

____________________________________________

  8 . write a program that outputs all 99 stanzas of the “ninety-nine bottles of beer 
on the wall” song. your program should print the number of bottles in english, 
not as a number: 
   ninety-nine bottles of beer on the wall, 
   ninety-nine bottles of beer, 
   take one down, pass it around, 
   ninety-eight bottles of beer on the wall. 
   … 
   one bottle of beer on the wall, 
   one bottle of beer, 
   take one down, pass it around, 
   zero bottles of beer on the wall. 
   your program should not use ninety-nine different output statements! 
  9.  in the game of craps, a “pass line” bet proceeds as follows. the first roll of the two, 
six-sided dice in a craps round is called the “come out roll.” the bet immediately 
wins when the come out roll is 7 or 11, and loses when the come out roll is 2, 3, 
or 12. if 4, 5, 6, 8, 9, or 10 is rolled on the come out roll, that number becomes 
“the point.” the player keeps rolling the dice until either 7 or the point is rolled. 
if the point is rolled first, then the player wins the bet. if the player rolls a 7 first, 
then the player loses. 
   write a program that plays craps using those rules so that it simulates a game without 
human input. instead of asking for a wager, the program should  calculate whether 
the player would win or lose. create a function that simulates rolling the two dice 
and returns the sum. add a loop so that the program plays 10,000 games. add 
counters that count how many times the player wins, and how many times the 
player loses. at the end of the 10,000 games, compute the probability of winning, 
as wins / (wins + losses), and output this value. over the long run, who is going 
to win more games of craps, you or the house? 
 10.  one way to estimate the height of a child is to use the following formula, which 
uses the height of the parents: 
   hmale_child = (( hmother 13>12) + hfather )>2
   hfemale_child = (( hfather 12>13) + hmother )>2
   all heights are in inches. write a function that takes as input parameters the gen-
der of the child, height of the mother in inches, and height of the father in inches, 
and outputs the estimated height of the child in inches. embed your function in a 
program that allows you to test the function over and over again until telling the 
program to exit. the user should be able to input the heights in feet and inches, 
and the program should output the estimated height of the child in feet and inches. 
use the integer data type to store the heights. 
140     chapter 3    function basics
solution to 
programming
project 3.9 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 171

____________________________________________

programming projects      141
 11.  the game of pig is a simple two player dice game in which the first player to 
reach 100 or more points wins. players take turns. on each turn a player rolls a 
six-sided die: 
   •  if the player rolls a 2–6 then he or she can either 
    — roll again or 
    —  hold. at this point the sum of all rolls made this turn is added to the 
player’s total score and it becomes the other player’s turn. 
   •  if the player rolls a 1 then the player loses his or her turn. the player gets no 
new points and it becomes the opponent’s turn. 
   if a player reaches 100 or more points after holding then the player wins. 
   write a program that plays the game of pig, where one player is a human and the 
other is the computer. allow the human to input “r” to roll again or “h” to hold. 
   the computer program should play according to the following rule: keep rolling 
on the computer’s turn until it has accumulated 20 or more points, then hold. of 
course, if the computer wins or rolls a 1 then the turn ends immediately. allow the 
human to roll first. 
   write your program using at least two functions: 
   int humanturn( int humantotalscore); 
   int computerturn( int computertotalscore); 
   these functions should perform the necessary logic to handle a single turn for 
either the computer or the human. the input parameter is the total score for the 
human or computer. the functions should return the turn total to be added to the 
total score upon completion of the turn. for example, if the human rolls a 3 and 
6 and then holds, then humanturn should return 9. however, if the human rolls a 
3 and 6 and then a 1, then the function should return 0. 
 12.  write a program that inputs a date (e.g., july 4, 2008) and outputs the day of 
the week that corresponds to that date. the following algorithm is from http://
en.wikipedia.org/wiki/calculating_the_day_of_the_week. the implementation 
will require several functions: 
   bool isleapyear(int year);
   this function should return  true if  year is a leap year and  false if it is not. here 
is pseudocode to determine a leap year: 
    leap_year = ((year divisible by 400) or (year divisible by 4 and year not divisible 
by 100)) 
   int getcenturyvalue(int year); 
   this function should take the first two digits of the year (i.e., the century),  divide 
by 4, and save the remainder. subtract the remainder from 3 and return this 
value multiplied by 2. for example, the year 2008 becomes (20/4) = 5 remainder 
0. 3 - 0 = 3. return 3 * 2 = 6. 
   int getyearvalue(int year); 


 
 

____________________________________________

Absolute.pdf page: 189

____________________________________________

parameters      159
 6 void swapvalues(int variable1, int variable2); 
 7 //interchanges the values of variable1 and variable2 . 
 8 void showresults(int output1, int output2); 
 9 //shows the values of variable1 and variable2, in that order . 
 10 int main( ) 
 11  { 
 12 int firstnum, secondnum; 
 13    getnumbers(firstnum, secondnum); 
 14    swapvalues(firstnum, secondnum); 
 15    showresults(firstnum, secondnum); 
 16 return 0; 
 17  } 
 18 void swapvalues(int variable1, int variable2) 
 19  { 
 20 int temp; 
 21      temp = variable1; 
 22      variable1 = variable2; 
 23      variable2 = temp; 
 24  } 
 25 the definitions of  getnumbers and 
 26 showresults are the same as in  display  4.2 . 
 sample dialogue 
enter two integers: 5 6 
in reverse order the numbers are: 5 6 
display 4.4 inadvertent local variable (part 2 of 2)
forgot the & here
inadvertent 
local variables
error due to 
inadvertent local 
variables
forgot the & here
 tip: choosing formal parameter names 
 functions should be self-contained modules that are designed separately from the 
rest of the program. on large programming projects, different programmers may 
be assigned to write different functions. the programmer should choose the most 
meaningful names he or she can find for formal parameters. the arguments that will 
be substituted for the formal parameters may well be variables in another function or 
in the main function. these variables should also be given meaningful names, often 
chosen by someone other than the programmer who writes the function definition. 
this makes it likely that some or all arguments will have the same names as some of 
the formal parameters. this is perfectly acceptable. no matter what names are chosen 
for the variables that will be used as arguments, these names will not produce any 
confusion with the names used for formal parameters. ■


 
 

____________________________________________

Absolute.pdf page: 209

____________________________________________

programming projects      179
 14.  //this is just a stub. 
   double rainprob( double pressure, 
           double humidity, double temp) 
   {
       return 0.25; //not correct,
                    //but good enough for some testing.
   }
 programming projects 
visit www.myprogramminglab.com to complete select exercises online and get instant 
feedback.
 1.  write a program that converts from 24-hour notation to 12-hour notation. for 
example, it should convert 14:25 to 2:25 p.m. the input is given as two integers. 
there should be at least three functions: one for input, one to do the conversion, 
and one for output. record the a.m./p.m. information as a value of type char , 
'a' for a.m. and  'p' for p.m. thus, the function for doing the conversions will 
have a call-by-reference formal parameter of type  char to record whether it is a.m. 
or p.m. (the function will have other parameters as well.) include a loop that lets 
the user repeat this computation for new input values again and again until the user 
says he or she wants to end the program. 
 2.  the area of an arbitrary triangle can be computed using the formula 
area = 2s1s - a2 1s - b2 1s - c2
   where a, b, and c are the lengths of the sides, and s is the semiperimeter. 
s = 1a + b + c2 >2 
   write a  void function that uses five parameters: three value parameters that pro-
vide the lengths of the edges, and two reference parameters that compute the area 
and perimeter ( not the semiperimeter ). make your function robust. note that not 
all combinations of a, b, and c produce a triangle. your function should produce 
correct results for legal data and reasonable results for illegal combinations. 
 3.  write a program that tells what coins to give out for any amount of change from 
1 cent to 99 cents. for example, if the amount is 86 cents, the output would be 
something like the following: 
   86 cents can be given as 
   3 quarter(s) 1 dime(s) and 1 penny(pennies) 
   use coin denominations of 25 cents (quarters), 10 cents (dimes), and 1 cent (pen-
nies). do not use nickel and half-dollar coins. your program will use the following 
function (among others): 
void computecoin( int coinvalue, int& number, int& amountleft); 
   //precondition: 0 < coinvalue < 100; 0 <= amountleft < 100. 
   //postcondition: number has been set equal to the maximum number 
   //of coins of denomination coinvalue cents that can be obtained 


 
 

____________________________________________

Absolute.pdf page: 210

____________________________________________

   //from amountleft cents. amountleft has been decreased by the 
   //value of the coins, that is, decreased by number*coinvalue .
   for example, suppose the value of the variable  amountleft is  86 . then, after the 
following call, the value of number will be  3 and the value of  amountleft will be  11
(because if you take three quarters from 86 cents, that leaves 11 cents): 
   computecoins(25, number, amountleft); 
   include a loop that lets the user repeat this computation for new input values until 
the user says he or she wants to end the program. ( hint: use integer division and 
the % operator to implement this function.) 
 4.  write a program that will read in a length in feet and inches and output the equiva-
lent length in meters and centimeters. use at least three functions: one for input, 
one or more for calculating, and one for output. include a loop that lets the user 
repeat this computation for new input values until the user says he or she wants to 
end the program. there are 0.3048 meters in a foot, 100 centimeters in a meter, 
and 12 inches in a foot. 
 5.  write a program like that of the previous exercise that converts from meters and 
centimeters into feet and inches. use functions for the subtasks. 
 6.  (you should do the previous two programming projects before doing this one.) 
write a program that combines the functions in the previous two programming 
projects. the program asks the user if he or she wants to convert from feet and 
inches to meters and centimeters or from meters and centimeters to feet and inches. 
the program then performs the desired conversion. have the user respond by typ-
ing the integer 1 for one type of conversion and  2 for the other conversion. the 
program reads the user’s answer and then executes an if-else statement. each 
branch of the if-else statement will be a function call. the two functions called 
in the if-else statement will have function definitions that are very similar to 
the programs for the previous two programming projects. thus, they will be fairly 
complicated function definitions that call other functions. include a loop that lets 
the user repeat this computation for new input values until the user says he or she 
wants to end the program. 
 7.  write a program that will read in a weight in pounds and ounces and will output 
the equivalent weight in kilograms and grams. use at least three functions: one for 
input, one or more for calculating, and one for output. include a loop that lets 
the user repeat this computation for new input values until the user says he or she 
wants to end the program. there are 2.2046 pounds in a kilogram, 1000 grams in 
a kilogram, and 16 ounces in a pound. 
 8.  write a program like that of the previous exercise that converts from kilograms and 
grams into pounds and ounces. use functions for the subtasks. 
 9.  (you should do the previous two programming projects before doing this one.) 
write a program that combines the functions of the previous two programming 
projects. the program asks the user if he or she wants to convert from pounds 
and ounces to kilograms and grams or from kilograms and grams to pounds and 
ounces. the program then performs the desired conversion. have the user respond 
by typing the integer 1 for one type of conversion and  2 for the other. the program 
180     chapter 4    parameters and overloading
solution to 
programming
project 4.4 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 211

____________________________________________

programming projects      181
reads the user’s answer and then executes an if-else statement. each branch of 
the if-else statement will be a function call. the two functions called in the 
if-else statement will have function definitions that are very similar to the programs 
for the previous two programming projects. thus, they will be fairly complicated 
function definitions that call other functions in their function bodies. include a 
loop that lets the user repeat this computation for new input values until the user 
says he or she wants to end the program. 
 10.  (you should do programming projects 4.6 and 4.9 before doing this program-
ming project.) write a program that combines the functions of programming 
projects 4.6 and 4.9. the program asks the user if he or she wants to convert 
lengths or weights. if the user chooses lengths, then the program asks the user if 
he or she wants to convert from feet and inches to meters and centimeters or from 
meters and centimeters to feet and inches. if the user chooses weights, a similar 
question about pounds, ounces, kilograms, and grams is asked. the program then 
performs the desired conversion. have the user respond by typing the integer 1 for 
one type of conversion and 2 for the other. the program reads the user’s answer 
and then executes an if-else statement. each branch of the  if-else statement 
will be a function call. the two functions called in the if-else statement will 
have function definitions that are very similar to the programs for programming 
projects 4.6 and 4.9. thus, these functions will be fairly complicated func-
tion definitions that call other functions; however, they will be very easy to 
write by adapting the programs you wrote for programming projects 4.6 and 
4.9. notice that your program will have if-else statements embedded inside of 
if-else statements, but only in an indirect way. the outer  if-else statement 
will include two function calls, as its two branches. these two function calls will 
each in turn include an if-else statement, but you need not think about that. 
they are just function calls and the details are in a black box that you create when 
you define these functions. if you try to create a four-way branch, you are probably 
on the wrong track. you should only need to think about two-way branches (even 
though the entire program does ultimately branch into four cases). include a loop 
that lets the user repeat this computation for new input values until the user says 
he or she wants to end the program. 
 11.  you are a contestant on a game show and have won a shot at the grand prize. 
before you are three doors. $1,000,000 in cash has randomly been placed behind 
one door. behind the other two doors are the consolation prizes of dishwasher 
 detergent. the game show host asks you to select a door, and you randomly pick 
one. however, before revealing the prize behind your door, the game show host 
reveals one of the other doors that contains a consolation prize. at this point, the 
game show host asks if you would like to stick with your original choice or to 
switch to the remaining door. 
   write a function to simulate the game show problem. your function should randomly 
select locations for the prizes, select a door at random chosen by the contestant, 
and then determine whether the contestant would win or lose by sticking with 
solution to 
programming
project 4.11 
videonote


 
 

____________________________________________

Absolute.pdf page: 261

____________________________________________

programming projects      231
   after you have completed the previous program, produce an enhanced version 
that also outputs a graph showing the average rainfall and the actual rainfall for 
each of the previous 12 months. the graph should be similar to the one shown in 
 display  5.4 , except that there should be two bar graphs for each month and they 
should be labeled as the average rainfall and the rainfall for the most recent month. 
your program should ask the user whether he or she wants to see the table or the 
bar graph, and then should display whichever format is requested. include a loop 
that allows the user to see either format as often as the user wishes until the user 
requests that the program end. 
 2.  write a function called  deleterepeats that has a partially filled array of characters 
as a formal parameter and that deletes all repeated letters from the array. since a 
partially filled array requires two arguments, the function will actually have two 
formal parameters: an array parameter and a formal parameter of type int that 
gives the number of array positions used. when a letter is deleted, the remaining 
letters are moved forward to fill in the gap. this will create empty positions at 
the end of the array so that less of the array is used. since the formal parameter is 
a partially filled array, a second formal parameter of type int will tell how many 
array positions are filled. this second formal parameter will be a call-by-reference 
parameter and will be changed to show how much of the array is used after the 
repeated letters are deleted. for example, consider the following code: 
   char a[10]; 
   a[0] = 'a'; 
   a[1] = 'b'; 
   a[2] = 'a'; 
   a[3] = 'c'; 
   int size = 4; 
   deleterepeats(a, size); 
 after this code is executed, the value of  a[0] is  'a' , the value of  a[1] is  'b' , the 
value of a[2] is  'c' , and the value of  size is  3 . (the value of  a[3] is no longer of 
any concern, since the partially filled array no longer uses this indexed variable.) 
you may assume that the partially filled array contains only lowercase letters. 
embed your function in a suitable test program. 
 3.  the standard deviation of a list of numbers is a measure of how much the num-
bers deviate from the average. if the standard deviation is small, the numbers are 
clustered close to the average. if the standard deviation is large, the numbers are 
scattered far from the average. the standard deviation, s , of a list of  n numbers x
i
is defined as follows, 
  s = ha
n
i = 1
1xi - x22
n
   where  x is the average of the  n numbers  x
1
 ,  x
2
 , …. define a function that takes a 
partially filled array of numbers as its argument and returns the standard deviation 
of the numbers in the partially filled array. since a partially filled array requires two 
arguments, the function will actually have two formal parameters: an array parameter 


 
 

____________________________________________

Absolute.pdf page: 346

____________________________________________

 3.  my mother always took a little red counter to the grocery store. the counter was 
used to keep tally of the amount of money she would have spent so far on that visit 
to the store if she bought everything in the basket. the counter had a four-digit 
display, increment buttons for each digit, and a reset button. an overflow indicator 
came up red if more money was entered than the $99.99 it would register. (this 
was a long time ago.) 
   write and implement the member functions of a class counter that simulates and 
slightly generalizes the behavior of this grocery store counter. the constructor should 
create a counter object that can count up to the constructor’s argument. that is, 
counter(9999) should provide a counter that can count up to  9999 . a newly con-
structed counter displays a reading of 0 . the member function  void reset( );
sets the counter’s number to 0 . the member function  void incr1( ); increments 
the units digits by 1 ,  void incr10( ); increments the tens digit by  1 , and void
incr100( ); and  void incr1000( ); increment the next two digits, respectively. 
accounting for any carrying when you increment should require no further action than 
adding an appropriate number to the private data member. a member function bool
overflow( ); detects overflow. (overflow is the result of incrementing the counter’s 
private data member beyond the maximum entered at counter construction.) 
  use this class to provide a simulation of my mother’s little red clicker. even though the 
display is an integer, in the simulation, the rightmost (lower order) two digits are always 
thought of as cents and tens of cents, the next digit is dollars, and the fourth digit is tens 
of dollars. 
  provide keys for cents, dimes, dollars, and tens of dollars. unfortunately, no choice of 
keys seems particularly mnemonic. one choice is to use the keys asdfo: a for cents, 
followed by a digit 1 to  9 ;  s for dimes, followed by a digit  1 to  9 ;  d for dollars, followed
by a digit  1 to  9 ; and  f for tens of dollars, again followed by a digit  1 to  9 . each entry 
(one of asdf followed by  1 to  9) is followed by pressing the return key. any overflow 
is reported after each operation. overflow can be requested by pressing the o key. 
 4 . you operate several hot dog stands distributed throughout town. define a class named 
hotdogstand that has a member variable for the hot dog stand’s id number and a 
member variable for how many hot dogs the stand sold that day. create a construc-
tor that allows a user of the class to initialize both values. 
   also create a function named  justsold that increments the number of hot dogs 
the stand has sold by one. this function will be invoked each time the stand sells a 
hot dog so that you can track the total number of hot dogs sold by the stand. add 
another function that returns the number of hot dogs sold. 
   finally, add a static variable that tracks the total number of hot dogs sold by all 
hot dog stands and a static function that returns the value in this variable. write 
a main function to test your class with at least three hot dog stands that each sell a 
variety of hot dogs. 
 5.  in an ancient land, the beautiful princess eve had many suitors. she decided on 
the following procedure to determine which suitor she would marry. first, all of 
the suitors would be lined up one after the other and assigned numbers. the first 
316     chapter 7    constructors and other tools
solution to 
programming
project 7.4 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 347

____________________________________________

suitor would be number 1, the second number 2, and so on up to the last suitor, 
number n. starting at the first suitor she would then count three suitors down 
the line (because of the three letters in her name) and the third suitor would be 
eliminated from winning her hand and removed from the line. eve would then 
continue, counting three more suitors, and eliminating every third suitor. when 
she reached the end of the line she would continue counting from the beginning. 
  for example, if there were six suitors then the elimination process would proceed 
as follows: 
  123456 initial list of suitors, start counting from 1 
  12456 suitor 3 eliminated, continue counting from 4 
   1245 suitor 6 eliminated, continue counting from 1 
  125 suitor 4 eliminated, continue counting from 5 
   15  suitor 2 eliminated, continue counting from 5 
  1  suitor 5 eliminated, 1 is the lucky winner 
   write a program that uses a vector to determine which position you should stand 
in to marry the princess if there are n suitors. you will find the following function 
from the vector class useful:
v.erase(iter);
// removes element at position iter
   for example, to use this function to erase the fourth element from the beginning 
of a vector variable named thevector , use 
thevector.erase(thevector.begin( ) + 3); 
   the number 3 is used because the first element in the vector is at index position 0. 
 6.  this programming project requires you to first complete programming 
project 6.7 from  chapter  6 , which is an implementation of a  pizza class.  add 
an order class that contains a private vector of type   pizza . this class  represents 
a customer’s entire order, where the order may consist of multiple  pizzas. include 
appropriate functions so that a user of the  order class can add pizzas to the order 
(type is deep dish, hand tossed, or pan; size is small, medium, or large;  number 
of pepperoni or cheese toppings). you can use constants to represent the type and 
size. also write a function that outputs everything in the order along with the total 
price. write a suitable test program that adds multiple pizzas to an order(s). 
  7.  do  programming project 6.8 , the definition of a money class, except  create a 
 default constructor that sets the monetary amount to 0 dollars and 0 cents, and 
create a second constructor with input parameters for the amount of the dollars 
and cents variables. modify your test code to invoke the constructors. 
programming projects      317
solution to 
programming
project 7.7 
videonote


 
 

____________________________________________

Absolute.pdf page: 375

____________________________________________

references and more overloaded operators      345
 we will only be returning a reference when defining certain kinds of overloaded 
operators.
 l-values and r-values 
the term  l-value is used for something that can appear on the left-hand side of an assignment 
operator. the term r-value is used for something that can appear on the right-hand side of an 
assignment operator. 
if you want the object returned by a function to be an l-value, it must be returned by 
reference.
(continued)
 tip: returning member variables of a class type 
 when returning a member variable of a class type, in almost all cases it is important 
to return the member value by const value. to see why, suppose you do not, as in 
the example outlined in what follows: 
class employee 
{
public:
    money& getsalary( ) { return salary; } 
    ... 
private:
    money salary; 
    ... 
};
 in this example,  salary is a private member variable that should not be changeable 
except by using some accessor function of the class employee . the  getsalary
function returns the variable salary which is of type  money . if we do not return 
salary by reference then a new, temporary copy of  salary will be created and 
returned. we might wish to avoid this overhead by returning a reference to salary
as indicated in the example. however, even though salary is declared as private, this 
privateness is easily circumvented as follows: 
employee joe; 
(joe.getsalary( )).input( ); 
 the lucky employee named  joe can now enter any salary she wishes! 
 on the other hand, suppose  getsalary returns its value by  const value, as follows: 
class employee 
{
public:
const money& getsalary( ) { return salary; } 
. . . 


 
 

____________________________________________

Absolute.pdf page: 411

____________________________________________

character manipulation tools      381
 9.2 character manipulation tools 
 they spell it vinci and pronounce it vinchy; foreigners always spell better than 
they pronounce. 
mark twain, the innocents abroad
 any form of string is ultimately composed of individual characters. thus, when doing 
string processing it is often helpful to have tools at your disposal to test and manipulate 
individual values of type char . this section is about such tools. 
 character i/o 
 all data is input and output as character data. when your program outputs the 
number 10 , it is really the two characters  '1' and  '0' that are output. similarly, 
when the user wants to type in the number 10, he or she types in the character 
'1'  followed by the character  '0' . whether the computer interprets this  "10" as 
two characters or as the number 10 depends on how your program is written. but, 
however your program is written, the computer hardware is always reading the 
 self-test exercises 
 13. consider the following code (and assume it is embedded in a complete and 
correct program and then run): 
char a[80], b[80]; 
cout << "enter some input:\n"; 
cin >> a >> b; 
cout << a << '-' << b << "end of output\n"; 
 if the dialogue begins as follows, what will be the next line of output? 
enter some input: 
the
time is now. 
 14. consider the following code (and assume it is embedded in a complete and 
correct program and then run): 
char mystring[80]; 
cout << "enter a line of input:\n"; 
cin.getline(mystring, 6); 
cout << mystring << "<end of output"; 
 if the dialogue begins as follows, what will be the next line of output? 
enter a line of input: 
may the hair on your toes grow long and curly. 


 
 

____________________________________________

Absolute.pdf page: 444

____________________________________________

414     chapter 9    strings
   the input 
   mary a. user 
   should also produce the output 
   user, mary a. 
   your program should place a period after the middle initial even if the input did 
not contain a period. your program should allow for users who give no middle 
name or middle initial. in that case, the output, of course, contains no middle 
name or initial. for example, the input 
   mary user 
   should produce the output 
   user, mary 
   if you are using c-strings, assume that each name is at most 20 characters long. 
 alternatively, use the class  string . ( hint: you may want to use three string vari-
ables rather than one large string variable for the input. you may find it easier to 
not use  getline .) 
 4.  write a program that reads in a line of text and replaces all four-letter words with 
the word "love" . for example, the input string 
   i hate you, you dodo! 
   should produce the following output: 
   i love you, you love! 
   of course, the output will not always make sense. for example, the input string 
   john will run home. 
   should produce the following output: 
   love love run love. 
   if the four-letter word starts wit h a capital letter, it should be replaced by  "love" , 
not by "love" . you need not check capitalization, except for the first letter of a 
word. a word is any string consisting of the letters of the alphabet and delimited at 
each end by a blank, the end of the line, or any other character that is not a letter. 
your program should repeat this action until the user says to quit. 
 5.  write a program that can be used to train the user to use less sexist language by 
suggesting alternative versions of sentences given by the user. the program will ask 
for a sentence, read the sentence into a string variable, and replace all occurrences 
of masculine pronouns with gender-neutral pronouns. for example, it will replace 
"he" with  "she or he" . thus, the input sentence 
   see an adviser, talk to him, and listen to him. 
   should produce the following suggested changed version of the sentence: 
   see an adviser, talk to her or him, and listen to her or him. 
   be sure to preserve uppercase letters for the first word of the sentence. the pronoun 
"his" can be replaced by  "her(s)" ; your program need not decide between  "her"
and "hers" . allow the user to repeat this for more sentences until the user says she 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 592

____________________________________________

562     chapter 12    streams and file i/o
  b.  for a sorted file, a quartile is one of three numbers: the first has one-fourth the 
data values less than or equal to it, one-fourth the data values between the first and 
second numbers (up to and including the second number), one-fourth the data 
points between the second and the third (up to and including the third number), 
and one-fourth above the third quartile. find the three quartiles for the data file you 
used for part a. note that “one-fourth” means as close to one-fourth as possible. 
hint : you should recognize that having done part a you have one-third of your 
job done. (you have the second quartile already.) you also should recognize that 
you have done almost all the work toward finding the other two quartiles as well. 
 4.  write a program that takes its input from a file of numbers of type  double . the 
program outputs to the screen the average and standard deviation of the numbers 
in the file. the file contains nothing but numbers of type double separated by 
blanks and/or line breaks. the standard deviation of a list of numbers n
1
, n
2
, n
3
,
and so forth, is defined as the square root of the average of the following numbers: 
  (n
1
- a)2 , (n
2
- a)2 , (n
3
- a)2 , and so forth 
 the number  a is the average of the numbers n
1
 , n
2
 , n
3
 , and so forth. 
hint : write your program so that it first reads the entire file and computes the 
 average of all the numbers, then closes the file, then reopens the file and com-
putes the standard deviation. you will find it helpful to first do  programming 
project  12.2 and then modify that program to obtain the program for this project. 
 5.  write a program that gives and takes advice on program writing. the program 
starts by writing a piece of advice to the screen and asking the user to type in a 
 different piece of advice. the program then ends. the next person to run the pro-
gram receives the advice given by the person who last ran the program. the advice 
is kept in a file, and the contents of the file change after each run of the program. 
you can use your editor to enter the initial piece of advice in the file so that the 
first person who runs the program receives some advice. allow the user to type in 
advice of any length (any number of lines long). the user is told to end his or her 
advice by pressing the return key two times. your program can then test to see that 
it has reached the end of the input by checking to see when it reads two consecutive 
occurrences of the character '\n' . 
 6.  write a program that merges the numbers in two files and writes all the numbers 
into a third file. your program takes input from two different files and writes its 
output to a third file. each input file contains a list of numbers of type int in 
sorted order from the smallest to the largest. after the program is run, the output 
file will contain all the numbers in the two input files in one longer list in sorted 
order from smallest to largest. your program should define a function that is called 
with the two input-file streams and the output-file stream as three arguments. 
 7.  write a program to generate personalized junk mail. the program takes input 
both from an input file and from the keyboard. the input file contains the text of 
a letter, except that the name of the recipient is indicated by the three characters 
#n# . the program asks the user for a name and then writes the letter to a second 
file but with the three letters #n# replaced by the name. the three-letter string  #n#
will occur exactly once in the letter. 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 597

____________________________________________

programming projects      567
 depending on the speed of your computer and your implementation, execution of 
this program may take from minutes to hours. 
 18.  the text files  boynames.txt and  girlnames.txt , which are included in the 
source code on this  book’s website , contain a list of the 1,000 most popular boy 
and girl names in the united states for the year 2003 as compiled by the social 
security administration. 
 these are blank-delimited files where the most popular name is listed first, the 
second most popular name is listed second, and so on to the 1,000th most popular 
name, which is listed last. each line consists of the first name, followed by a blank 
space, followed by the number of registered births in the year using that name. for 
example, the girlnames.txt file begins with 
 emily 25494 
 emma 22532 
 madison 19986 
 this indicates that emily is the most popular name with 25,494 registered nam-
ings, emma is the second most popular with 22,532, and madison is the third 
most popular with 19,986. 
 write a program that reads both the girl’s and boy’s files into memory using arrays. 
then, allow the user to input a name. the program should search through both 
arrays and, when there is a match, output the popularity and number of namings. 
the program should also indicate if there is no match. 
 for example, if the user enters the name “justice,” the program should output 
 justice is ranked 456 in popularity among girls with 655 namings. 
 justice is ranked 401 in popularity among boys with 653 namings. 
 if the user enters the name “walter,” the program should output 
 walter is not ranked among the top 1000 girl names. 
 walter is ranked 356 in popularity among boys with 775 namings. 
 19.  html files use tags enclosed in angle brackets to denote formatting instructions. 
for example, <b> indicates bold and  <i> indicates italics. if a web browser is dis-
playing an html document that contains < or >, it may mistake these symbols 
for tags. this is a common problem with c++ files, which contain many <’s and 
>’s. for example, the line "#include <iostream>" may result in the browser 
interpreting <iostream> as a tag. 
 to avoid this problem, html uses special symbols to denote < and >. the <
symbol is created with the string &lt; while the > symbol is created with the 
string &gt; . 
 write a program that reads in a c++ source file and converts all < symbols to 
& it; and all > symbols to &gt;. also add the tag <pre> to the beginning of the 
file and </pre> to the end of the file. this tag preserves whitespace and formatting 
in the html document. your program should output the html file to disk. 


 
 

____________________________________________

Absolute.pdf page: 602

____________________________________________

 after a lecture on cosmology and the structure of the solar system, 
william james was accosted by a little old lady. 
 “your theory that the sun is the center of the solar system, and the 
earth is a ball which rotates around it has a very convincing ring to it, 
mr. james, but it’s wrong. i’ve got a better theory,” said the little old lady. 
 “and what is that, madam?” inquired james politely. 
 “that we live on a crust of earth which is on the back of a giant turtle.” 
 not wishing to demolish this absurd little theory by bringing to bear the 
masses of scientific evidence he had at his command, james decided 
to gently dissuade his opponent by making her see some of the 
inadequacies of her position. 
 “if your theory is correct, madam,” he asked, “what does this turtle 
stand on?” 
 “you’re a very clever man, mr. james, and that’s a very good question” 
replied the little old lady, “but i have an answer to it. and it is this: the 
first turtle stands on the back of a second, far larger, turtle, who stands 
directly under him.” 
 “but what does this second turtle stand on?” persisted james patiently. 
to this the little old lady crowed triumphantly. “it’s no use, mr. james— 
it’s turtles all the way down.” 
 j. r. ross, constraints on variables in syntax
 introduction 
 a function definition that includes a call to itself is said to be  recursive . like most 
modern programming languages, c++ allows functions to be recursive. if used with a 
little care, recursion can be a useful programming technique. this chapter introduces 
the basic techniques needed for defining successful recursive functions. there is 
nothing in this chapter that is truly unique to c++. if you are already familiar with 
recursion, you can safely skip this chapter. 
 this chapter uses material from  chapters  1 to  5 only.  sections  13.1 and  13.2 do not 
use any material from  chapter  5 , so you can cover recursion any time after  chapter  4 . 
if you have not read  chapter  11 , you may find it helpful to review the section of 
 chapter  1 on namespaces. 
13 recursion
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 603

____________________________________________

 recursive void functions      573
 13.1 recursive  void functions 
 i remembered too that night which is at the middle of the thousand 
and one nights when scheherazade (through a magical oversight 
of the copyist) begins to relate word for word the story of the thousand 
and one nights, establishing the risk of coming once again to the night 
when she must repeat it, and thus to infinity. 
 jorge luis borges, the garden of forking paths
 when you are writing a function to solve a task, one basic design technique is to 
break the task into subtasks. sometimes it turns out that at least one of the subtasks 
is a smaller example of the same task. for example, if the task is to search a list for a 
particular value, you might divide this into the subtask of searching the first half of the 
list and the subtask of searching the second half of the list. the subtasks of searching 
the halves of the list are “smaller” versions of the original task. whenever one subtask is 
a smaller version of the original task to be accomplished, you can solve the original task 
using a recursive function. we begin with a simple example to illustrate this technique. 
 recursion 
in c++ a function definition may contain a call to the function being defined. in such cases 
the function is said to be recursive.
 example:  vertical numbers 
 display  13.1 contains a demonstration program for a recursive function named 
writevertical that takes one (nonnegative)  int argument and writes that  int
to the screen, with the digits going down the screen one per line. for example, the 
invocation
 writevertical(1234); 
 would produce the output 
 1 
 2 
 3 
 4 
 the task to be performed by  writevertical may be broken down into the following 
two cases: 
■  simple case: if n < 10, then write the number n to the screen. 
 after all, if the number is only one digit long, the task is trivial. 
(continued)


 
 

____________________________________________

Absolute.pdf page: 639

____________________________________________

programming projects      609
do  programming project  13.1 iteratively, rather than recursively; that is, do 
the problem with a loop. you should compute each fibonacci number once 
on the way to the number requested and discard the numbers when they 
are no longer needed. 
  b.  time the solution for  programming project  13.1 and part a of this project in
      finding the 1 st , 3 rd , 5 th , 7 th , 9 th , 11 th , 13 th , and 15 th fibonacci numbers. determine
      how long each function takes. compare and comment on your results. 
   hints: if you are running linux, you can use the bash  time utility. it gives real time 
(as in wall clock time), user time (time measured by cpu cycles devoted to your 
program), and sys time (cpu cycles devoted to tasks other than your program). if 
you are running in some other environment, you will have to read your manual, 
or ask your instructor, in order to find out how to measure the time a program 
takes to run. 
 7.  (you need to have first completed  programming project  13.6 to work on this 
project.) when computing a fibonacci number using the most straightforward 
recursive function definition, the recursive solution recomputes each fibonacci 
number too many times. to compute f
i+2
= f
i
+ f
i+1
 , it computes all the numbers 
computed in  f i a second time in computing  f
i+1
 . you can avoid this by saving the 
numbers in an array while computing f
i
 . write another version of your recursive 
fibonacci function based on this idea. in the recursive solution for calculating the 
n th fibonacci number, declare an array of size n. array entry with index i stores 
the ith (i … n) fibonacci number as it is computed the first time. then use the array 
to avoid the second (redundant) recalculation of the fibonacci numbers. time this 
solution as you did in  programming project  13.6 , and compare it to your results 
for the iterative solution. 
 8.  a savings account typically accrues savings using compound interest. if you deposit 
$1000 with a 10% interest rate per year, after one year you will have $1100. if you 
leave this money in the account for another year at 10% interest, you will have 
$1210. after three years you will have $1331, and so on. 
   write a program that inputs the initial amount, an interest rate per year, and the 
number of years the money will accrue compound interest. write a recursive func-
tion that calculates the amount of money that will be in the savings account using 
the input information. 
  to verify your function, the amount should be equal to  p(1+i)n , where  p is the 
amount initially saved, i is the interest rate per year, and  n is the number of years. 
 9 . we have  n people in a room, where  n is an integer greater than or equal to 1. each 
person shakes hands once with every other person. what is the total number, h(n) , 
of handshakes? write a recursive function to solve this problem. to get you started, 
if there are only one or two people in the room, then 
 handshake(1) = 0 
 handshake(2) = 1 
  if a third person enters the room, he or she must shake hands with each of the 
two people already there. this is two handshakes in addition to the number 
solution to 
programming
project 13.9
videonote


 
 

____________________________________________

Absolute.pdf page: 640

____________________________________________

610     chapter 13    recursion
of handshakes that would be made in a room of two people, or a total of three 
handshakes. 
  if a fourth person enters the room, he or she must shake hands with each of the 
three people already there. this is three handshakes in addition to the number of 
handshakes that would be made in a room of three people, or six handshakes. 
   if you can generalize this to  n handshakes, you should be able to write the 
recursive solution. 
 10.  consider a frame of bowling pins, where each * represents a pin: 
solution to 
programming 
project  13.11
videonote
   there are five rows and a total of fifteen pins. if we had only the top four rows, 
there would be a total of ten pins. if we had only the top three rows, there would 
be a total of six pins. if we had only the top two rows, there would be a total of 
three pins. if we had only the top row, there would be a total of one pin. 
  write a recursive function that takes as input the number of rows,  n , and outputs 
the total number of pins that would exist in a pyramid with n rows. your program 
should allow for values of n that are larger than 5. 
 11.  write a recursive function named  contains with the following header: 
 bool contains ( char *haystack,  char *needle) 
  the function should return  true if the c-string  needle is contained within the 
c-string haystack and false if needle is not in haystack . for example,
 contains("c++ programming", "ogra") should return  true 
 contains("c++ programming", "grammy") should return  false 
  you are not allowed to use the string class substr or find functions to 
determine a match.
 12.  the following diagram is an example of a  deterministic finite state automaton , or 
dfa. this particular dfa describes an algorithm to determine if a sequence of 
characters is a properly formatted monetary amount with commas. for example, 
“$1,000” and “$25” and “$551,323,991,391” are properly formatted but “1,000” 
(no initial $) and “$1000” (missing comma) and “$5424,132” (missing comma) 
are not. 
*
* *
* * *
* * * *
* * * * *
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 644

____________________________________________

 like mother, like daughter 
 common saying 
 introduction 
 object-oriented programming is a popular and powerful programming technique. 
among other things, it provides for a dimension of abstraction known as inheritance.
this means that a very general form of a class can be defined and compiled. later, 
more specialized versions of that class may be defined and can inherit the properties of 
the general class. this chapter covers inheritance in general and, more specifically, how 
it is realized in c++. 
 this chapter does not use any of the material presented in  chapter  12 (file i/o) 
or  chapter  13 (recursion). it also does not use the material in  section  7.3 of 
 chapter  7 , which covers vectors.  section  14.1 also does not use any material from 
 chapter  10 (pointers and dynamic arrays). 
 14.1 inheritance basics 
 if there is anything that we wish to change in the child, we should first 
examine it and see whether it is not something that could better be 
changed in ourselves. 
 carl gustav jung, the integration of the personality 
 inheritance is the process by which a new class—known as a  derived class —is created 
from another class, called the base class . a derived class automatically has all the 
member variables and all the ordinary member functions that the base class has, and 
can have additional member functions and additional member variables. 
 derived classes 
 suppose we are designing a record-keeping program that has records for salaried 
employees and hourly employees. there is a natural hierarchy for grouping these 
classes. these are all classes of people who share the property of being employees. 
 employees who are paid an hourly wage are one subset of employees. another 
subset consists of employees who are paid a fixed wage each month or week. although 
the program may not need any type corresponding to the set of all employees, thinking 
in terms of the more general concept of employees can be useful. for example, all 
employees have names and social security numbers, and the member functions for 
 14  inheritance 
derived class 
base class 
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 850

____________________________________________

820     chapter 17    linked data structures
  b.  implement the member and friend functions and overloaded operators. note 
that some of the functions listed here are already implemented in the text. 
you should make full use of the text’s code. you should test your package 
 thoroughly. 
  c.  design and implement an iterator class for the tree class. you will need to 
 decide what a  begin and end element means for your  searchtree , and what 
will be the next element the ++ operator will point to. 
    hint 1: you might maintain a private size variable that is increased by  insertion 
and decreased by deletion, and whose value is returned by the size function. 
an alternative (use this if you know calls to size will be quite infrequent) is 
to calculate the size when you need it by traversing the tree. similar tech-
niques, though with more sophisticated details, can be used to implement the 
height function. 
    hint 2: do these a few members at a time. compile and test after doing each 
group of a few members. you will be glad you did it this way. 
    hint 3: before you write the operator, =, and copy constructor, note that their 
jobs have a common task—duplicating another tree. write a copytree  function 
that abstracts out the common task of the copy constructor and operator, =.
then write these two important functions using the common code. 
    hint 4: the function  makeempty and the destructor have a common tree 
 destruction task. 
  5.  in an ancient land, the beautiful princess eve had many suitors. she decided on 
the following procedure to determine which suitor she would marry. first, all of 
the suitors would be lined up one after the other and assigned numbers. the first 
suitor would be number 1, the second number 2, and so on up to the last suitor, 
number n. starting at the first suitor, she would then count three suitors down 
the line (because of the three letters in her name) and the third suitor would be 
eliminated from winning her hand and removed from the line. eve would then 
continue, counting three more suitors, and eliminating every third suitor. when 
she reached the end of the line, she would continue counting from the beginning. 
  for example, if there were six suitors, then the elimination process would proceed 
as follows: 
  123456 initial list of suitors, start counting from 1 
  12456 suitor 3 eliminated, continue counting from 4 
  1245 suitor 6 eliminated, continue counting from 1 
  125 suitor 4 eliminated, continue counting from 5 
  15 suitor 2 eliminated, continue counting from 5 
  1 suitor 5 eliminated, 1 is the lucky winner 
solution to 
programming
project 17.5 
videonote
www.itpub.net


 
 

____________________________________________

Absolute.pdf page: 889

____________________________________________

iterators      859
 if you have not already done so, you should read  section  7.3 of  chapter  7 , which 
covers the vector template class of the stl.  although the current chapter does not 
use any of the material in  chapter  17 , most readers will find that reading  chapter  17 
before reading this one will aid his or her comprehension of this chapter by giving 
sample concrete implementations of some of the abstract ideas intrinsic to the stl. 
 this chapter does not use any of the material in  chapters  12 to  15 . 
 19.1 iterators 
 to iterate is human, and programmers are human. 
 anonymous 
 if you have not yet done so, you should read  chapter  10 on pointers and arrays 
and also read  section  7.3 of  chapter  7 , which covers vectors. vectors are one of the 
container template classes in the stl. iterators are a generalization of pointers. this 
section shows how to use iterators with vectors. other container template classes 
that we introduce in  section  19.2 use iterators in the same way. so, all that you learn 
about iterators in this section will apply across a wide range of containers rather than 
applying solely to vectors. this reflects one of the basic tenets of the stl philosophy: 
the semantics, naming, and syntax for iterator usage should be (and is) uniform across 
different container types. 
 iterator basics 
 an  iterator is a generalization of a pointer, and in fact is typically even implemented 
using a pointer, but the abstraction of an iterator is designed to spare you the 
details of the implementation and give you a uniform interface to iterators that is 
the same across different container classes. each container class has its own iterator 
types, just like each data type has its own pointer type. but just as all pointer types 
behave essentially the same for dynamic variables of their particular data type, so too 
does each iterator type behave the same, but each iterator is used only with its own 
container type. 
 an iterator is not a pointer, but you will not go far wrong if you think of it and use 
it as if it were. like a pointer variable, an iterator variable is located at (meaning, it 
points to) one data entry in the container. you manipulate iterators using the following 
overloaded operators that apply to iterator objects: 
■  prefix and postfix increment operators ( ++ ) for advancing the iterator to the next 
data item. 
■  prefix and postfix decrement operators ( -- ) for moving the iterator to the 
previous data item. 
■  equal and unequal operators (== and  != ) to test whether two iterators point to 
the same data location. 
iterator


 
 

____________________________________________

Absolute.pdf page: 920

____________________________________________

890     chapter 19    standard template library
gives the important details but not the fine details of the coding. the stl specifies 
certain details about the algorithms underlying the stl template functions, which is 
why they are sometimes called generic algorithms . these stl function templates do 
more than just deliver a value in any way that the implementers wish. the function 
templates in the stl come with minimum requirements that must be satisfied by 
their implementations if they are to satisfy the standard. in most cases, they must be 
implemented with a guaranteed running time. this adds an entirely new dimension to 
the idea of a function interface. in the stl, the interface not only tells a programmer 
what the function does and how to use the functions, but also how rapidly the task 
will be done. in some cases, the standard even specifies the particular algorithm that 
is used, although not the exact details of the coding. moreover, when it does specify 
the particular algorithm, it does so because of the known efficiency of the algorithm. 
the key new point is the specification of an efficiency guarantee for the code. in this 
chapter, we will use the terms generic algorithm ,  generic function , and  stl function 
template to all mean the same thing. 
 in order to have some terminology to discuss the efficiency of these template 
functions or generic algorithms, we first present some background on how the 
efficiency of algorithms is usually measured. 
 running times and big- o notation 
 if you ask a programmer how fast his or her program is, you might expect an answer 
like “two seconds.” however, the speed of a program cannot be given by a single 
number. a program will typically take a longer amount of time on larger inputs than 
it will on smaller inputs. you would expect that a program for sorting numbers would 
take less time to sort 10 numbers than it would to sort 1000 numbers. perhaps it takes 
2 seconds to sort 10 numbers, but 10 seconds to sort 1000 numbers. how then should 
the programmer answer the question “how fast is your program?” the programmer 
would have to give a table of values showing how long the program takes for different 
sizes of input. for example, the table might be as shown in  display  19.15 . this table 
does not give a single time, but instead gives different times for a variety of different 
input sizes. 
 the table is a description of what is called a  function in mathematics. just as a (non-
void ) c ++ function takes an argument and returns a value, so too does this function 
take an argument, which is an input size, and returns a number, which is the time the 
program takes on an input of that size. if we call this function t , then  t (10) is 2 seconds, 
t (100) is 2.1 seconds,  t (1000) is 10 seconds, and  t (10,000) is 2.5 minutes. the 
table is just a sample of some of the values of this function t . the program will take 
some amount of time on inputs of every size. so although they are not shown in the 
table, there are also values for t (1),  t (2), . . .,  t (101),  t (102), and so forth. for 
any positive integer n ,  t (n ) is the amount of time it takes for the program to sort 
n numbers. the function  t is called the  running time of the program. 
  so far we have been assuming that this sorting program will take the same amount 
of time on any list of n numbers. that need not be true. perhaps it takes much less 
time if the list is already sorted or almost sorted. in that case, t (n ) is defined to be the 
time taken by the “hardest” list—that is, the time taken on that list of n numbers that 
mathematical
function
running time 
www.itpub.net


